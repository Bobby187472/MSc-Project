{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **AI DRIVEN PRECISION AGRICULTURE FOR EARLY DISEASE DETECTION AND SUSTAINABLE CROP PROTECTION**"
      ],
      "metadata": {
        "id": "0OuHl_0vRwhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STAGE-2-MULTI-CLASS CLASSIFIER FOR 59 CLASSES (ONLY-DISEASED)**"
      ],
      "metadata": {
        "id": "R8FHfniVSfZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATASET: PlantWild (Benchmarking In-the-Wild Multimodal Plant Disease Recognition and A Versatile Baseline)**\n",
        "# **LINK TO PAPER: https://tqwei05.github.io/PlantWild**\n",
        "# **LINK TO DATASET: https://huggingface.co/datasets/uqtwei2/PlantWild/tree/main**"
      ],
      "metadata": {
        "id": "6hIgknB6UxVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**"
      ],
      "metadata": {
        "id": "ik_rGSt_Naf-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zeh85QYaNY4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WokrzEEwpiZW"
      },
      "outputs": [],
      "source": [
        "# Import essential libraries for deep learning and data processing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Import data manipulation and analysis libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "import json\n",
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "# Import machine learning evaluation libraries\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
        "from sklearn.metrics import cohen_kappa_score, matthews_corrcoef\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Rectangle\n",
        "import cv2\n",
        "\n",
        "# Import statistical analysis libraries\n",
        "from scipy import stats\n",
        "from scipy.stats import bootstrap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import Google Drive integration\n",
        "from google.colab import drive\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Configure matplotlib for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(\"Ready for enhanced Stage 2 implementation...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GOOGLE DRIVE MOUNT AND DATASET DOWNLOAD**"
      ],
      "metadata": {
        "id": "f8RoElWyNwqL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCJS9O3zpw9U"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access files and save models\n",
        "from google.colab import drive  # Import Google Drive mounting functionality\n",
        "drive.mount('/content/drive')  # Mount Google Drive to access files\n",
        "print(\"Google Drive mounted successfully!\")  # Confirm successful mounting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create working directory and download dataset\n",
        "import os  # Import operating system interface\n",
        "import zipfile  # Import zip file handling functionality\n",
        "import gdown  # Import Google Drive download utility\n",
        "\n",
        "# Create working directory for dataset storage\n",
        "os.makedirs(\"/content/plantwild\", exist_ok=True)  # Create directory for dataset storage\n",
        "\n",
        "# Google Drive File ID from your shared PlantWild link\n",
        "file_id = \"1TVvXiJIWvpOYUba78gm6ALuy52Ks6IwW\"  # Unique identifier for dataset file\n",
        "zip_path = \"/content/plantwild/plantwild.zip\"  # Local path for downloaded zip file\n",
        "\n",
        "# Download the file using gdown utility\n",
        "print(\"Downloading PlantWild dataset...\")  # Inform user of download start\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", zip_path, quiet=False)  # Download dataset from Google Drive\n",
        "\n",
        "# Unzip the dataset to extract contents\n",
        "print(\"Extracting dataset...\")  # Inform user of extraction start\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:  # Open zip file for reading\n",
        "    zip_ref.extractall(\"/content/plantwild\")  # Extract all contents to plantwild directory\n",
        "\n",
        "print(\"PlantWild dataset downloaded and extracted to /content/plantwild\")  # Confirm successful extraction\n",
        "\n",
        "# Verify dataset structure and confirm successful setup\n",
        "DATASET_ROOT = \"/content/plantwild/plantwild\"  # Define path to extracted dataset\n",
        "if os.path.exists(DATASET_ROOT):  # Check if dataset directory exists\n",
        "    print(f\"Dataset found at: {DATASET_ROOT}\")  # Confirm dataset location\n",
        "    print(f\"Contents: {os.listdir(DATASET_ROOT)}\")  # Display dataset contents\n",
        "else:\n",
        "    print(\"Dataset not found!\")  # Error message if dataset not found\n",
        "\n",
        "print(\"Dataset download and verification completed!\")  # Confirm dataset setup completion"
      ],
      "metadata": {
        "id": "nhzcWJYH3qH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONFIGURATION AND SET-UP**"
      ],
      "metadata": {
        "id": "xalS0sPRN4H8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef1mdIWrxrLH"
      },
      "outputs": [],
      "source": [
        "# Enhanced Stage 2 configuration for precision agriculture optimization\n",
        "IMG_HEIGHT = 224  # Image height for MobileNetV2 input\n",
        "IMG_WIDTH = 224   # Image width for MobileNetV2 input\n",
        "TOTAL_CLASSES = 89  # Total classes in PlantWild dataset\n",
        "DISEASED_CLASSES = 59  # Number of diseased classes for Stage 2\n",
        "HEALTHY_CLASSES = 30  # Number of healthy classes (filtered out in Stage 2)\n",
        "\n",
        "# Enhanced training parameters for better convergence\n",
        "EPOCHS_PHASE1 = 15  # Epochs for head training (increased from 10)\n",
        "EPOCHS_PHASE2 = 20  # Epochs for fine-tuning (increased from 15)\n",
        "BATCH_SIZE = 16  # Batch size for training (optimized for A-100 GPU)\n",
        "\n",
        "# Advanced learning rates for different optimizers\n",
        "LEARNING_RATES = {\n",
        "    'AdamW': {'phase1': 1e-3, 'phase2': 1e-5},  # AdamW with weight decay\n",
        "    'Lion': {'phase1': 1e-3, 'phase2': 1e-5},   # Lion optimizer for speed\n",
        "    'AdaBelief': {'phase1': 1e-3, 'phase2': 1e-5}  # AdaBelief for imbalanced data\n",
        "}\n",
        "\n",
        "# Enhanced regularization parameters\n",
        "DROPOUT_RATE = 0.5  # Increased dropout for better generalization\n",
        "L2_REGULARIZATION = 1e-4  # L2 regularization for weight decay\n",
        "WEIGHT_DECAY = 1e-4  # Weight decay for AdamW optimizer\n",
        "\n",
        "# Advanced data augmentation parameters for minority classes (FIXED - removed unsupported parameters)\n",
        "AUGMENTATION_PARAMS = {\n",
        "    'rotation_range': 25,  # Increased rotation range\n",
        "    'width_shift_range': 0.2,  # Horizontal shift range\n",
        "    'height_shift_range': 0.2,  # Vertical shift range\n",
        "    'zoom_range': 0.2,  # Zoom range for scale variation\n",
        "    'horizontal_flip': True,  # Enable horizontal flipping\n",
        "    'vertical_flip': False,  # Disable vertical flipping (unrealistic for plants)\n",
        "    'brightness_range': [0.7, 1.3],  # Brightness variation range\n",
        "    'fill_mode': 'nearest',  # Fill mode for augmented images\n",
        "    'cval': 0.0  # Constant value for fill mode\n",
        "}\n",
        "\n",
        "# Directory configuration for model storage and analysis\n",
        "LOCAL_MODEL_DIR = \"/content/models_stage2_enhanced\"  # Local model storage directory\n",
        "DRIVE_BACKUP_DIR = \"/content/drive/MyDrive/plantwild_stage2_enhanced\"  # Google Drive backup directory\n",
        "DRIVE_MODELS_DIR = \"/content/drive/MyDrive/Stage2_Enhanced_Models\"  # Enhanced models directory\n",
        "DRIVE_ANALYSIS_DIR = \"/content/drive/MyDrive/Stage2_Enhanced_Analysis\"  # Analysis results directory\n",
        "DRIVE_VISUALIZATIONS_DIR = \"/content/drive/MyDrive/Stage2_Enhanced_Visualizations\"  # Visualization directory\n",
        "DRIVE_GRADCAM_DIR = \"/content/drive/MyDrive/Stage2_Enhanced_GradCAM\"  # GradCAM analysis directory\n",
        "\n",
        "# Create necessary directories for organization\n",
        "os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)  # Create local model directory\n",
        "os.makedirs(DRIVE_BACKUP_DIR, exist_ok=True)  # Create Google Drive backup directory\n",
        "os.makedirs(DRIVE_MODELS_DIR, exist_ok=True)  # Create enhanced models directory\n",
        "os.makedirs(DRIVE_ANALYSIS_DIR, exist_ok=True)  # Create analysis directory\n",
        "os.makedirs(DRIVE_VISUALIZATIONS_DIR, exist_ok=True)  # Create visualizations directory\n",
        "os.makedirs(DRIVE_GRADCAM_DIR, exist_ok=True)  # Create GradCAM directory\n",
        "\n",
        "# Enhanced ensemble configuration with powerful optimizers\n",
        "ENSEMBLE_CONFIGS_STAGE2 = [\n",
        "    {'name': 'AdamW', 'optimizer_class': 'AdamW', 'weight_decay': 1e-4},  # AdamW with weight decay\n",
        "    {'name': 'Lion', 'optimizer_class': 'Lion', 'weight_decay': 1e-4},    # Lion optimizer for speed\n",
        "    {'name': 'AdaBelief', 'optimizer_class': 'AdaBelief', 'eps': 1e-16}   # AdaBelief for imbalanced data\n",
        "]\n",
        "\n",
        "print(\"Enhanced Stage 2 configuration completed!\")\n",
        "print(f\"Training on {DISEASED_CLASSES} diseased classes with advanced optimizers\")\n",
        "print(f\"Models will be saved to: {DRIVE_MODELS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA LOADING FUNCTIONS**"
      ],
      "metadata": {
        "id": "1gjOxVotOCeW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqUndNnCyf2U"
      },
      "outputs": [],
      "source": [
        "# Enhanced data loading functions with precision agriculture focus\n",
        "def load_class_mapping(classes_file):\n",
        "    \"\"\"\n",
        "    Load class mapping from classes.txt with enhanced categorization\n",
        "    for precision agriculture applications\n",
        "    \"\"\"\n",
        "    class_mapping = {}  # Dictionary to store class ID to name mapping\n",
        "    healthy_classes = []  # List to store healthy class names\n",
        "    diseased_classes = []  # List to store diseased class names\n",
        "\n",
        "    # Read classes.txt file and parse class information\n",
        "    with open(classes_file, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(' ', 1)  # Split on first space to separate ID and name\n",
        "            if len(parts) == 2:  # Ensure line has both ID and name\n",
        "                class_id = int(parts[0])  # Convert class ID to integer\n",
        "                class_name = parts[1]  # Extract class name\n",
        "                class_mapping[class_id] = class_name  # Store in mapping dictionary\n",
        "\n",
        "                # Categorize class as healthy or diseased for binary classification\n",
        "                if is_healthy(class_name):\n",
        "                    healthy_classes.append(class_name)  # Add to healthy classes list\n",
        "                else:\n",
        "                    diseased_classes.append(class_name)  # Add to diseased classes list\n",
        "\n",
        "    return class_mapping, healthy_classes, diseased_classes  # Return all mappings\n",
        "\n",
        "def is_healthy(class_name):\n",
        "    \"\"\"\n",
        "    Enhanced function to determine if a class represents a healthy plant\n",
        "    based on disease-related keywords and precision agriculture criteria\n",
        "    \"\"\"\n",
        "    # Comprehensive list of disease indicators for agricultural applications\n",
        "    disease_keywords = [\n",
        "        'rot', 'blight', 'virus', 'mosaic', 'rust', 'scab',\n",
        "        'spot', 'canker', 'wilt', 'anthracnose', 'mildew',\n",
        "        'curl', 'greening', 'smut', 'cavity', 'pocket', 'scorch',\n",
        "        'disease', 'infection', 'lesion', 'necrosis'\n",
        "    ]\n",
        "\n",
        "    # A class is considered healthy if it ends with 'leaf' AND doesn't contain disease keywords\n",
        "    # This is crucial for early disease detection in precision agriculture\n",
        "    return (class_name.endswith(' leaf') and\n",
        "            not any(keyword in class_name.lower() for keyword in disease_keywords))\n",
        "\n",
        "def load_stage2_dataframe(dataset_root):\n",
        "    \"\"\"\n",
        "    Load and prepare Stage 2 dataset with enhanced analysis for precision agriculture\n",
        "    \"\"\"\n",
        "    # Define file paths for dataset loading\n",
        "    classes_file = os.path.join(dataset_root, 'classes.txt')  # Path to classes file\n",
        "    trainval_file = os.path.join(dataset_root, 'trainval.txt')  # Path to trainval file\n",
        "\n",
        "    # Load class mapping and categorization\n",
        "    class_mapping, healthy_classes, diseased_classes = load_class_mapping(classes_file)\n",
        "\n",
        "    # Read trainval.txt file with proper delimiter\n",
        "    df = pd.read_csv(trainval_file, sep='=', names=['image_path', 'class_id', 'split'])\n",
        "\n",
        "    # Add class names and binary labels for two-stage classification\n",
        "    df['class_name'] = df['class_id'].map(class_mapping)  # Map class IDs to names\n",
        "    df['binary_label'] = df['class_name'].apply(is_healthy)  # Apply healthy/diseased classification\n",
        "    df['binary_label'] = df['binary_label'].map({True: 'healthy', False: 'diseased'})  # Convert to string labels\n",
        "\n",
        "    # Add split information for training, validation, and testing\n",
        "    # FIXED: The split values are 0=test, 1=train, 2=val, but we need to handle them correctly\n",
        "    df['split_name'] = df['split'].map({0: 'test', 1: 'train', 2: 'val'})  # Map split codes to names\n",
        "\n",
        "    # Verify image file existence for data integrity\n",
        "    df['image_exists'] = df['image_path'].apply(lambda x: os.path.exists(os.path.join(dataset_root, 'images', x)))\n",
        "\n",
        "    # Filter out non-existent images for reliable training\n",
        "    df = df[df['image_exists'] == True].copy()  # Keep only existing images\n",
        "    df = df.drop('image_exists', axis=1)  # Remove verification column\n",
        "\n",
        "    return df, class_mapping, healthy_classes, diseased_classes  # Return processed dataframe and mappings\n",
        "\n",
        "# Load Stage 2 dataset with enhanced analysis\n",
        "print(\"Loading enhanced Stage 2 dataset for precision agriculture analysis...\")\n",
        "df_stage2, class_mapping, healthy_classes, diseased_classes = load_stage2_dataframe(DATASET_ROOT)\n",
        "\n",
        "# Display comprehensive dataset statistics for precision agriculture insights\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ENHANCED STAGE 2 DATASET ANALYSIS FOR PRECISION AGRICULTURE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Basic dataset statistics\n",
        "print(f\"Total samples: {len(df_stage2):,}\")\n",
        "print(f\"Total classes: {len(class_mapping)}\")\n",
        "print(f\"Healthy classes: {len(healthy_classes)}\")\n",
        "print(f\"Diseased classes: {len(diseased_classes)}\")\n",
        "\n",
        "# Split distribution analysis\n",
        "print(\"\\nDataset Split Distribution:\")\n",
        "split_counts = df_stage2['split_name'].value_counts()\n",
        "for split, count in split_counts.items():\n",
        "    print(f\"  {split.capitalize()}: {count:,} samples ({count/len(df_stage2)*100:.1f}%)\")\n",
        "\n",
        "# Binary label distribution for Stage 1 analysis\n",
        "print(\"\\nBinary Classification Distribution:\")\n",
        "binary_counts = df_stage2['binary_label'].value_counts()\n",
        "for label, count in binary_counts.items():\n",
        "    print(f\"  {label.capitalize()}: {count:,} samples ({count/len(df_stage2)*100:.1f}%)\")\n",
        "\n",
        "# Class distribution analysis for precision agriculture insights\n",
        "print(\"\\nTop 10 Most Frequent Classes:\")\n",
        "class_counts = df_stage2['class_name'].value_counts().head(10)\n",
        "for i, (class_name, count) in enumerate(class_counts.items(), 1):\n",
        "    print(f\"  {i:2d}. {class_name}: {count:,} samples\")\n",
        "\n",
        "# Diseased class analysis for early detection focus\n",
        "print(\"\\nDiseased Classes Sample Counts:\")\n",
        "diseased_df = df_stage2[df_stage2['binary_label'] == 'diseased']\n",
        "diseased_class_counts = diseased_df['class_name'].value_counts()\n",
        "print(f\"Total diseased samples: {len(diseased_df):,}\")\n",
        "print(f\"Average samples per diseased class: {len(diseased_df)/len(diseased_classes):.1f}\")\n",
        "\n",
        "# Debug: Show actual split values to understand the data structure\n",
        "print(\"\\nDEBUG: Actual split values in dataset:\")\n",
        "print(df_stage2['split'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nDataset loading and analysis completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA PREPARATION AND PREPROCESSING**"
      ],
      "metadata": {
        "id": "8O2YppaZOQCt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35MY1s7aMe-i"
      },
      "outputs": [],
      "source": [
        "# **ENHANCED STAGE 2 DATA PREPARATION AND PREPROCESSING FOR PRECISION AGRICULTURE**\n",
        "\n",
        "# Import required libraries for enhanced data preparation\n",
        "import pandas as pd # Import pandas for data manipulation\n",
        "import numpy as np # Import numpy for numerical operations\n",
        "from sklearn.model_selection import train_test_split # Import train-test split functionality\n",
        "from sklearn.preprocessing import LabelEncoder # Import label encoder for class encoding\n",
        "import os # Import operating system interface\n",
        "import json # Import JSON handling for saving class mappings\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator # Import image data generator\n",
        "import tensorflow as tf # Import TensorFlow\n",
        "\n",
        "print(\"Preparing enhanced Stage 2 data for precision agriculture disease classification...\")\n",
        "\n",
        "# Define dataset paths and parameters\n",
        "DATASET_ROOT = \"/content/plantwild/plantwild\" # Root directory of dataset\n",
        "IMAGES_DIR = os.path.join(DATASET_ROOT, \"images\") # Path to images directory\n",
        "IMG_HEIGHT, IMG_WIDTH = 224, 224 # Image dimensions for model input\n",
        "BATCH_SIZE = 32 # Batch size for training\n",
        "\n",
        "# Verify dataset structure\n",
        "if not os.path.exists(DATASET_ROOT):\n",
        "    print(f\"Dataset not found at: {DATASET_ROOT}\")\n",
        "    print(\"Please ensure you have downloaded and extracted the PlantWild dataset\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Dataset found at: {DATASET_ROOT}\")\n",
        "print(f\"Contents: {os.listdir(DATASET_ROOT)}\")\n",
        "\n",
        "# Check if images directory exists\n",
        "if not os.path.exists(IMAGES_DIR):\n",
        "    print(f\"Images directory not found at: {IMAGES_DIR}\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Images directory found at: {IMAGES_DIR}\")\n",
        "\n",
        "# Get all class folders (subdirectories in images folder)\n",
        "class_folders = [d for d in os.listdir(IMAGES_DIR) if os.path.isdir(os.path.join(IMAGES_DIR, d))]\n",
        "print(f\"Found {len(class_folders)} class folders\")\n",
        "\n",
        "# Create dataset dataframe from folder structure\n",
        "dataset_data = [] # List to store dataset information\n",
        "\n",
        "for class_name in class_folders:\n",
        "    class_path = os.path.join(IMAGES_DIR, class_name) # Path to class folder\n",
        "    image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))] # Get image files\n",
        "\n",
        "    for image_file in image_files:\n",
        "        # Create relative path from images directory\n",
        "        relative_path = os.path.join(class_name, image_file) # Relative path for image\n",
        "        dataset_data.append({\n",
        "            'image_path': relative_path, # Store relative path\n",
        "            'class_name': class_name, # Store class name\n",
        "            'full_path': os.path.join(class_path, image_file) # Store full path for verification\n",
        "        })\n",
        "\n",
        "# Create dataframe from collected data\n",
        "df_stage2 = pd.DataFrame(dataset_data) # Convert to pandas dataframe\n",
        "\n",
        "print(f\"Total images found: {len(df_stage2)}\")\n",
        "print(f\"Classes found: {len(df_stage2['class_name'].unique())}\")\n",
        "\n",
        "# Display first few classes\n",
        "print(\"\\nFirst 10 classes found:\")\n",
        "for i, class_name in enumerate(df_stage2['class_name'].unique()[:10]):\n",
        "    class_count = len(df_stage2[df_stage2['class_name'] == class_name])\n",
        "    print(f\"  {i+1}. {class_name}: {class_count} images\")\n",
        "\n",
        "# Create label encoder for class encoding\n",
        "label_encoder = LabelEncoder() # Initialize label encoder\n",
        "df_stage2['encoded_class_id'] = label_encoder.fit_transform(df_stage2['class_name']) # Encode class names to integers\n",
        "\n",
        "# Filter for diseased classes only (excluding healthy classes)\n",
        "def is_healthy(class_name):\n",
        "    \"\"\"Determine if a class is healthy based on disease keywords and leaf suffix\"\"\"\n",
        "    healthy_keywords = ['healthy', 'normal', 'good']\n",
        "    disease_keywords = ['rot', 'blight', 'mildew', 'mosaic', 'rust', 'scab', 'spot', 'virus', 'disease', 'infected', 'anthracnose', 'canker', 'wilt', 'curl', 'yellows', 'mottle', 'streak', 'necrosis', 'chlorosis', 'lesion']\n",
        "\n",
        "    # Check if class name contains any disease keywords\n",
        "    has_disease = any(keyword in class_name.lower() for keyword in disease_keywords)\n",
        "\n",
        "    # Check if class name contains healthy keywords\n",
        "    has_healthy = any(keyword in class_name.lower() for keyword in healthy_keywords)\n",
        "\n",
        "    # Class is healthy if it has healthy keywords and no disease keywords\n",
        "    # OR if it ends with 'leaf' and has no disease keywords\n",
        "    return (has_healthy and not has_disease) or (class_name.lower().endswith('leaf') and not has_disease)\n",
        "\n",
        "# Apply binary classification\n",
        "df_stage2['binary_label'] = df_stage2['class_name'].apply(lambda x: 'healthy' if is_healthy(x) else 'diseased') # Classify as healthy or diseased\n",
        "\n",
        "# Display binary classification results\n",
        "binary_counts = df_stage2['binary_label'].value_counts()\n",
        "print(f\"\\nBinary classification results:\")\n",
        "print(f\"  Healthy classes: {binary_counts.get('healthy', 0)}\")\n",
        "print(f\"  Diseased classes: {binary_counts.get('diseased', 0)}\")\n",
        "\n",
        "# Filter for diseased classes only\n",
        "df_diseased = df_stage2[df_stage2['binary_label'] == 'diseased'].copy() # Keep only diseased samples\n",
        "\n",
        "# Reset index and re-encode class IDs for diseased classes only\n",
        "df_diseased = df_diseased.reset_index(drop=True) # Reset index for clean numbering\n",
        "diseased_label_encoder = LabelEncoder() # Create new label encoder for diseased classes\n",
        "df_diseased['encoded_class_id'] = diseased_label_encoder.fit_transform(df_diseased['class_name']) # Re-encode diseased classes\n",
        "\n",
        "# Create class mapping for diseased classes\n",
        "diseased_class_mapping = {i: name for i, name in enumerate(diseased_label_encoder.classes_)} # Map encoded IDs to class names\n",
        "\n",
        "# Display dataset information\n",
        "print(f\"\\nStage 2 data preparation completed:\")\n",
        "print(f\" Original diseased samples: {len(df_diseased):,}\")\n",
        "print(f\" Diseased classes: {len(diseased_class_mapping)}\")\n",
        "print(f\" Class ID range: 0 to {len(diseased_class_mapping) - 1}\")\n",
        "\n",
        "# Create train/validation/test split\n",
        "train_df, temp_df = train_test_split(df_diseased, test_size=0.3, random_state=42, stratify=df_diseased['encoded_class_id']) # Split 70% train, 30% temp\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['encoded_class_id']) # Split temp into 15% val, 15% test\n",
        "\n",
        "# Add split identifiers\n",
        "train_df['split'] = 0 # Training split identifier\n",
        "val_df['split'] = 1 # Validation split identifier\n",
        "test_df['split'] = 2 # Test split identifier\n",
        "\n",
        "# Combine all splits\n",
        "df_diseased = pd.concat([train_df, val_df, test_df], ignore_index=True) # Combine all splits into single dataframe\n",
        "\n",
        "# Create split name mapping\n",
        "split_mapping = {0: 'train', 1: 'val', 2: 'test'} # Map split IDs to names\n",
        "df_diseased['split_name'] = df_diseased['split'].map(split_mapping) # Add split names for clarity\n",
        "\n",
        "# Debug: Check split values\n",
        "print(\"DEBUG: Checking split values in diseased dataframe:\")\n",
        "print(f\" Total diseased samples: {len(df_diseased)}\")\n",
        "print(f\" Split value counts: {df_diseased['split'].value_counts()}\")\n",
        "print(f\" Split name counts: {df_diseased['split_name'].value_counts()}\")\n",
        "\n",
        "# Create absolute paths for images\n",
        "def create_absolute_path(relative_path):\n",
        "    \"\"\"Convert relative path to absolute path for image loading\"\"\"\n",
        "    if pd.isna(relative_path):\n",
        "        return None\n",
        "    # Remove any leading/trailing whitespace and quotes\n",
        "    clean_path = str(relative_path).strip().strip('\"\\'')\n",
        "    # Create absolute path\n",
        "    absolute_path = os.path.join(IMAGES_DIR, clean_path)\n",
        "    return absolute_path\n",
        "\n",
        "# Apply absolute path conversion\n",
        "df_diseased['image_path_absolute'] = df_diseased['image_path'].apply(create_absolute_path) # Convert to absolute paths\n",
        "\n",
        "# Filter out rows with invalid image paths\n",
        "df_diseased = df_diseased.dropna(subset=['image_path_absolute']) # Remove rows with missing image paths\n",
        "\n",
        "# Verify file existence\n",
        "df_diseased['file_exists'] = df_diseased['image_path_absolute'].apply(lambda x: os.path.exists(x) if x else False) # Check if files exist\n",
        "df_diseased = df_diseased[df_diseased['file_exists'] == True] # Keep only existing files\n",
        "\n",
        "print(f\"Valid image files found: {len(df_diseased)}\")\n",
        "\n",
        "# Create data generators for Stage 2 training\n",
        "def create_stage2_generators(df_diseased, image_size, batch_size):\n",
        "    \"\"\"Create data generators for Stage 2 training with proper categorical encoding\"\"\"\n",
        "\n",
        "    # Define augmentation parameters for precision agriculture\n",
        "    AUGMENTATION_PARAMS = {\n",
        "        'rotation_range': 20, # Rotation range in degrees\n",
        "        'width_shift_range': 0.2, # Width shift range as fraction of total width\n",
        "        'height_shift_range': 0.2, # Height shift range as fraction of total height\n",
        "        'shear_range': 0.2, # Shear intensity\n",
        "        'zoom_range': 0.2, # Zoom range\n",
        "        'horizontal_flip': True, # Enable horizontal flipping\n",
        "        'fill_mode': 'nearest', # Fill mode for augmented pixels\n",
        "        'validation_split': 0.0 # No validation split in generators\n",
        "    }\n",
        "\n",
        "    # Create training data generator with comprehensive augmentation\n",
        "    train_generator = ImageDataGenerator(\n",
        "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input, # MobileNetV2 preprocessing\n",
        "        rotation_range=AUGMENTATION_PARAMS['rotation_range'], # Rotation augmentation\n",
        "        width_shift_range=AUGMENTATION_PARAMS['width_shift_range'], # Width shift augmentation\n",
        "        height_shift_range=AUGMENTATION_PARAMS['height_shift_range'], # Height shift augmentation\n",
        "        shear_range=AUGMENTATION_PARAMS['shear_range'], # Shear augmentation\n",
        "        zoom_range=AUGMENTATION_PARAMS['zoom_range'], # Zoom augmentation\n",
        "        horizontal_flip=AUGMENTATION_PARAMS['horizontal_flip'], # Horizontal flip augmentation\n",
        "        fill_mode=AUGMENTATION_PARAMS['fill_mode'], # Fill mode for augmented pixels\n",
        "        validation_split=AUGMENTATION_PARAMS['validation_split'] # No validation split\n",
        "    )\n",
        "\n",
        "    # Create validation data generator (minimal augmentation)\n",
        "    val_generator = ImageDataGenerator(\n",
        "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input, # MobileNetV2 preprocessing\n",
        "        validation_split=0.0 # No validation split\n",
        "    )\n",
        "\n",
        "    # Create test data generator (no augmentation)\n",
        "    test_generator = ImageDataGenerator(\n",
        "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input, # MobileNetV2 preprocessing\n",
        "        validation_split=0.0 # No validation split\n",
        "    )\n",
        "\n",
        "    # Get training, validation, and test dataframes\n",
        "    train_df = df_diseased[df_diseased['split'] == 0] # Training samples\n",
        "    val_df = df_diseased[df_diseased['split'] == 1] # Validation samples\n",
        "    test_df = df_diseased[df_diseased['split'] == 2] # Test samples\n",
        "\n",
        "    # Convert encoded_class_id to string for categorical mode\n",
        "    train_df['encoded_class_id_str'] = train_df['encoded_class_id'].astype(str) # Convert to string\n",
        "    val_df['encoded_class_id_str'] = val_df['encoded_class_id'].astype(str) # Convert to string\n",
        "    test_df['encoded_class_id_str'] = test_df['encoded_class_id'].astype(str) # Convert to string\n",
        "\n",
        "    # Create generators with categorical mode and string labels\n",
        "    train_gen = train_generator.flow_from_dataframe(\n",
        "        dataframe=train_df, # Training dataframe\n",
        "        x_col='image_path_absolute', # Column containing image paths\n",
        "        y_col='encoded_class_id_str', # Column containing class labels as strings\n",
        "        target_size=image_size, # Target image size\n",
        "        batch_size=batch_size, # Batch size\n",
        "        class_mode='categorical', # Categorical mode for one-hot encoding\n",
        "        shuffle=True, # Shuffle training data\n",
        "        seed=42 # Random seed for reproducibility\n",
        "    )\n",
        "\n",
        "    val_gen = val_generator.flow_from_dataframe(\n",
        "        dataframe=val_df, # Validation dataframe\n",
        "        x_col='image_path_absolute', # Column containing image paths\n",
        "        y_col='encoded_class_id_str', # Column containing class labels as strings\n",
        "        target_size=image_size, # Target image size\n",
        "        batch_size=batch_size, # Batch size\n",
        "        class_mode='categorical', # Categorical mode for one-hot encoding\n",
        "        shuffle=False, # No shuffling for validation\n",
        "        seed=42 # Random seed for reproducibility\n",
        "    )\n",
        "\n",
        "    test_gen = test_generator.flow_from_dataframe(\n",
        "        dataframe=test_df, # Test dataframe\n",
        "        x_col='image_path_absolute', # Column containing image paths\n",
        "        y_col='encoded_class_id_str', # Column containing class labels as strings\n",
        "        target_size=image_size, # Target image size\n",
        "        batch_size=batch_size, # Batch size\n",
        "        class_mode='categorical', # Categorical mode for one-hot encoding\n",
        "        shuffle=False, # No shuffling for testing\n",
        "        seed=42 # Random seed for reproducibility\n",
        "    )\n",
        "\n",
        "    return train_gen, val_gen, test_gen\n",
        "\n",
        "# Create enhanced data generators for Stage 2 training\n",
        "train_generator, val_generator, test_generator = create_stage2_generators(\n",
        "    df_diseased, (IMG_HEIGHT, IMG_WIDTH), BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Display generator information\n",
        "print(f\"Stage 2 data generators created successfully:\")\n",
        "print(f\" Training samples: {len(train_generator) * BATCH_SIZE}\")\n",
        "print(f\" Validation samples: {len(val_generator) * BATCH_SIZE}\")\n",
        "print(f\" Test samples: {len(test_generator) * BATCH_SIZE}\")\n",
        "print(f\" Number of classes: {len(diseased_class_mapping)}\")\n",
        "\n",
        "# Create Google Drive directory if it doesn't exist\n",
        "DRIVE_MODELS_DIR = '/content/drive/MyDrive/Stage2_Enhanced_Models' # Google Drive models directory\n",
        "os.makedirs(DRIVE_MODELS_DIR, exist_ok=True) # Create directory if it doesn't exist\n",
        "\n",
        "# Save class mapping to Google Drive\n",
        "class_mapping_path = os.path.join(DRIVE_MODELS_DIR, 'stage2_class_mapping.json') # Path for class mapping\n",
        "with open(class_mapping_path, 'w') as f:\n",
        "    json.dump(diseased_class_mapping, f, indent=2) # Save class mapping as JSON\n",
        "\n",
        "print(f\"Class mapping saved to: {class_mapping_path}\")\n",
        "\n",
        "# Save final dataset to Google Drive\n",
        "final_dataset_path = os.path.join(DRIVE_MODELS_DIR, 'stage2_final_dataset.csv') # Save final dataset\n",
        "df_diseased.to_csv(final_dataset_path, index=False) # Save final dataset\n",
        "\n",
        "print(f\"Final dataset saved to: {final_dataset_path}\")\n",
        "\n",
        "print(\"Stage 2 data preparation completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5B_ZgzgC5un1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJoOFj10MezB"
      },
      "outputs": [],
      "source": [
        "# **FINAL FIX FOR CLASS COUNT DISPLAY**\n",
        "\n",
        "# Fix the num_classes to show the correct count for diseased classes only\n",
        "print(f\"\\nFinal verification:\")  # Print final verification header\n",
        "print(f\"  Total classes in dataset: {len(label_encoder.classes_)}\")  # Display total classes\n",
        "print(f\"  Diseased classes after filtering: {df_diseased['encoded_class_id'].nunique()}\")  # Display diseased class count\n",
        "\n",
        "# Update num_classes for the actual diseased classes\n",
        "num_classes = df_diseased['encoded_class_id'].nunique()  # Get actual diseased class count\n",
        "print(f\"  Corrected number of classes: {num_classes}\")  # Display corrected count\n",
        "\n",
        "# Get the unique class names in the order they appear\n",
        "unique_class_names = df_diseased['class_name'].unique()  # Get unique class names\n",
        "print(f\"  Unique diseased class names: {len(unique_class_names)}\")  # Display unique class count\n",
        "\n",
        "# Create a simple sequential mapping\n",
        "class_mapping = {  # Create updated class mapping dictionary\n",
        "    'num_classes': num_classes,  # Correct number of diseased classes\n",
        "    'class_indices': {str(i): i for i in range(num_classes)},  # Sequential class index mapping\n",
        "    'class_names': {str(i): unique_class_names[i] for i in range(num_classes)},  # Class name mapping by position\n",
        "    'label_encoder_classes': unique_class_names.tolist()  # Diseased class names only\n",
        "}\n",
        "\n",
        "# Save updated class mapping\n",
        "with open(class_mapping_path, 'w') as f:  # Open file for writing\n",
        "    json.dump(class_mapping, f, indent=4)  # Save updated class mapping\n",
        "\n",
        "print(f\"Updated class mapping saved with {num_classes} diseased classes\")  # Print update confirmation\n",
        "\n",
        "# Display first few class names for verification\n",
        "print(f\"\\nFirst 10 diseased classes:\")  # Print verification header\n",
        "for i in range(min(10, num_classes)):  # Show first 10 classes\n",
        "    print(f\"  Class {i}: {unique_class_names[i]}\")  # Display class mapping"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dP5ZEF4J58vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **VISUALIZE STAGE-2 DATASET: TRAIN/VAL/TEST DISTRIBUTION, IMAGES PER CLASS AND CLASS BALANCE**"
      ],
      "metadata": {
        "id": "IHuhI3BtOgz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNWesMhLNt6G"
      },
      "outputs": [],
      "source": [
        "# **VISUALIZE STAGE 2 DATASET: DISEASED CLASSES ANALYSIS AND SAMPLE IMAGES**\n",
        "\n",
        "# Import required libraries\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
        "import numpy as np # Import numpy for numerical operations\n",
        "import os # Import operating system interface\n",
        "import json # Import JSON handling for saving analysis\n",
        "\n",
        "# Set style for professional visualizations\n",
        "plt.style.use('default') # Use default matplotlib style\n",
        "\n",
        "def visualize_stage2_dataset_overview(df_diseased, num_images_per_class=3, max_classes_to_show=20):\n",
        "    \"\"\"Visualize Stage 2 dataset with sample images per diseased class and class distribution for precision agriculture\"\"\"\n",
        "\n",
        "    print(\"=\"*60)  # Print separator line\n",
        "    print(\" STAGE 2 DATASET VISUALIZATION FOR PRECISION AGRICULTURE\")  # Print section header\n",
        "    print(\"=\"*60)  # Print separator line\n",
        "\n",
        "    # 1. Diseased Class Distribution Analysis\n",
        "    print(\"\\n DISEASED CLASS DISTRIBUTION ANALYSIS\")  # Print subsection header\n",
        "    print(\"-\" * 50)  # Print subsection separator\n",
        "\n",
        "    # Get class counts for each split - FIXED: Use split_name instead of split\n",
        "    train_counts = df_diseased[df_diseased['split_name'] == 'train']['class_name'].value_counts()  # Count classes in training set\n",
        "    val_counts = df_diseased[df_diseased['split_name'] == 'val']['class_name'].value_counts()  # Count classes in validation set\n",
        "    test_counts = df_diseased[df_diseased['split_name'] == 'test']['class_name'].value_counts()  # Count classes in test set\n",
        "\n",
        "    print(f\"Total diseased classes: {len(df_diseased['class_name'].unique())}\")  # Display total number of unique diseased classes\n",
        "    print(f\"Total diseased images: {len(df_diseased)}\")  # Display total number of diseased images\n",
        "    print(f\"Train images: {len(df_diseased[df_diseased['split_name'] == 'train'])}\")  # Display number of training images\n",
        "    print(f\"Validation images: {len(df_diseased[df_diseased['split_name'] == 'val'])}\")  # Display number of validation images\n",
        "    print(f\"Test images: {len(df_diseased[df_diseased['split_name'] == 'test'])}\")  # Display number of test images\n",
        "\n",
        "    # 2. Diseased Class Balance Visualization\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(16, 12))  # Create figure with 2 subplots\n",
        "\n",
        "    # Top diseased classes by total count\n",
        "    all_counts = df_diseased['class_name'].value_counts()  # Get counts for all diseased classes\n",
        "    top_classes = all_counts.head(max_classes_to_show)  # Select top classes to display\n",
        "\n",
        "    # Create stacked bar chart for diseased classes\n",
        "    x_pos = np.arange(len(top_classes))  # Create x-axis positions\n",
        "    width = 0.25  # Set bar width\n",
        "\n",
        "    train_vals = [train_counts.get(cls, 0) for cls in top_classes.index]  # Get training counts for top classes\n",
        "    val_vals = [val_counts.get(cls, 0) for cls in top_classes.index]  # Get validation counts for top classes\n",
        "    test_vals = [test_counts.get(cls, 0) for cls in top_classes.index]  # Get test counts for top classes\n",
        "\n",
        "    axes[0].bar(x_pos - width, train_vals, width, label='Train', color='#2E86AB', alpha=0.8)  # Plot training bars\n",
        "    axes[0].bar(x_pos, val_vals, width, label='Validation', color='#A23B72', alpha=0.8)  # Plot validation bars\n",
        "    axes[0].bar(x_pos + width, test_vals, width, label='Test', color='#F18F01', alpha=0.8)  # Plot test bars\n",
        "\n",
        "    axes[0].set_xlabel('Diseased Classes', fontsize=12, fontweight='bold')  # Set x-axis label\n",
        "    axes[0].set_ylabel('Number of Images', fontsize=12, fontweight='bold')  # Set y-axis label\n",
        "    axes[0].set_title(f'Diseased Class Distribution - Top {max_classes_to_show} Classes', fontsize=14, fontweight='bold')  # Set title\n",
        "    axes[0].set_xticks(x_pos)  # Set x-axis tick positions\n",
        "    axes[0].set_xticklabels([cls[:20] + '...' if len(cls) > 20 else cls for cls in top_classes.index],  # Set x-axis labels\n",
        "                           rotation=45, ha='right')  # Rotate labels for readability\n",
        "    axes[0].legend()  # Add legend\n",
        "    axes[0].grid(True, alpha=0.3)  # Add grid\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, (train, val, test) in enumerate(zip(train_vals, val_vals, test_vals)):  # Iterate through bar values\n",
        "        if train > 0:  # If training count is greater than 0\n",
        "            axes[0].text(i - width, train + 5, str(train), ha='center', va='bottom', fontsize=8)  # Add training count label\n",
        "        if val > 0:  # If validation count is greater than 0\n",
        "            axes[0].text(i, val + 5, str(val), ha='center', va='bottom', fontsize=8)  # Add validation count label\n",
        "        if test > 0:  # If test count is greater than 0\n",
        "            axes[0].text(i + width, test + 5, str(test), ha='center', va='bottom', fontsize=8)  # Add test count label\n",
        "\n",
        "    # 3. Class Balance Analysis for Precision Agriculture\n",
        "    # Calculate balance threshold (classes above mean are considered balanced)\n",
        "    mean_samples = all_counts.mean()  # Calculate mean samples per class\n",
        "    balanced_classes = int(sum(all_counts >= mean_samples))  # Count balanced classes\n",
        "    imbalanced_classes = int(len(all_counts) - balanced_classes)  # Count imbalanced classes\n",
        "\n",
        "    balance_data = [balanced_classes, imbalanced_classes]  # Prepare data for pie chart\n",
        "    balance_labels = ['Balanced Classes', 'Imbalanced Classes']  # Labels for pie chart\n",
        "    balance_colors = ['#4CAF50', '#F44336']  # Green for balanced, Red for imbalanced\n",
        "\n",
        "    axes[1].pie(balance_data, labels=balance_labels, colors=balance_colors, autopct='%1.1f%%', startangle=90)  # Create pie chart\n",
        "    axes[1].set_title('Class Balance Analysis for Early Disease Detection', fontsize=14, fontweight='bold')  # Set title\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "    plt.savefig(os.path.join(DRIVE_MODELS_DIR, 'stage2_class_distribution.png'), dpi=300, bbox_inches='tight')  # Save high-quality image\n",
        "    plt.show()  # Display the plot\n",
        "\n",
        "    # 4. Sample Images Per Diseased Class - FIXED: Show 5 samples per row\n",
        "    print(f\"\\n SAMPLE IMAGES PER DISEASED CLASS (showing first {max_classes_to_show} classes)\")  # Print subsection header\n",
        "    print(\"-\" * 70)  # Print subsection separator\n",
        "\n",
        "    # Get sample images for each diseased class\n",
        "    sample_images = {}  # Initialize dictionary for sample images\n",
        "    for class_name in top_classes.index[:max_classes_to_show]:  # Iterate through top diseased classes\n",
        "        class_df = df_diseased[df_diseased['class_name'] == class_name]  # Filter data for current class\n",
        "        if len(class_df) >= num_images_per_class:  # Check if enough images available\n",
        "            samples = class_df.sample(n=num_images_per_class)  # Randomly sample images\n",
        "            sample_images[class_name] = samples  # Store samples\n",
        "\n",
        "    # Create visualization grid for diseased class samples - FIXED: 5 samples per row\n",
        "    num_classes_to_show = min(len(sample_images), max_classes_to_show)  # Determine number of classes to show\n",
        "    num_cols = 5  # Set number of columns to 5 as requested\n",
        "    num_rows = int(np.ceil(num_classes_to_show * num_images_per_class / num_cols))  # Calculate required rows\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 4*num_rows))  # Create subplot grid with 5 columns\n",
        "\n",
        "    if num_rows == 1:  # Handle single row case\n",
        "        axes = axes.reshape(1, -1)  # Reshape axes for single row\n",
        "    elif num_cols == 1:  # Handle single column case\n",
        "        axes = axes.reshape(-1, 1)  # Reshape axes for single column\n",
        "\n",
        "    # Flatten axes for easier iteration\n",
        "    axes_flat = axes.flatten() if hasattr(axes, 'flatten') else [axes] if num_rows == 1 and num_cols == 1 else axes.flatten()\n",
        "\n",
        "    # Counter for placing images\n",
        "    img_counter = 0  # Initialize image counter\n",
        "\n",
        "    for class_name, samples in list(sample_images.items())[:num_classes_to_show]:  # Iterate through classes\n",
        "        for _, sample in samples.iterrows():  # Iterate through samples for each class\n",
        "            if img_counter < len(axes_flat):  # Check if we have enough subplot space\n",
        "                ax = axes_flat[img_counter]  # Get current subplot\n",
        "\n",
        "                try:\n",
        "                    # FIXED: Use 'image_path_absolute' instead of 'full_image_path'\n",
        "                    img_path = sample['image_path_absolute']  # Get absolute image path\n",
        "                    if os.path.exists(img_path):  # Check if file exists\n",
        "                        # Try PIL first (most reliable)\n",
        "                        try:\n",
        "                            from PIL import Image  # Import PIL for image loading\n",
        "                            img = Image.open(img_path)  # Open image with PIL\n",
        "                            img = img.convert('RGB')  # Convert to RGB format\n",
        "                            img_array = np.array(img)  # Convert to numpy array\n",
        "                        except Exception as e:\n",
        "                            # Fallback to matplotlib if PIL fails\n",
        "                            img_array = plt.imread(img_path)  # Load image from path\n",
        "\n",
        "                        ax.imshow(img_array)  # Display image\n",
        "\n",
        "                        # Set title with class name and split\n",
        "                        split_name = sample['split_name']  # Get split name\n",
        "                        title_text = f\"{class_name[:15]}...\\n{split_name}\"  # Create title text\n",
        "                        ax.set_title(title_text, fontsize=8, fontweight='bold')  # Set subplot title\n",
        "                        ax.axis('off')  # Hide axes\n",
        "\n",
        "                        # Add border color based on split (different colors for train/val/test)\n",
        "                        if split_name == 'train':  # If image is from training set\n",
        "                            border_color = '#2E86AB'  # Blue for training\n",
        "                        elif split_name == 'val':  # If image is from validation set\n",
        "                            border_color = '#A23B72'  # Purple for validation\n",
        "                        else:  # If image is from test set\n",
        "                            border_color = '#F18F01'  # Orange for test\n",
        "\n",
        "                        # Apply border color to all spines\n",
        "                        for spine in ax.spines.values():  # Iterate through all spines\n",
        "                            spine.set_color(border_color)  # Set spine color\n",
        "                            spine.set_linewidth(3)  # Set spine width\n",
        "\n",
        "                    else:  # If file doesn't exist\n",
        "                        ax.text(0.5, 0.5, 'File\\nNot Found',  # Display file not found message\n",
        "                               ha='center', va='center', fontsize=10, fontweight='bold', color='red')\n",
        "                        ax.axis('off')  # Hide axes\n",
        "\n",
        "                except Exception as e:  # Handle image loading errors\n",
        "                    ax.text(0.5, 0.5, 'Error\\nLoading Image',  # Display error message\n",
        "                           ha='center', va='center', fontsize=10, fontweight='bold', color='red')\n",
        "                    ax.axis('off')  # Hide axes\n",
        "\n",
        "                img_counter += 1  # Increment image counter\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for i in range(img_counter, len(axes_flat)):  # Iterate through unused subplots\n",
        "        axes_flat[i].axis('off')  # Hide unused subplots\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "    plt.savefig(os.path.join(DRIVE_MODELS_DIR, 'stage2_sample_images_per_class.png'), dpi=300, bbox_inches='tight')  # Save high-quality image\n",
        "    plt.show()  # Display the plot\n",
        "\n",
        "    # 5. Summary Statistics for Precision Agriculture\n",
        "    print(f\"\\n SUMMARY STATISTICS FOR PRECISION AGRICULTURE\")  # Print subsection header\n",
        "    print(\"-\" * 50)  # Print subsection separator\n",
        "    print(f\"Most common diseased class: {all_counts.index[0]} ({all_counts.iloc[0]} images)\")  # Display most common diseased class\n",
        "    print(f\"Least common diseased class: {all_counts.index[-1]} ({all_counts.iloc[-1]} images)\")  # Display least common diseased class\n",
        "    print(f\"Average images per diseased class: {all_counts.mean():.1f}\")  # Display average images per diseased class\n",
        "    print(f\"Standard deviation: {all_counts.std():.1f}\")  # Display standard deviation\n",
        "    print(f\"Diseased classes with < 10 images: {(all_counts < 10).sum()}\")  # Count diseased classes with few images\n",
        "    print(f\"Diseased classes with > 100 images: {(all_counts > 100).sum()}\")  # Count diseased classes with many images\n",
        "\n",
        "    # Class balance statistics for precision agriculture\n",
        "    print(f\"\\nClass balance analysis:\")  # Print balance analysis header\n",
        "    print(f\"  Balanced classes ({mean_samples:.1f} samples): {balanced_classes} ({balanced_classes/len(all_counts)*100:.1f}%)\")  # Display balanced class statistics\n",
        "    print(f\"  Imbalanced classes (<{mean_samples:.1f} samples): {imbalanced_classes} ({imbalanced_classes/len(all_counts)*100:.1f}%)\")  # Display imbalanced class statistics\n",
        "\n",
        "    # Split distribution statistics\n",
        "    print(f\"\\nSplit distribution:\")  # Print split distribution header\n",
        "    split_counts = df_diseased['split_name'].value_counts()  # Count images in each split\n",
        "    for split_name, count in split_counts.items():  # Iterate through splits\n",
        "        percentage = count / len(df_diseased) * 100  # Calculate percentage\n",
        "        print(f\"  {split_name.capitalize()}: {count} images ({percentage:.1f}%)\")  # Display split statistics\n",
        "\n",
        "    print(f\"\\n Visualizations saved to Google Drive:\")  # Print save confirmation\n",
        "    print(f\"   Class distribution: {os.path.join(DRIVE_MODELS_DIR, 'stage2_class_distribution.png')}\")  # Display class distribution file path\n",
        "    print(f\"   Sample images: {os.path.join(DRIVE_MODELS_DIR, 'stage2_sample_images_per_class.png')}\")  # Display sample images file path\n",
        "\n",
        "    # Return analysis results for further processing\n",
        "    return {\n",
        "        'total_classes': int(len(all_counts)),  # Convert to Python int\n",
        "        'total_samples': int(len(df_diseased)),  # Convert to Python int\n",
        "        'mean_samples_per_class': float(all_counts.mean()),  # Convert to Python float\n",
        "        'median_samples_per_class': float(all_counts.median()),  # Convert to Python float\n",
        "        'balanced_classes': int(balanced_classes),  # Convert to Python int\n",
        "        'imbalanced_classes': int(imbalanced_classes),  # Convert to Python float\n",
        "        'class_balance_ratio': float(balanced_classes / len(all_counts))  # Convert to Python float\n",
        "    }\n",
        "\n",
        "# **RUN THE STAGE 2 VISUALIZATION**\n",
        "print(\"Creating enhanced Stage 2 dataset visualizations for precision agriculture analysis...\")\n",
        "dataset_analysis = visualize_stage2_dataset_overview(df_diseased, num_images_per_class=3, max_classes_to_show=20)\n",
        "\n",
        "# Display analysis summary for precision agriculture planning\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STAGE 2 DATASET ANALYSIS SUMMARY FOR PRECISION AGRICULTURE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total diseased classes: {dataset_analysis['total_classes']}\")\n",
        "print(f\"Total diseased samples: {dataset_analysis['total_samples']:,}\")\n",
        "print(f\"Average samples per class: {dataset_analysis['mean_samples_per_class']:.1f}\")\n",
        "print(f\"Class balance ratio: {dataset_analysis['class_balance_ratio']:.2%}\")\n",
        "print(f\"Classes requiring augmentation: {dataset_analysis['imbalanced_classes']}\")\n",
        "\n",
        "# Save analysis results to Google Drive for future reference\n",
        "analysis_path = os.path.join(DRIVE_MODELS_DIR, 'stage2_dataset_analysis.json')\n",
        "with open(analysis_path, 'w') as f:\n",
        "    json.dump(dataset_analysis, f, indent=4)  # Save analysis results as JSON\n",
        "\n",
        "print(f\"Dataset analysis saved to: {analysis_path}\")\n",
        "print(\"Stage 2 dataset visualization completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL TRAINING**"
      ],
      "metadata": {
        "id": "JDnjcABVO0Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 2: Single-model MobileNetV2 training cell (with class order) =====\n",
        "import os, json, numpy as np, pandas as pd, tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# -----------------------------\n",
        "# sanity checks / defaults\n",
        "# -----------------------------\n",
        "assert 'df_diseased' in globals(), \"df_diseased not found. Run dataset-prep cell first.\"\n",
        "need_cols = {'image_path_absolute','encoded_class_id','split_name'}\n",
        "missing = need_cols - set(df_diseased.columns)\n",
        "assert not missing, f\"df_diseased is missing columns: {missing}\"\n",
        "\n",
        "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_PHASE1 = 20\n",
        "EPOCHS_PHASE2 = 15\n",
        "PATIENCE = 10\n",
        "FACTOR = 0.5\n",
        "MIN_LR = 1e-7\n",
        "DROPOUT_RATE = 0.5\n",
        "L2_REG = 1e-4\n",
        "MODEL_TAG = \"stage2_mnv2\"\n",
        "DRIVE_MODELS_DIR = '/content/drive/MyDrive/Stage2_Enhanced_Models'\n",
        "os.makedirs(DRIVE_MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Generators with IDENTICAL class order\n",
        "# -----------------------------\n",
        "df_diseased = df_diseased.copy()\n",
        "df_diseased['encoded_class_id_str'] = df_diseased['encoded_class_id'].astype(str)\n",
        "\n",
        "num_classes = int(df_diseased['encoded_class_id'].nunique())\n",
        "classes_fixed = [str(i) for i in range(num_classes)]  # identical order everywhere\n",
        "\n",
        "train_df = df_diseased[df_diseased['split_name'] == 'train'].copy()\n",
        "val_df   = df_diseased[df_diseased['split_name'] == 'val'].copy()\n",
        "test_df  = df_diseased[df_diseased['split_name'] == 'test'].copy()\n",
        "\n",
        "train_idg = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
        "    rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
        "    shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest'\n",
        ")\n",
        "plain_idg = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
        ")\n",
        "\n",
        "train_gen = train_idg.flow_from_dataframe(\n",
        "    train_df, x_col='image_path_absolute', y_col='encoded_class_id_str',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=True, seed=42, classes=classes_fixed\n",
        ")\n",
        "val_gen = plain_idg.flow_from_dataframe(\n",
        "    val_df, x_col='image_path_absolute', y_col='encoded_class_id_str',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False, seed=42, classes=classes_fixed\n",
        ")\n",
        "test_gen = plain_idg.flow_from_dataframe(\n",
        "    test_df, x_col='image_path_absolute', y_col='encoded_class_id_str',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False, seed=42, classes=classes_fixed\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {train_gen.n}\")\n",
        "print(f\"Val samples:   {val_gen.n}\")\n",
        "print(f\"Test samples:  {test_gen.n}\")\n",
        "print(f\"Num classes:   {num_classes}\")\n",
        "print(\"class_indices consistent across splits:\",\n",
        "      train_gen.class_indices == val_gen.class_indices == test_gen.class_indices)\n",
        "\n",
        "# -----------------------------\n",
        "# Class weights (inverse frequency on encoded_class_id)\n",
        "# -----------------------------\n",
        "counts = train_df['encoded_class_id'].value_counts().sort_index()\n",
        "total = counts.sum()\n",
        "class_weights = {i: float(total / (num_classes * counts.get(i, 1))) for i in range(num_classes)}\n",
        "print(f\"class_weight range: {min(class_weights.values()):.4f} -> {max(class_weights.values()):.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Model (do NOT preprocess again inside the model; generators already do it)\n",
        "# -----------------------------\n",
        "def build_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=num_classes,\n",
        "                dropout=DROPOUT_RATE, l2_reg=L2_REG):\n",
        "    base = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base.trainable = False  # Phase 1\n",
        "\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = base(inputs, training=False)              # features from frozen base\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
        "    x = BatchNormalization()(x); x = Dropout(dropout)(x)\n",
        "    x = Dense(512, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
        "    x = BatchNormalization()(x); x = Dropout(dropout)(x)\n",
        "    x = Dense(256, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
        "    x = BatchNormalization()(x); x = Dropout(dropout/2)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax', kernel_regularizer=l2(l2_reg))(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model, base\n",
        "\n",
        "model, base_model = build_model()\n",
        "\n",
        "# -----------------------------\n",
        "# Callbacks\n",
        "# -----------------------------\n",
        "phase1_best = os.path.join(DRIVE_MODELS_DIR, f\"{MODEL_TAG}_phase1_best.keras\")\n",
        "phase2_best = os.path.join(DRIVE_MODELS_DIR, f\"{MODEL_TAG}_phase2_best.keras\")\n",
        "history_csv1 = os.path.join(DRIVE_MODELS_DIR, f\"{MODEL_TAG}_phase1_history.csv\")\n",
        "history_csv2 = os.path.join(DRIVE_MODELS_DIR, f\"{MODEL_TAG}_phase2_history.csv\")\n",
        "\n",
        "def make_callbacks(best_path, csv_path):\n",
        "    return [\n",
        "        EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True, verbose=1),\n",
        "        ModelCheckpoint(best_path, monitor='val_loss', save_best_only=True, verbose=1),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=FACTOR, patience=max(1, PATIENCE//2),\n",
        "                          min_lr=MIN_LR, verbose=1),\n",
        "        CSVLogger(csv_path, append=False)\n",
        "    ]\n",
        "\n",
        "# -----------------------------\n",
        "# Phase 1: train head\n",
        "# -----------------------------\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy',\n",
        "             tf.keras.metrics.Precision(name='precision'),\n",
        "             tf.keras.metrics.Recall(name='recall'),\n",
        "             tf.keras.metrics.AUC(name='auc')]\n",
        ")\n",
        "print(\"\\n=== Phase 1: training with frozen base ===\")\n",
        "hist1 = model.fit(\n",
        "    train_gen, validation_data=val_gen, epochs=EPOCHS_PHASE1,\n",
        "    callbacks=make_callbacks(phase1_best, history_csv1),\n",
        "    class_weight=class_weights, verbose=1\n",
        ")\n",
        "phase1_final = os.path.join(DRIVE_MODELS_DIR, f\"{MODEL_TAG}_phase1_final.keras\")\n",
        "model.save(phase1_final)\n",
        "print(\"Saved:\", phase1_final)\n",
        "\n",
        "# -----------------------------\n",
        "# Phase 2: fine-tune whole base (lower LR)\n",
        "# -----------------------------\n",
        "base_model.trainable = True\n",
        "# optionally: fine-tune last N layers only by setting .trainable for subsets\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy',\n",
        "             tf.keras.metrics.Precision(name='precision'),\n",
        "             tf.keras.metrics.Recall(name='recall'),\n",
        "             tf.keras.metrics.AUC(name='auc')]\n",
        ")\n",
        "print(\"\\n=== Phase 2: fine-tuning entire model ===\")\n",
        "hist2 = model.fit(\n",
        "    train_gen, validation_data=val_gen, epochs=EPOCHS_PHASE2,\n",
        "    callbacks=make_callbacks(phase2_best, history_csv2),\n",
        "    class_weight=class_weights, verbose=1\n",
        ")\n",
        "phase2_final = os.path.join(DRIVE_MODELS_DIR, f\"{MODEL_TAG}_phase2_final.keras\")\n",
        "model.save(phase2_final)\n",
        "print(\"Saved:\", phase2_final)\n",
        "\n",
        "# -----------------------------\n",
        "# Save combined history + class_indices for reproducibility\n",
        "# -----------------------------\n",
        "combined_history = {'phase1': hist1.history, 'phase2': hist2.history}\n",
        "with open(os.path.join(DRIVE_MODELS_DIR, f\"{MODEL_TAG}_history.json\"), \"w\") as f:\n",
        "    json.dump(combined_history, f, indent=2)\n",
        "with open(os.path.join(DRIVE_MODELS_DIR, f\"{MODEL_TAG}_class_indices.json\"), \"w\") as f:\n",
        "    json.dump(train_gen.class_indices, f, indent=2)\n",
        "\n",
        "print(\"\\nDone. Best checkpoints:\")\n",
        "print(\" -\", phase1_best)\n",
        "print(\" -\", phase2_best)\n"
      ],
      "metadata": {
        "id": "Akap0qpA0Ptt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 2: SAFE RESUME (hardened single cell) =====\n",
        "import os, json, math, numpy as np, pandas as pd, tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
        "\n",
        "# -----------------------------\n",
        "# paths & knobs\n",
        "# -----------------------------\n",
        "DRIVE_MODELS_DIR   = \"/content/drive/MyDrive/Stage2_Enhanced_Models\"\n",
        "FINAL_DATASET_CSV  = os.path.join(DRIVE_MODELS_DIR, \"stage2_final_dataset.csv\")  # written by your prep cell\n",
        "CLASS_INDICES_JSON = os.path.join(DRIVE_MODELS_DIR, \"stage2_mnv2_class_indices.json\")  # saved by your training cell\n",
        "MODEL_TAG          = \"stage2_mnv2\"\n",
        "RESUME_CANDIDATES  = [\n",
        "    f\"{MODEL_TAG}_phase2_best.keras\",\n",
        "    f\"{MODEL_TAG}_phase2_final.keras\",\n",
        "    f\"{MODEL_TAG}_phase1_best.keras\",\n",
        "    f\"{MODEL_TAG}_phase1_final.keras\",\n",
        "]\n",
        "IMG_SIZE     = (224, 224)\n",
        "BATCH_SIZE   = 32\n",
        "EPOCHS_RESUME= 10         # adjust if you want longer\n",
        "BASE_LR      = 5e-5       # warm restart LR (small)\n",
        "WEIGHT_DECAY = 1e-4\n",
        "PATIENCE     = 6\n",
        "FACTOR       = 0.5\n",
        "MIN_LR       = 1e-7\n",
        "\n",
        "# -----------------------------\n",
        "# 0) bring df_diseased into memory (load if needed)\n",
        "# -----------------------------\n",
        "if \"df_diseased\" not in globals():\n",
        "    assert os.path.exists(FINAL_DATASET_CSV), \"Can't find df_diseased or stage2_final_dataset.csv.\"\n",
        "    df_diseased = pd.read_csv(FINAL_DATASET_CSV)\n",
        "\n",
        "# -----------------------------\n",
        "# 1) recreate helper columns if missing\n",
        "# -----------------------------\n",
        "if \"encoded_class_id_str\" not in df_diseased.columns:\n",
        "    assert \"encoded_class_id\" in df_diseased.columns, \"encoded_class_id missing in df_diseased.\"\n",
        "    df_diseased[\"encoded_class_id_str\"] = df_diseased[\"encoded_class_id\"].astype(str)\n",
        "\n",
        "if \"split_name\" not in df_diseased.columns:\n",
        "    assert \"split\" in df_diseased.columns, \"split/split_name missing in df_diseased.\"\n",
        "    df_diseased[\"split_name\"] = df_diseased[\"split\"].map({0:\"train\",1:\"val\",2:\"test\"})\n",
        "\n",
        "if \"image_path_absolute\" not in df_diseased.columns:\n",
        "    # derive from dataset root used earlier\n",
        "    IMAGES_DIR = \"/content/plantwild/plantwild/images\"\n",
        "    df_diseased[\"image_path_absolute\"] = df_diseased[\"image_path\"].apply(lambda p: os.path.join(IMAGES_DIR, str(p)))\n",
        "\n",
        "# basic sanity\n",
        "need_cols = {\"image_path_absolute\",\"encoded_class_id\",\"encoded_class_id_str\",\"split_name\"}\n",
        "missing = need_cols - set(df_diseased.columns)\n",
        "assert not missing, f\"df_diseased missing required cols: {missing}\"\n",
        "\n",
        "# -----------------------------\n",
        "# 2) load class order exactly as used during training\n",
        "# -----------------------------\n",
        "assert os.path.exists(CLASS_INDICES_JSON), f\"Missing {CLASS_INDICES_JSON}. Run the training cell once to create it.\"\n",
        "with open(CLASS_INDICES_JSON, \"r\") as f:\n",
        "    saved_ci = json.load(f)  # {\"0\":0,\"1\":1,...}\n",
        "# rebuild the list of classes in index order\n",
        "classes_in_order = [None]*len(saved_ci)\n",
        "for lbl, idx in saved_ci.items():\n",
        "    classes_in_order[int(idx)] = str(lbl)\n",
        "\n",
        "# extra guard: ensure list is contiguous and complete\n",
        "assert all(x is not None for x in classes_in_order), \"Class indices JSON is incomplete.\"\n",
        "\n",
        "# -----------------------------\n",
        "# 3) build generators with the *saved* class order\n",
        "# -----------------------------\n",
        "train_df = df_diseased[df_diseased[\"split_name\"]==\"train\"].copy()\n",
        "val_df   = df_diseased[df_diseased[\"split_name\"]==\"val\"].copy()\n",
        "test_df  = df_diseased[df_diseased[\"split_name\"]==\"test\"].copy()\n",
        "\n",
        "train_idg = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
        "    rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
        "    shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=\"nearest\"\n",
        ")\n",
        "plain_idg = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
        ")\n",
        "\n",
        "train_gen = train_idg.flow_from_dataframe(\n",
        "    train_df, x_col=\"image_path_absolute\", y_col=\"encoded_class_id_str\",\n",
        "    target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"categorical\",\n",
        "    shuffle=True, seed=42, classes=classes_in_order\n",
        ")\n",
        "val_gen = plain_idg.flow_from_dataframe(\n",
        "    val_df, x_col=\"image_path_absolute\", y_col=\"encoded_class_id_str\",\n",
        "    target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"categorical\",\n",
        "    shuffle=False, seed=42, classes=classes_in_order\n",
        ")\n",
        "test_gen = plain_idg.flow_from_dataframe(\n",
        "    test_df, x_col=\"image_path_absolute\", y_col=\"encoded_class_id_str\",\n",
        "    target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"categorical\",\n",
        "    shuffle=False, seed=42, classes=classes_in_order\n",
        ")\n",
        "\n",
        "print(\"Class order locked to saved mapping:\",\n",
        "      train_gen.class_indices == val_gen.class_indices == test_gen.class_indices)\n",
        "\n",
        "num_classes = len(classes_in_order)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) compute class weights (inverse freq on encoded_class_id)\n",
        "# -----------------------------\n",
        "counts = train_df[\"encoded_class_id\"].value_counts().sort_index()\n",
        "total  = counts.sum()\n",
        "class_weights = {i: float(total / (num_classes * counts.get(i, 1))) for i in range(num_classes)}\n",
        "print(f\"class_weight range: {min(class_weights.values()):.4f} -> {max(class_weights.values()):.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5) locate a checkpoint to resume from\n",
        "# -----------------------------\n",
        "resume_path = None\n",
        "for name in RESUME_CANDIDATES:\n",
        "    p = os.path.join(DRIVE_MODELS_DIR, name)\n",
        "    if os.path.exists(p):\n",
        "        resume_path = p\n",
        "        break\n",
        "assert resume_path is not None, f\"No resume checkpoint found in {DRIVE_MODELS_DIR}.\"\n",
        "print(\"Resuming from:\", resume_path)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) load model & set trainable policy (warm restart)\n",
        "# -----------------------------\n",
        "model = tf.keras.models.load_model(resume_path, compile=False)\n",
        "\n",
        "# unfreeze entire network for fine-tuning (safe even if already unfrozen)\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# recompile with small LR (warm restart)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(learning_rate=BASE_LR, weight_decay=WEIGHT_DECAY),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\",\n",
        "             tf.keras.metrics.Precision(name=\"precision\"),\n",
        "             tf.keras.metrics.Recall(name=\"recall\"),\n",
        "             tf.keras.metrics.AUC(name=\"auc\")]\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 7) callbacks\n",
        "# -----------------------------\n",
        "resume_tag   = f\"{MODEL_TAG}_resume\"\n",
        "best_ckpt    = os.path.join(DRIVE_MODELS_DIR, f\"{resume_tag}_best.keras\")\n",
        "history_csv  = os.path.join(DRIVE_MODELS_DIR, f\"{resume_tag}_history.csv\")\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint(best_ckpt, monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=FACTOR, patience=max(1, PATIENCE//2),\n",
        "                      min_lr=MIN_LR, verbose=1),\n",
        "    CSVLogger(history_csv, append=True)\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# 8) resume training\n",
        "# -----------------------------\n",
        "print(\"\\n=== Resuming fine-tuning (warm restart) ===\")\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=EPOCHS_RESUME,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# save a final snapshot too\n",
        "final_path = os.path.join(DRIVE_MODELS_DIR, f\"{resume_tag}_final.keras\")\n",
        "model.save(final_path)\n",
        "print(\"Saved final snapshot:\", final_path)\n",
        "print(\"Best checkpoint:\", best_ckpt)\n"
      ],
      "metadata": {
        "id": "_Ajl8sMhPDFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COMPREHENSIVE MODEL EVALUATION AND ANALYSIS**"
      ],
      "metadata": {
        "id": "8Y--WEhTPNKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== # **STAGE 2 COMPREHENSIVE EVALUATION FRAMEWORK**  ====\n",
        "import os, json, math, itertools, numpy as np, pandas as pd, tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_auc_score,\n",
        "                             average_precision_score, precision_recall_fscore_support,\n",
        "                             log_loss, precision_recall_curve)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Paths (feel free to tweak)\n",
        "DRIVE_MODELS_DIR   = '/content/drive/MyDrive/Stage2_Enhanced_Models'\n",
        "DRIVE_ANALYSIS_DIR = '/content/drive/MyDrive/Stage2_Enhanced_Analysis'\n",
        "DRIVE_GRADCAM_DIR  = '/content/drive/MyDrive/Stage2_Enhanced_GradCAM'\n",
        "\n",
        "os.makedirs(DRIVE_ANALYSIS_DIR, exist_ok=True)\n",
        "os.makedirs(DRIVE_GRADCAM_DIR, exist_ok=True)\n",
        "\n",
        "# Best model to evaluate (from your logs)\n",
        "BEST_MODEL_PATH  = os.path.join(DRIVE_MODELS_DIR, 'stage2_mnv2_resume_best.keras')\n",
        "SAVEDMODEL_DIR   = os.path.join(DRIVE_MODELS_DIR, 'stage2_mnv2_savedmodel')\n",
        "CLASS_INDEX_JSON = os.path.join(DRIVE_MODELS_DIR, 'stage2_mnv2_class_indices.json')\n",
        "\n",
        "# Outputs\n",
        "PREDICTIONS_CSV  = os.path.join(DRIVE_ANALYSIS_DIR, 'stage2_test_predictions.csv')\n",
        "CLASS_REPORT_CSV = os.path.join(DRIVE_ANALYSIS_DIR, 'stage2_classification_report.csv')\n",
        "METRICS_JSON     = os.path.join(DRIVE_ANALYSIS_DIR, 'stage2_metrics.json')\n",
        "\n",
        "IMG_SIZE   = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print(\"Ready. Edit BEST_MODEL_PATH above if needed.\")\n"
      ],
      "metadata": {
        "id": "as1V8Imo0Pdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **Multi-Class Disease Classification Analysis for Precision Agriculture**\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_auc_score,\n",
        "                             average_precision_score, precision_recall_fscore_support,\n",
        "                             log_loss, precision_recall_curve, cohen_kappa_score)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\" STAGE 2 COMPREHENSIVE EVALUATION FRAMEWORK\")\n",
        "print(\" Multi-Class Disease Classification Analysis\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Configuration\n",
        "DRIVE_MODELS_DIR = '/content/drive/MyDrive/Stage2_Enhanced_Models'\n",
        "DRIVE_ANALYSIS_DIR = '/content/drive/MyDrive/Stage2_Enhanced_Analysis'\n",
        "DRIVE_VISUALIZATIONS_DIR = '/content/drive/MyDrive/Stage2_Enhanced_Visualizations'\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(DRIVE_ANALYSIS_DIR, exist_ok=True)\n",
        "os.makedirs(DRIVE_VISUALIZATIONS_DIR, exist_ok=True)\n",
        "\n",
        "# Load your trained model\n",
        "BEST_MODEL_PATH = os.path.join(DRIVE_MODELS_DIR, 'stage2_mnv2_resume_best.keras')\n",
        "if os.path.exists(BEST_MODEL_PATH):\n",
        "    print(f\" Loading model: {BEST_MODEL_PATH}\")\n",
        "    model = tf.keras.models.load_model(BEST_MODEL_PATH)\n",
        "else:\n",
        "    print(f\" Model not found: {BEST_MODEL_PATH}\")\n",
        "    print(\"Please ensure you have a trained Stage 2 model\")\n",
        "    exit()\n",
        "\n",
        "print(\"Stage 2 evaluation framework ready!\")"
      ],
      "metadata": {
        "id": "RwGI4NdirX0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **LOAD DATASET AND PREPARE FOR EVALUATION**\n",
        "\n",
        "print(\"Loading Stage 2 dataset for evaluation...\")\n",
        "\n",
        "# Load the saved dataset\n",
        "if 'df_diseased' not in globals():\n",
        "    dataset_path = os.path.join(DRIVE_MODELS_DIR, 'stage2_final_dataset.csv')\n",
        "    if os.path.exists(dataset_path):\n",
        "        df_diseased = pd.read_csv(dataset_path)\n",
        "        print(f\" Dataset loaded: {len(df_diseased)} samples\")\n",
        "    else:\n",
        "        print(f\" Dataset not found: {dataset_path}\")\n",
        "        exit()\n",
        "\n",
        "# Prepare data for evaluation\n",
        "df_eval = df_diseased.copy()\n",
        "if 'encoded_class_id_str' not in df_eval.columns:\n",
        "    df_eval['encoded_class_id_str'] = df_eval['encoded_class_id'].astype(str)\n",
        "\n",
        "num_classes = int(df_eval['encoded_class_id'].nunique())\n",
        "classes_fixed = [str(i) for i in range(num_classes)]\n",
        "\n",
        "# Create class name mapping\n",
        "idx2name = (df_eval[['encoded_class_id','class_name']]\n",
        "            .drop_duplicates()\n",
        "            .sort_values('encoded_class_id')\n",
        "            .set_index('encoded_class_id')['class_name'].to_dict())\n",
        "\n",
        "# Split data\n",
        "val_df = df_eval[df_eval['split_name']=='val'].copy()\n",
        "test_df = df_eval[df_eval['split_name']=='test'].copy()\n",
        "\n",
        "print(f\"Dataset prepared:\")\n",
        "print(f\"  Total classes: {num_classes}\")\n",
        "print(f\"  Validation samples: {len(val_df)}\")\n",
        "print(f\"  Test samples: {len(test_df)}\")\n",
        "print(f\"  Class range: 0 to {num_classes-1}\")\n",
        "\n",
        "# Create data generators\n",
        "plain_idg = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
        ")\n",
        "\n",
        "val_gen = plain_idg.flow_from_dataframe(\n",
        "    val_df, x_col='image_path_absolute', y_col='encoded_class_id_str',\n",
        "    target_size=(224, 224), batch_size=32, class_mode='categorical',\n",
        "    shuffle=False, seed=42, classes=classes_fixed\n",
        ")\n",
        "\n",
        "test_gen = plain_idg.flow_from_dataframe(\n",
        "    test_df, x_col='image_path_absolute', y_col='encoded_class_id_str',\n",
        "    target_size=(224, 224), batch_size=32, class_mode='categorical',\n",
        "    shuffle=False, seed=42, classes=classes_fixed\n",
        ")\n",
        "\n",
        "print(\" Data generators created successfully!\")"
      ],
      "metadata": {
        "id": "8hWFwtQjAJdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **GENERATE PREDICTIONS AND BASIC METRICS**\n",
        "\n",
        "print(\"Generating predictions for evaluation...\")\n",
        "\n",
        "def predict_generator(gen):\n",
        "    \"\"\"Generate predictions from generator\"\"\"\n",
        "    probs = model.predict(gen, verbose=1)\n",
        "    y_true_attr = getattr(gen, 'classes', None)\n",
        "    if y_true_attr is None:\n",
        "        y_true_attr = getattr(gen, 'labels', None)\n",
        "    y_true = np.asarray(y_true_attr, dtype=int)\n",
        "    y_pred = probs.argmax(axis=1)\n",
        "    assert probs.shape[0] == len(y_true), f\"Mismatch: probs {probs.shape[0]} vs y_true {len(y_true)}\"\n",
        "    return y_true, y_pred, probs\n",
        "\n",
        "# Generate predictions\n",
        "print(\"Validation predictions...\")\n",
        "y_val, yhat_val, p_val = predict_generator(val_gen)\n",
        "\n",
        "print(\"Test predictions...\")\n",
        "y_test, yhat_test, p_test = predict_generator(test_gen)\n",
        "\n",
        "# Basic accuracy metrics\n",
        "acc_val = float((yhat_val == y_val).mean())\n",
        "acc_test = float((yhat_test == y_test).mean())\n",
        "\n",
        "print(f\"\\nBasic Performance:\")\n",
        "print(f\"  Validation Accuracy: {acc_val:.4f}\")\n",
        "print(f\"  Test Accuracy: {acc_test:.4f}\")\n",
        "\n",
        "# Top-K accuracy for multi-class\n",
        "def topk_acc(probs, y_true, k=5):\n",
        "    \"\"\"Calculate top-K accuracy\"\"\"\n",
        "    topk = np.argpartition(-probs, kth=range(k), axis=1)[:, :k]\n",
        "    return float(np.mean([y_true[i] in topk[i] for i in range(len(y_true))]))\n",
        "\n",
        "acc_top3 = topk_acc(p_test, y_test, k=3)\n",
        "acc_top5 = topk_acc(p_test, y_test, k=5)\n",
        "\n",
        "print(f\"  Top-3 Accuracy: {acc_top3:.4f}\")\n",
        "print(f\"  Top-5 Accuracy: {acc_top5:.4f}\")\n",
        "\n",
        "print(\" Predictions generated successfully!\")"
      ],
      "metadata": {
        "id": "c3KVjx0YAOKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **TRAINING HISTORY  AND LOSS FUNCTION ANALYSIS + PLOTS**\n",
        "\n",
        "def analyze_training_loss_and_history():\n",
        "    \"\"\"Analyze training loss, validation loss, and training history with comprehensive visualizations\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" TRAINING LOSS AND HISTORY ANALYSIS\")\n",
        "    print(\" Categorical Cross-Entropy Loss Analysis with Visualizations\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Check if we have training history\n",
        "    history_files = [\n",
        "        os.path.join(DRIVE_MODELS_DIR, 'stage2_mnv2_history.json'),\n",
        "        os.path.join(DRIVE_MODELS_DIR, 'stage2_mnv2_phase1_history.csv'),\n",
        "        os.path.join(DRIVE_MODELS_DIR, 'stage2_mnv2_phase2_history.csv'),\n",
        "        os.path.join(DRIVE_MODELS_DIR, 'stage2_mnv2_resume_history.csv')\n",
        "    ]\n",
        "\n",
        "    training_history = None\n",
        "    history_source = None\n",
        "\n",
        "    # Try to load training history\n",
        "    for history_file in history_files:\n",
        "        if os.path.exists(history_file):\n",
        "            try:\n",
        "                if history_file.endswith('.json'):\n",
        "                    with open(history_file, 'r') as f:\n",
        "                        training_history = json.load(f)\n",
        "                    history_source = history_file\n",
        "                    print(f\" Loaded training history from: {history_file}\")\n",
        "                    break\n",
        "                elif history_file.endswith('.csv'):\n",
        "                    training_history = pd.read_csv(history_file)\n",
        "                    history_source = history_file\n",
        "                    print(f\" Loaded training history from: {history_file}\")\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                print(f\" Error loading {history_file}: {e}\")\n",
        "                continue\n",
        "\n",
        "    if training_history is None:\n",
        "        print(\" No training history found. Proceeding with current model evaluation only.\")\n",
        "        return None\n",
        "\n",
        "    # Analyze training history\n",
        "    print(f\"\\nTRAINING HISTORY ANALYSIS:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    plot_data = {}\n",
        "\n",
        "    if isinstance(training_history, dict):\n",
        "        # JSON format (combined history)\n",
        "        print(\"  Format: Combined JSON history\")\n",
        "\n",
        "        if 'phase1' in training_history and 'phase2' in training_history:\n",
        "            print(\"  Phases: Phase 1 (Head training) + Phase 2 (Fine-tuning)\")\n",
        "\n",
        "            # Phase 1 analysis\n",
        "            phase1 = training_history['phase1']\n",
        "            if 'loss' in phase1 and 'val_loss' in phase1:\n",
        "                phase1_epochs = len(phase1['loss'])\n",
        "                phase1_final_loss = phase1['loss'][-1]\n",
        "                phase1_final_val_loss = phase1['val_loss'][-1]\n",
        "\n",
        "                print(f\"  Phase 1:\")\n",
        "                print(f\"    Epochs: {phase1_epochs}\")\n",
        "                print(f\"    Final Training Loss: {phase1_final_loss:.4f}\")\n",
        "                print(f\"    Final Validation Loss: {phase1_final_val_loss:.4f}\")\n",
        "                print(f\"    Overfitting Check: {'Yes' if phase1_final_loss < phase1_final_val_loss else 'No'}\")\n",
        "\n",
        "                # Store for plotting\n",
        "                plot_data['phase1'] = {\n",
        "                    'epochs': list(range(1, phase1_epochs + 1)),\n",
        "                    'loss': phase1['loss'],\n",
        "                    'val_loss': phase1['val_loss'],\n",
        "                    'accuracy': phase1.get('accuracy', []),\n",
        "                    'val_accuracy': phase1.get('val_accuracy', [])\n",
        "                }\n",
        "\n",
        "            # Phase 2 analysis\n",
        "            phase2 = training_history['phase2']\n",
        "            if 'loss' in phase2 and 'val_loss' in phase2:\n",
        "                phase2_epochs = len(phase2['loss'])\n",
        "                phase2_final_loss = phase2['loss'][-1]\n",
        "                phase2_final_val_loss = phase2['val_loss'][-1]\n",
        "\n",
        "                print(f\"  Phase 2:\")\n",
        "                print(f\"    Epochs: {phase2_epochs}\")\n",
        "                print(f\"    Final Training Loss: {phase2_final_loss:.4f}\")\n",
        "                print(f\"    Final Validation Loss: {phase2_final_val_loss:.4f}\")\n",
        "                print(f\"    Overfitting Check: {'Yes' if phase2_final_loss < phase2_final_val_loss else 'No'}\")\n",
        "\n",
        "                # Store for plotting\n",
        "                plot_data['phase2'] = {\n",
        "                    'epochs': list(range(1, phase2_epochs + 1)),\n",
        "                    'loss': phase2['loss'],\n",
        "                    'val_loss': phase2['val_loss'],\n",
        "                    'accuracy': phase2.get('accuracy', []),\n",
        "                    'val_accuracy': phase2.get('val_accuracy', [])\n",
        "                }\n",
        "\n",
        "    elif isinstance(training_history, pd.DataFrame):\n",
        "        # CSV format\n",
        "        print(\"  Format: CSV history\")\n",
        "        print(f\"  Total rows: {len(training_history)}\")\n",
        "\n",
        "        if 'loss' in training_history.columns and 'val_loss' in training_history.columns:\n",
        "            final_loss = training_history['loss'].iloc[-1]\n",
        "            final_val_loss = training_history['val_loss'].iloc[-1]\n",
        "\n",
        "            print(f\"  Final Training Loss: {final_loss:.4f}\")\n",
        "            print(f\"  Final Validation Loss: {final_val_loss:.4f}\")\n",
        "            print(f\"  Overfitting Check: {'Yes' if final_loss < final_val_loss else 'No'}\")\n",
        "\n",
        "            # Store for plotting\n",
        "            plot_data['single_phase'] = {\n",
        "                'epochs': list(range(1, len(training_history) + 1)),\n",
        "                'loss': training_history['loss'].tolist(),\n",
        "                'val_loss': training_history['val_loss'].tolist(),\n",
        "                'accuracy': training_history.get('accuracy', []).tolist() if 'accuracy' in training_history.columns else [],\n",
        "                'val_accuracy': training_history.get('val_accuracy', []).tolist() if 'val_accuracy' in training_history.columns else []\n",
        "            }\n",
        "\n",
        "    # Current model loss evaluation\n",
        "    print(f\"\\nCURRENT MODEL LOSS EVALUATION:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Calculate categorical cross-entropy loss on test set\n",
        "    from sklearn.metrics import log_loss\n",
        "\n",
        "    # Convert predictions to proper format for loss calculation\n",
        "    y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "    # Calculate loss\n",
        "    test_loss = log_loss(y_test_onehot, p_test)\n",
        "\n",
        "    print(f\"  Test Set Categorical Cross-Entropy Loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Loss interpretation\n",
        "    if test_loss < 0.5:\n",
        "        loss_quality = \"Excellent\"\n",
        "    elif test_loss < 1.0:\n",
        "        loss_quality = \"Good\"\n",
        "    elif test_loss < 2.0:\n",
        "        loss_quality = \"Fair\"\n",
        "    else:\n",
        "        loss_quality = \"Poor\"\n",
        "\n",
        "    print(f\"  Loss Quality: {loss_quality}\")\n",
        "\n",
        "    # Compare with random baseline\n",
        "    random_loss = -np.log(1.0 / num_classes)\n",
        "    print(f\"  Random Baseline Loss: {random_loss:.4f}\")\n",
        "    print(f\"  Improvement over Random: {((random_loss - test_loss) / random_loss * 100):.1f}%\")\n",
        "\n",
        "    # ===== CREATE COMPREHENSIVE VISUALIZATIONS =====\n",
        "    print(f\"\\nCreating comprehensive training analysis visualizations...\")\n",
        "\n",
        "    if plot_data:\n",
        "        # Determine the number of subplots needed\n",
        "        if 'phase1' in plot_data and 'phase2' in plot_data:\n",
        "            # Two-phase training\n",
        "            fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "\n",
        "            # Phase 1: Loss Curves\n",
        "            axes[0, 0].plot(plot_data['phase1']['epochs'], plot_data['phase1']['loss'],\n",
        "                           'b-', linewidth=2, label='Training Loss', alpha=0.8)\n",
        "            axes[0, 0].plot(plot_data['phase1']['epochs'], plot_data['phase1']['val_loss'],\n",
        "                           'r-', linewidth=2, label='Validation Loss', alpha=0.8)\n",
        "            axes[0, 0].set_title('Phase 1: Loss Curves (Head Training)', fontsize=14, fontweight='bold')\n",
        "            axes[0, 0].set_xlabel('Epochs', fontsize=12)\n",
        "            axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
        "            axes[0, 0].legend()\n",
        "            axes[0, 0].grid(True, alpha=0.3)\n",
        "            axes[0, 0].set_yscale('log')  # Log scale for better visualization\n",
        "\n",
        "            # Phase 1: Accuracy Curves\n",
        "            if plot_data['phase1']['accuracy'] and plot_data['phase1']['val_accuracy']:\n",
        "                axes[0, 1].plot(plot_data['phase1']['epochs'], plot_data['phase1']['accuracy'],\n",
        "                               'b-', linewidth=2, label='Training Accuracy', alpha=0.8)\n",
        "                axes[0, 1].plot(plot_data['phase1']['epochs'], plot_data['phase1']['val_accuracy'],\n",
        "                               'r-', linewidth=2, label='Validation Accuracy', alpha=0.8)\n",
        "                axes[0, 1].set_title('Phase 1: Accuracy Curves (Head Training)', fontsize=14, fontweight='bold')\n",
        "                axes[0, 1].set_xlabel('Epochs', fontsize=12)\n",
        "                axes[0, 1].set_ylabel('Accuracy', fontsize=12)\n",
        "                axes[0, 1].legend()\n",
        "                axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            # Phase 1: Overfitting Analysis\n",
        "            if plot_data['phase1']['loss'] and plot_data['phase1']['val_loss']:\n",
        "                loss_diff = [t - v for t, v in zip(plot_data['phase1']['loss'], plot_data['phase1']['val_loss'])]\n",
        "                axes[0, 2].plot(plot_data['phase1']['epochs'], loss_diff,\n",
        "                               'g-', linewidth=2, alpha=0.8)\n",
        "                axes[0, 2].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='No Overfitting')\n",
        "                axes[0, 2].set_title('Phase 1: Overfitting Analysis\\n(Training - Validation Loss)', fontsize=14, fontweight='bold')\n",
        "                axes[0, 2].set_xlabel('Epochs', fontsize=12)\n",
        "                axes[0, 2].set_ylabel('Loss Difference', fontsize=12)\n",
        "                axes[0, 2].legend()\n",
        "                axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "            # Phase 2: Loss Curves\n",
        "            axes[1, 0].plot(plot_data['phase2']['epochs'], plot_data['phase2']['loss'],\n",
        "                           'b-', linewidth=2, label='Training Loss', alpha=0.8)\n",
        "            axes[1, 0].plot(plot_data['phase2']['epochs'], plot_data['phase2']['val_loss'],\n",
        "                           'r-', linewidth=2, label='Validation Loss', alpha=0.8)\n",
        "            axes[1, 0].set_title('Phase 2: Loss Curves (Fine-tuning)', fontsize=14, fontweight='bold')\n",
        "            axes[1, 0].set_xlabel('Epochs', fontsize=12)\n",
        "            axes[1, 0].set_ylabel('Loss', fontsize=12)\n",
        "            axes[1, 0].legend()\n",
        "            axes[1, 0].grid(True, alpha=0.3)\n",
        "            axes[1, 0].set_yscale('log')  # Log scale for better visualization\n",
        "\n",
        "            # Phase 2: Accuracy Curves\n",
        "            if plot_data['phase2']['accuracy'] and plot_data['phase2']['val_accuracy']:\n",
        "                axes[1, 1].plot(plot_data['phase2']['epochs'], plot_data['phase2']['accuracy'],\n",
        "                               'b-', linewidth=2, label='Training Accuracy', alpha=0.8)\n",
        "                axes[1, 1].plot(plot_data['phase2']['epochs'], plot_data['phase2']['val_accuracy'],\n",
        "                               'r-', linewidth=2, label='Validation Accuracy', alpha=0.8)\n",
        "                axes[1, 1].set_title('Phase 2: Accuracy Curves (Fine-tuning)', fontsize=14, fontweight='bold')\n",
        "                axes[1, 0].set_xlabel('Epochs', fontsize=12)\n",
        "                axes[1, 1].set_ylabel('Accuracy', fontsize=12)\n",
        "                axes[1, 1].legend()\n",
        "                axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            # Phase 2: Overfitting Analysis\n",
        "            if plot_data['phase2']['loss'] and plot_data['phase2']['val_loss']:\n",
        "                loss_diff = [t - v for t, v in zip(plot_data['phase2']['loss'], plot_data['phase2']['val_loss'])]\n",
        "                axes[1, 2].plot(plot_data['phase2']['epochs'], loss_diff,\n",
        "                               'g-', linewidth=2, alpha=0.8)\n",
        "                axes[1, 2].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='No Overfitting')\n",
        "                axes[1, 2].set_title('Phase 2: Overfitting Analysis\\n(Training - Validation Loss)', fontsize=14, fontweight='bold')\n",
        "                axes[1, 2].set_xlabel('Epochs', fontsize=12)\n",
        "                axes[1, 2].set_ylabel('Loss Difference', fontsize=12)\n",
        "                axes[1, 2].legend()\n",
        "                axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "        elif 'single_phase' in plot_data:\n",
        "            # Single phase training\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "            # Loss Curves\n",
        "            axes[0, 0].plot(plot_data['single_phase']['epochs'], plot_data['single_phase']['loss'],\n",
        "                           'b-', linewidth=2, label='Training Loss', alpha=0.8)\n",
        "            axes[0, 0].plot(plot_data['single_phase']['epochs'], plot_data['single_phase']['val_loss'],\n",
        "                           'r-', linewidth=2, label='Validation Loss', alpha=0.8)\n",
        "            axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "            axes[0, 0].set_xlabel('Epochs', fontsize=12)\n",
        "            axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
        "            axes[0, 0].legend()\n",
        "            axes[0, 0].grid(True, alpha=0.3)\n",
        "            axes[0, 0].set_yscale('log')  # Log scale for better visualization\n",
        "\n",
        "            # Accuracy Curves\n",
        "            if plot_data['single_phase']['accuracy'] and plot_data['single_phase']['val_accuracy']:\n",
        "                axes[0, 1].plot(plot_data['single_phase']['epochs'], plot_data['single_phase']['accuracy'],\n",
        "                               'b-', linewidth=2, label='Training Accuracy', alpha=0.8)\n",
        "                axes[0, 1].plot(plot_data['single_phase']['epochs'], plot_data['single_phase']['val_accuracy'],\n",
        "                               'r-', linewidth=2, label='Validation Accuracy', alpha=0.8)\n",
        "                axes[0, 1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "                axes[0, 1].set_xlabel('Epochs', fontsize=12)\n",
        "                axes[0, 1].set_ylabel('Accuracy', fontsize=12)\n",
        "                axes[0, 1].legend()\n",
        "                axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            # Overfitting Analysis\n",
        "            if plot_data['single_phase']['loss'] and plot_data['single_phase']['val_loss']:\n",
        "                loss_diff = [t - v for t, v in zip(plot_data['single_phase']['loss'], plot_data['single_phase']['val_loss'])]\n",
        "                axes[1, 0].plot(plot_data['single_phase']['epochs'], loss_diff,\n",
        "                               'g-', linewidth=2, alpha=0.8)\n",
        "                axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='No Overfitting')\n",
        "                axes[1, 0].set_title('Overfitting Analysis\\n(Training - Validation Loss)', fontsize=14, fontweight='bold')\n",
        "                axes[1, 0].set_xlabel('Epochs', fontsize=12)\n",
        "                axes[1, 0].set_ylabel('Loss Difference', fontsize=12)\n",
        "                axes[1, 0].legend()\n",
        "                axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            # Loss vs Accuracy Correlation\n",
        "            if plot_data['single_phase']['loss'] and plot_data['single_phase']['accuracy']:\n",
        "                axes[1, 1].scatter(plot_data['single_phase']['loss'], plot_data['single_phase']['accuracy'],\n",
        "                                  alpha=0.7, color='purple', s=50)\n",
        "                axes[1, 1].set_title('Training Loss vs Training Accuracy', fontsize=14, fontweight='bold')\n",
        "                axes[1, 1].set_xlabel('Training Loss', fontsize=12)\n",
        "                axes[1, 1].set_ylabel('Training Accuracy', fontsize=12)\n",
        "                axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save training analysis visualization\n",
        "        training_viz_path = os.path.join(DRIVE_VISUALIZATIONS_DIR, 'stage2_training_analysis.png')\n",
        "        plt.savefig(training_viz_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Training analysis visualizations saved to: {training_viz_path}\")\n",
        "\n",
        "    # Create additional loss analysis plots\n",
        "    print(f\"\\nCreating additional loss analysis plots...\")\n",
        "\n",
        "    # 1. Loss Quality Assessment\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Loss Quality Bar Chart\n",
        "    loss_categories = ['Random\\nBaseline', 'Current\\nModel', 'Perfect\\nModel']\n",
        "    loss_values = [random_loss, test_loss, 0.0]  # Perfect model has 0 loss\n",
        "    colors = ['red', 'orange', 'green']\n",
        "\n",
        "    bars = axes[0].bar(loss_categories, loss_values, color=colors, alpha=0.7)\n",
        "    axes[0].set_title('Loss Quality Assessment', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Model Type', fontsize=12)\n",
        "    axes[0].set_ylabel('Categorical Cross-Entropy Loss', fontsize=12)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, loss_values):\n",
        "        height = bar.get_height()\n",
        "        axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Improvement over Random\n",
        "    improvement = ((random_loss - test_loss) / random_loss * 100)\n",
        "    axes[1].pie([improvement, 100-improvement], labels=[f'Improvement\\n{improvement:.1f}%', 'Remaining\\nGap'],\n",
        "                colors=['lightgreen', 'lightcoral'], autopct='%1.1f%%', startangle=90)\n",
        "    axes[1].set_title('Improvement over Random Baseline', fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save loss quality visualization\n",
        "    loss_quality_path = os.path.join(DRIVE_VISUALIZATIONS_DIR, 'stage2_loss_quality_analysis.png')\n",
        "    plt.savefig(loss_quality_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Loss Distribution Analysis\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Test Loss Distribution\n",
        "    axes[0].hist(p_test.max(axis=1), bins=30, color='skyblue', alpha=0.7, edgecolor='black')\n",
        "    axes[0].axvline(p_test.max(axis=1).mean(), color='red', linestyle='--',\n",
        "                    label=f'Mean Confidence: {p_test.max(axis=1).mean():.3f}')\n",
        "    axes[0].set_title('Model Confidence Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Prediction Confidence', fontsize=12)\n",
        "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Loss vs Confidence Correlation\n",
        "    # Calculate per-sample loss\n",
        "    per_sample_loss = []\n",
        "    for i in range(len(y_test)):\n",
        "        true_probs = y_test_onehot[i]\n",
        "        pred_probs = p_test[i]\n",
        "        sample_loss = -np.sum(true_probs * np.log(pred_probs + 1e-15))\n",
        "        per_sample_loss.append(sample_loss)\n",
        "\n",
        "    per_sample_loss = np.array(per_sample_loss)\n",
        "    confidence = p_test.max(axis=1)\n",
        "\n",
        "    axes[1].scatter(confidence, per_sample_loss, alpha=0.6, color='purple', s=30)\n",
        "    axes[1].set_title('Loss vs Confidence Correlation', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Prediction Confidence', fontsize=12)\n",
        "    axes[1].set_ylabel('Per-Sample Loss', fontsize=12)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add trend line\n",
        "    z = np.polyfit(confidence, per_sample_loss, 1)\n",
        "    p = np.poly1d(z)\n",
        "    correlation = np.corrcoef(confidence, per_sample_loss)[0, 1]\n",
        "    axes[1].plot(confidence, p(confidence), \"r--\", alpha=0.8,\n",
        "                 label=f\"Trend (r={correlation:.3f})\")\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save loss distribution visualization\n",
        "    loss_dist_path = os.path.join(DRIVE_VISUALIZATIONS_DIR, 'stage2_loss_distribution_analysis.png')\n",
        "    plt.savefig(loss_dist_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Additional loss analysis plots saved to:\")\n",
        "    print(f\"  - Loss quality analysis: {loss_quality_path}\")\n",
        "    print(f\"  - Loss distribution analysis: {loss_dist_path}\")\n",
        "\n",
        "    return {\n",
        "        'training_history': training_history,\n",
        "        'history_source': history_source,\n",
        "        'test_loss': test_loss,\n",
        "        'loss_quality': loss_quality,\n",
        "        'random_baseline': random_loss,\n",
        "        'plot_data': plot_data\n",
        "    }\n",
        "\n",
        "# Run loss analysis with comprehensive plots\n",
        "loss_analysis = analyze_training_loss_and_history()"
      ],
      "metadata": {
        "id": "icUhEXpRNmkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **BOOTSTRAP CONFIDENCE INTERVALS FOR MULTI-CLASS + PLOTS**\n",
        "\n",
        "def bootstrap_confidence_intervals_multi_class(n_bootstrap=1000, confidence=0.95):\n",
        "    \"\"\"Calculate bootstrap confidence intervals for multi-class metrics with comprehensive visualizations\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" BOOTSTRAP CONFIDENCE INTERVALS - MULTI-CLASS\")\n",
        "    print(\" Disease Classification Analysis for 59 Classes\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Bootstrap accuracy\n",
        "    n_samples = len(y_test)\n",
        "    bootstrap_accuracies = []\n",
        "    bootstrap_f1s = []\n",
        "    bootstrap_precisions = []\n",
        "    bootstrap_recalls = []\n",
        "    bootstrap_top3_accuracies = []\n",
        "    bootstrap_top5_accuracies = []\n",
        "\n",
        "    print(f\"Performing bootstrap analysis on {n_samples} test samples...\")\n",
        "    print(f\"Number of disease classes: {num_classes}\")\n",
        "\n",
        "    for i in range(n_bootstrap):\n",
        "        if (i + 1) % 200 == 0:\n",
        "            print(f\"  Progress: {i + 1}/{n_bootstrap} iterations\")\n",
        "\n",
        "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
        "        bootstrap_acc = (yhat_test[indices] == y_test[indices]).mean()\n",
        "        bootstrap_accuracies.append(bootstrap_acc)\n",
        "\n",
        "        # Calculate precision, recall, F1 for this bootstrap sample\n",
        "        bootstrap_precision, bootstrap_recall, bootstrap_f1, _ = precision_recall_fscore_support(\n",
        "            y_test[indices], yhat_test[indices], average='macro', zero_division=0\n",
        "        )\n",
        "        bootstrap_precisions.append(bootstrap_precision)\n",
        "        bootstrap_recalls.append(bootstrap_recall)\n",
        "        bootstrap_f1s.append(bootstrap_f1)\n",
        "\n",
        "        # Top-K accuracy bootstrap\n",
        "        bootstrap_top3_acc = topk_acc(p_test[indices], y_test[indices], k=3)\n",
        "        bootstrap_top5_acc = topk_acc(p_test[indices], y_test[indices], k=5)\n",
        "        bootstrap_top3_accuracies.append(bootstrap_top3_acc)\n",
        "        bootstrap_top5_accuracies.append(bootstrap_top5_acc)\n",
        "\n",
        "    # Calculate confidence intervals for all metrics\n",
        "    alpha = 1 - confidence\n",
        "\n",
        "    # Accuracy\n",
        "    acc_ci_lower = np.percentile(bootstrap_accuracies, alpha/2 * 100)\n",
        "    acc_ci_upper = np.percentile(bootstrap_accuracies, (1-alpha/2) * 100)\n",
        "    acc_mean = np.mean(bootstrap_accuracies)\n",
        "    acc_std = np.std(bootstrap_accuracies)\n",
        "\n",
        "    # F1-Score\n",
        "    f1_ci_lower = np.percentile(bootstrap_f1s, alpha/2 * 100)\n",
        "    f1_ci_upper = np.percentile(bootstrap_f1s, (1-alpha/2) * 100)\n",
        "    f1_mean = np.mean(bootstrap_f1s)\n",
        "    f1_std = np.std(bootstrap_f1s)\n",
        "\n",
        "    # Precision\n",
        "    prec_ci_lower = np.percentile(bootstrap_precisions, alpha/2 * 100)\n",
        "    prec_ci_upper = np.percentile(bootstrap_precisions, (1-alpha/2) * 100)\n",
        "    prec_mean = np.mean(bootstrap_precisions)\n",
        "    prec_std = np.std(bootstrap_precisions)\n",
        "\n",
        "    # Recall\n",
        "    rec_ci_lower = np.percentile(bootstrap_recalls, alpha/2 * 100)\n",
        "    rec_ci_upper = np.percentile(bootstrap_recalls, (1-alpha/2) * 100)\n",
        "    rec_mean = np.mean(bootstrap_recalls)\n",
        "    rec_std = np.std(bootstrap_recalls)\n",
        "\n",
        "    # Top-3 Accuracy\n",
        "    top3_ci_lower = np.percentile(bootstrap_top3_accuracies, alpha/2 * 100)\n",
        "    top3_ci_upper = np.percentile(bootstrap_top3_accuracies, (1-alpha/2) * 100)\n",
        "    top3_mean = np.mean(bootstrap_top3_accuracies)\n",
        "    top3_std = np.std(bootstrap_top3_accuracies)\n",
        "\n",
        "    # Top-5 Accuracy\n",
        "    top5_ci_lower = np.percentile(bootstrap_top5_accuracies, alpha/2 * 100)\n",
        "    top5_ci_upper = np.percentile(bootstrap_top5_accuracies, (1-alpha/2) * 100)\n",
        "    top5_mean = np.mean(bootstrap_top5_accuracies)\n",
        "    top5_std = np.std(bootstrap_top5_accuracies)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nACCURACY ANALYSIS:\")\n",
        "    print(f\"  Mean: {acc_mean:.4f}\")\n",
        "    print(f\"  {confidence*100}% CI: [{acc_ci_lower:.4f}, {acc_ci_upper:.4f}]\")\n",
        "    print(f\"  Width: {acc_ci_upper - acc_ci_lower:.4f}\")\n",
        "\n",
        "    print(f\"\\nF1-SCORE ANALYSIS (Macro):\")\n",
        "    print(f\"  Mean: {f1_mean:.4f}\")\n",
        "    print(f\"  {confidence*100}% CI: [{f1_ci_lower:.4f}, {f1_ci_upper:.4f}]\")\n",
        "    print(f\"  Width: {f1_ci_upper - f1_ci_lower:.4f}\")\n",
        "\n",
        "    print(f\"\\nPRECISION ANALYSIS (Macro):\")\n",
        "    print(f\"  Mean: {prec_mean:.4f}\")\n",
        "    print(f\"  {confidence*100}% CI: [{prec_ci_lower:.4f}, {prec_ci_upper:.4f}]\")\n",
        "    print(f\"  Width: {prec_ci_upper - prec_ci_lower:.4f}\")\n",
        "\n",
        "    print(f\"\\nRECALL ANALYSIS (Macro):\")\n",
        "    print(f\"  Mean: {rec_mean:.4f}\")\n",
        "    print(f\"  {confidence*100}% CI: [{rec_ci_lower:.4f}, {rec_ci_upper:.4f}]\")\n",
        "    print(f\"  Width: {rec_ci_upper - rec_ci_lower:.4f}\")\n",
        "\n",
        "    print(f\"\\nTOP-3 ACCURACY ANALYSIS:\")\n",
        "    print(f\"  Mean: {top3_mean:.4f}\")\n",
        "    print(f\"  {confidence*100}% CI: [{top3_ci_lower:.4f}, {top3_ci_upper:.4f}]\")\n",
        "    print(f\"  Width: {top3_ci_upper - top3_ci_lower:.4f}\")\n",
        "\n",
        "    print(f\"\\nTOP-5 ACCURACY ANALYSIS:\")\n",
        "    print(f\"  Mean: {top5_mean:.4f}\")\n",
        "    print(f\"  {confidence*100}% CI: [{top5_ci_lower:.4f}, {top5_ci_upper:.4f}]\")\n",
        "    print(f\"  Width: {top5_ci_upper - top5_ci_lower:.4f}\")\n",
        "\n",
        "    print(f\"\\nBootstrap analysis completed with {n_bootstrap} iterations\")\n",
        "    print(f\"Confidence level: {confidence*100}%\")\n",
        "\n",
        "    # Store results for visualization\n",
        "    bootstrap_results = {\n",
        "        'accuracy': {'mean': acc_mean, 'ci': [acc_ci_lower, acc_ci_upper], 'std': acc_std, 'values': bootstrap_accuracies},\n",
        "        'f1_score': {'mean': f1_mean, 'ci': [f1_ci_lower, f1_ci_upper], 'std': f1_std, 'values': bootstrap_f1s},\n",
        "        'precision': {'mean': prec_mean, 'ci': [prec_ci_lower, prec_ci_upper], 'std': prec_std, 'values': bootstrap_precisions},\n",
        "        'recall': {'mean': rec_mean, 'ci': [rec_ci_lower, rec_ci_upper], 'std': rec_std, 'values': bootstrap_recalls},\n",
        "        'top3_accuracy': {'mean': top3_mean, 'ci': [top3_ci_lower, top3_ci_upper], 'std': top3_std, 'values': bootstrap_top3_accuracies},\n",
        "        'top5_accuracy': {'mean': top5_mean, 'ci': [top5_ci_lower, top5_ci_upper], 'std': top5_std, 'values': bootstrap_top5_accuracies}\n",
        "    }\n",
        "\n",
        "    # Create comprehensive visualizations\n",
        "    create_stage2_bootstrap_visualizations(bootstrap_results, confidence, n_bootstrap, num_classes)\n",
        "\n",
        "    return bootstrap_results\n",
        "\n",
        "def create_stage2_bootstrap_visualizations(bootstrap_results, confidence, n_bootstrap, num_classes):\n",
        "    \"\"\"Create comprehensive bootstrap analysis visualizations for Stage 2 multi-class classification\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" CREATING STAGE 2 BOOTSTRAP ANALYSIS VISUALIZATIONS\")\n",
        "    print(\" Multi-Class Disease Classification Analysis\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Set up the plotting style\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "\n",
        "    # Create a comprehensive figure\n",
        "    fig = plt.figure(figsize=(20, 16))\n",
        "    fig.suptitle(f'Stage 2: Multi-Class Disease Classification Bootstrap Analysis\\n'\n",
        "                 f'{n_bootstrap} iterations, {confidence*100}% confidence level, {num_classes} disease classes',\n",
        "                 fontsize=18, fontweight='bold')\n",
        "\n",
        "    # Define colors for different metrics\n",
        "    colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum', 'lightsteelblue']\n",
        "\n",
        "    # Plot 1: Performance Metrics with Confidence Intervals\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    metrics = ['accuracy', 'f1_score', 'precision', 'recall', 'top3_accuracy', 'top5_accuracy']\n",
        "    metric_labels = ['Accuracy', 'F1-Score', 'Precision', 'Recall', 'Top-3 Acc', 'Top-5 Acc']\n",
        "\n",
        "    means = [bootstrap_results[metric]['mean'] for metric in metrics]\n",
        "    ci_lowers = [bootstrap_results[metric]['ci'][0] for metric in metrics]\n",
        "    ci_uppers = [bootstrap_results[metric]['ci'][1] for metric in metrics]\n",
        "\n",
        "    x = np.arange(len(metrics))\n",
        "    bars = ax1.bar(x, means, color=colors[:len(metrics)], alpha=0.8)\n",
        "\n",
        "    # Add error bars for confidence intervals\n",
        "    ax1.errorbar(x, means, yerr=[np.array(means) - np.array(ci_lowers),\n",
        "                                 np.array(ci_uppers) - np.array(means)],\n",
        "                fmt='none', color='black', capsize=5, capthick=1)\n",
        "\n",
        "    ax1.set_xlabel('Metrics')\n",
        "    ax1.set_ylabel('Score')\n",
        "    ax1.set_title('Multi-Class Performance Metrics with Confidence Intervals')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(metric_labels, rotation=45)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim(0, 1)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, mean_val in zip(bars, means):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{mean_val:.3f}',\n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 2: Bootstrap Distribution of Accuracy\n",
        "    ax2 = plt.subplot(3, 3, 2)\n",
        "    acc_values = bootstrap_results['accuracy']['values']\n",
        "    ax2.hist(acc_values, bins=30, alpha=0.7, color='skyblue', density=True, edgecolor='black')\n",
        "    ax2.axvline(bootstrap_results['accuracy']['mean'], color='red', linestyle='--', linewidth=2,\n",
        "                label=f'Mean: {bootstrap_results[\"accuracy\"][\"mean\"]:.3f}')\n",
        "    ax2.axvline(bootstrap_results['accuracy']['ci'][0], color='orange', linestyle=':', linewidth=2,\n",
        "                label=f'CI Lower: {bootstrap_results[\"accuracy\"][\"ci\"][0]:.3f}')\n",
        "    ax2.axvline(bootstrap_results['accuracy']['ci'][1], color='orange', linestyle=':', linewidth=2,\n",
        "                label=f'CI Upper: {bootstrap_results[\"accuracy\"][\"ci\"][1]:.3f}')\n",
        "\n",
        "    ax2.set_xlabel('Accuracy')\n",
        "    ax2.set_ylabel('Density')\n",
        "    ax2.set_title('Bootstrap Distribution of Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Bootstrap Distribution of F1-Score\n",
        "    ax3 = plt.subplot(3, 3, 3)\n",
        "    f1_values = bootstrap_results['f1_score']['values']\n",
        "    ax3.hist(f1_values, bins=30, alpha=0.7, color='lightcoral', density=True, edgecolor='black')\n",
        "    ax3.axvline(bootstrap_results['f1_score']['mean'], color='red', linestyle='--', linewidth=2,\n",
        "                label=f'Mean: {bootstrap_results[\"f1_score\"][\"mean\"]:.3f}')\n",
        "    ax3.axvline(bootstrap_results['f1_score']['ci'][0], color='orange', linestyle=':', linewidth=2,\n",
        "                label=f'CI Lower: {bootstrap_results[\"f1_score\"][\"ci\"][0]:.3f}')\n",
        "    ax3.axvline(bootstrap_results['f1_score']['ci'][1], color='orange', linestyle=':', linewidth=2,\n",
        "                label=f'CI Upper: {bootstrap_results[\"f1_score\"][\"ci\"][1]:.3f}')\n",
        "\n",
        "    ax3.set_xlabel('F1-Score')\n",
        "    ax3.set_ylabel('Density')\n",
        "    ax3.set_title('Bootstrap Distribution of F1-Score')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Top-K Accuracy Comparison\n",
        "    ax4 = plt.subplot(3, 3, 4)\n",
        "    topk_metrics = ['accuracy', 'top3_accuracy', 'top5_accuracy']\n",
        "    topk_labels = ['Top-1', 'Top-3', 'Top-5']\n",
        "    topk_means = [bootstrap_results[metric]['mean'] for metric in topk_metrics]\n",
        "    topk_stds = [bootstrap_results[metric]['std'] for metric in topk_metrics]\n",
        "\n",
        "    bars = ax4.bar(topk_labels, topk_means, color=['skyblue', 'lightgreen', 'plum'], alpha=0.8)\n",
        "    ax4.errorbar(topk_labels, topk_means, yerr=topk_stds, fmt='none', color='black', capsize=5, capthick=1)\n",
        "\n",
        "    ax4.set_ylabel('Accuracy')\n",
        "    ax4.set_title('Top-K Accuracy Comparison')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    ax4.set_ylim(0, 1)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, mean_val in zip(bars, topk_means):\n",
        "        height = bar.get_height()\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{mean_val:.3f}',\n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 5: Precision vs Recall Analysis\n",
        "    ax5 = plt.subplot(3, 3, 5)\n",
        "    precision_values = bootstrap_results['precision']['values']\n",
        "    recall_values = bootstrap_results['recall']['values']\n",
        "\n",
        "    ax5.scatter(precision_values, recall_values, alpha=0.6, color='lightgreen', s=20)\n",
        "    ax5.axhline(bootstrap_results['recall']['mean'], color='red', linestyle='--', alpha=0.7,\n",
        "                label=f'Recall Mean: {bootstrap_results[\"recall\"][\"mean\"]:.3f}')\n",
        "    ax5.axvline(bootstrap_results['precision']['mean'], color='blue', linestyle='--', alpha=0.7,\n",
        "                label=f'Precision Mean: {bootstrap_results[\"precision\"][\"mean\"]:.3f}')\n",
        "\n",
        "    ax5.set_xlabel('Precision')\n",
        "    ax5.set_ylabel('Recall')\n",
        "    ax5.set_title('Precision vs Recall Distribution')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 6: Confidence Interval Widths\n",
        "    ax6 = plt.subplot(3, 3, 6)\n",
        "    ci_widths = [bootstrap_results[metric]['ci'][1] - bootstrap_results[metric]['ci'][0] for metric in metrics]\n",
        "\n",
        "    bars = ax6.bar(metric_labels, ci_widths, color=colors[:len(metrics)], alpha=0.8)\n",
        "    ax6.set_ylabel('CI Width (Lower is Better)')\n",
        "    ax6.set_title('Confidence Interval Widths')\n",
        "    ax6.set_xticklabels(metric_labels, rotation=45)\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, width in zip(bars, ci_widths):\n",
        "        height = bar.get_height()\n",
        "        ax6.text(bar.get_x() + bar.get_width()/2., height + 0.001, f'{width:.4f}',\n",
        "                ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "    # Plot 7: Bootstrap Stability Analysis\n",
        "    ax7 = plt.subplot(3, 3, 7)\n",
        "    stds = [bootstrap_results[metric]['std'] for metric in metrics]\n",
        "\n",
        "    bars = ax7.bar(metric_labels, stds, color=colors[:len(metrics)], alpha=0.8)\n",
        "    ax7.set_ylabel('Standard Deviation (Lower is Better)')\n",
        "    ax7.set_title('Bootstrap Stability Analysis')\n",
        "    ax7.set_xticklabels(metric_labels, rotation=45)\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, std_val in zip(bars, stds):\n",
        "        height = bar.get_height()\n",
        "        ax7.text(bar.get_x() + bar.get_width()/2., height + 0.001, f'{std_val:.4f}',\n",
        "                ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "    # Plot 8: Performance Heatmap\n",
        "    ax8 = plt.subplot(3, 3, 8)\n",
        "    performance_data = np.array([\n",
        "        [bootstrap_results['accuracy']['mean'], bootstrap_results['f1_score']['mean']],\n",
        "        [bootstrap_results['precision']['mean'], bootstrap_results['recall']['mean']],\n",
        "        [bootstrap_results['top3_accuracy']['mean'], bootstrap_results['top5_accuracy']['mean']]\n",
        "    ])\n",
        "\n",
        "    im = ax8.imshow(performance_data, cmap='RdYlGn', aspect='auto')\n",
        "    ax8.set_xticks([0, 1])\n",
        "    ax8.set_yticks([0, 1, 2])\n",
        "    ax8.set_xticklabels(['Metric 1', 'Metric 2'])\n",
        "    ax8.set_yticklabels(['Accuracy/F1', 'Precision/Recall', 'Top-3/Top-5'])\n",
        "    ax8.set_title('Performance Heatmap')\n",
        "\n",
        "    # Add text annotations\n",
        "    for i in range(3):\n",
        "        for j in range(2):\n",
        "            text = ax8.text(j, i, f'{performance_data[i, j]:.3f}',\n",
        "                           ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "\n",
        "    plt.colorbar(im, ax=ax8, shrink=0.8)\n",
        "\n",
        "    # Plot 9: Statistical Summary\n",
        "    ax9 = plt.subplot(3, 3, 9)\n",
        "    ax9.axis('off')\n",
        "\n",
        "    summary_text = \"STAGE 2 BOOTSTRAP SUMMARY:\\n\\n\"\n",
        "    summary_text += f\"Iterations: {n_bootstrap}\\n\"\n",
        "    summary_text += f\"Confidence Level: {confidence*100}%\\n\"\n",
        "    summary_text += f\"Disease Classes: {num_classes}\\n\\n\"\n",
        "\n",
        "    summary_text += \"KEY METRICS:\\n\"\n",
        "    summary_text += f\"Accuracy: {bootstrap_results['accuracy']['mean']:.4f}\\n\"\n",
        "    summary_text += f\"F1-Score: {bootstrap_results['f1_score']['mean']:.4f}\\n\"\n",
        "    summary_text += f\"Top-3 Acc: {bootstrap_results['top3_accuracy']['mean']:.4f}\\n\"\n",
        "    summary_text += f\"Top-5 Acc: {bootstrap_results['top5_accuracy']['mean']:.4f}\\n\\n\"\n",
        "\n",
        "    summary_text += \"PERFORMANCE ASSESSMENT:\\n\"\n",
        "    if bootstrap_results['accuracy']['mean'] > 0.8:\n",
        "        summary_text += \" High accuracy achieved\\n\"\n",
        "    elif bootstrap_results['accuracy']['mean'] > 0.6:\n",
        "        summary_text += \"~ Moderate accuracy\\n\"\n",
        "    else:\n",
        "        summary_text += \" Low accuracy - needs improvement\\n\"\n",
        "\n",
        "    if bootstrap_results['top3_accuracy']['mean'] > bootstrap_results['accuracy']['mean'] + 0.1:\n",
        "        summary_text += \" Top-K accuracy shows improvement\\n\"\n",
        "\n",
        "    summary_text += f\"\\nAnalysis completed successfully!\"\n",
        "\n",
        "    ax9.text(0.05, 0.95, summary_text, transform=ax9.transAxes, fontsize=9,\n",
        "             verticalalignment='top', fontfamily='monospace',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the comprehensive visualization\n",
        "    save_path = '/content/drive/MyDrive/plantwild_stage1_models/stage2_bootstrap_analysis_comprehensive.png'\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\" Comprehensive Stage 2 bootstrap visualization saved to: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Run enhanced Stage 2 bootstrap analysis with plots\n",
        "bootstrap_results = bootstrap_confidence_intervals_multi_class()"
      ],
      "metadata": {
        "id": "4tcjWO8hXnVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **PRECISION, RECALL, F1-SCORE COMPREHENSIVE ANALYSIS + PLOTS (FIXED)**\n",
        "\n",
        "def comprehensive_precision_recall_f1_analysis():\n",
        "    \"\"\"Comprehensive analysis of precision, recall, and F1-score metrics with visualizations\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" PRECISION, RECALL, F1-SCORE COMPREHENSIVE ANALYSIS\")\n",
        "    print(\" Multi-Class Disease Classification Performance Analysis\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Calculate all metrics with different averaging methods\n",
        "    per_class_precision, per_class_recall, per_class_f1, per_class_support = precision_recall_fscore_support(\n",
        "        y_test, yhat_test, average=None, zero_division=0\n",
        "    )\n",
        "\n",
        "    # Macro-averaged metrics (treats all classes equally)\n",
        "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
        "        y_test, yhat_test, average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Micro-averaged metrics (aggregates all classes)\n",
        "    micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
        "        y_test, yhat_test, average='micro', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Weighted-averaged metrics (weighted by class support)\n",
        "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
        "        y_test, yhat_test, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    print(f\"OVERALL PERFORMANCE METRICS:\")\n",
        "    print(f\"  Accuracy: {acc_test:.4f}\")\n",
        "    print(f\"  Top-3 Accuracy: {acc_top3:.4f}\")\n",
        "    print(f\"  Top-5 Accuracy: {acc_top5:.4f}\")\n",
        "\n",
        "    print(f\"\\nMACRO-AVERAGED METRICS (All classes treated equally):\")\n",
        "    print(f\"  Precision: {macro_precision:.4f}\")\n",
        "    print(f\"  Recall: {macro_recall:.4f}\")\n",
        "    print(f\"  F1-Score: {macro_f1:.4f}\")\n",
        "\n",
        "    print(f\"\\nMICRO-AVERAGED METRICS (Aggregated across all classes):\")\n",
        "    print(f\"  Precision: {micro_precision:.4f}\")\n",
        "    print(f\"  Recall: {micro_recall:.4f}\")\n",
        "    print(f\"  F1-Score: {micro_f1:.4f}\")\n",
        "\n",
        "    print(f\"\\nWEIGHTED-AVERAGED METRICS (Weighted by class frequency):\")\n",
        "    print(f\"  Precision: {weighted_precision:.4f}\")\n",
        "    print(f\"  Recall: {weighted_recall:.4f}\")\n",
        "    print(f\"  F1-Score: {weighted_f1:.4f}\")\n",
        "\n",
        "    # Per-class detailed analysis\n",
        "    print(f\"\\nPER-CLASS PERFORMANCE ANALYSIS:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Class ID':<8} {'Class Name':<35} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<8}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Sort classes by F1-score for better analysis\n",
        "    class_performance = []\n",
        "    for i in range(num_classes):\n",
        "        class_name = idx2name.get(i, f\"Class_{i}\")\n",
        "        class_performance.append({\n",
        "            'class_id': i,\n",
        "            'class_name': class_name,\n",
        "            'precision': per_class_precision[i],\n",
        "            'recall': per_class_recall[i],\n",
        "            'f1_score': per_class_f1[i],\n",
        "            'support': per_class_support[i]\n",
        "        })\n",
        "\n",
        "    # Sort by F1-score (descending)\n",
        "    class_performance.sort(key=lambda x: x['f1_score'], reverse=True)\n",
        "\n",
        "    # Display top 15 and bottom 15 classes\n",
        "    print(f\"TOP 15 PERFORMING CLASSES:\")\n",
        "    for i, perf in enumerate(class_performance[:15]):\n",
        "        print(f\"{perf['class_id']:<8} {perf['class_name'][:34]:<35} {perf['precision']:<10.4f} {perf['recall']:<10.4f} {perf['f1_score']:<10.4f} {perf['support']:<8}\")\n",
        "\n",
        "    print(f\"\\nBOTTOM 15 PERFORMING CLASSES:\")\n",
        "    for i, perf in enumerate(class_performance[-15:]):\n",
        "        print(f\"{perf['class_id']:<8} {perf['class_name'][:34]:<35} {perf['precision']:<10.4f} {perf['recall']:<10.4f} {perf['f1_score']:<10.4f} {perf['support']:<8}\")\n",
        "\n",
        "    # Class imbalance analysis\n",
        "    print(f\"\\nCLASS IMBALANCE ANALYSIS:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    support_values = np.array(per_class_support)\n",
        "    mean_support = support_values.mean()\n",
        "    median_support = np.median(support_values)\n",
        "    std_support = support_values.std()\n",
        "\n",
        "    print(f\"  Mean samples per class: {mean_support:.1f}\")\n",
        "    print(f\"  Median samples per class: {median_support:.1f}\")\n",
        "    print(f\"  Standard deviation: {std_support:.1f}\")\n",
        "    print(f\"  Classes with < 10 samples: {(support_values < 10).sum()}\")\n",
        "    print(f\"  Classes with > 100 samples: {(support_values > 100).sum()}\")\n",
        "\n",
        "    # Performance correlation with class support\n",
        "    f1_scores = np.array(per_class_f1)\n",
        "    correlation = np.corrcoef(support_values, f1_scores)[0, 1]\n",
        "    print(f\"  Correlation (Support vs F1-Score): {correlation:.4f}\")\n",
        "\n",
        "    # Create comprehensive visualizations\n",
        "    create_precision_recall_f1_visualizations(\n",
        "        class_performance, per_class_precision, per_class_recall, per_class_f1,\n",
        "        per_class_support, macro_precision, macro_recall, macro_f1,\n",
        "        micro_precision, micro_recall, micro_f1,\n",
        "        weighted_precision, weighted_recall, weighted_f1,\n",
        "        acc_test, acc_top3, acc_top5, correlation,\n",
        "        mean_support, median_support, support_values  # Added these variables\n",
        "    )\n",
        "\n",
        "    # Save detailed metrics\n",
        "    metrics_summary = {\n",
        "        'overall': {\n",
        "            'accuracy': acc_test,\n",
        "            'top3_accuracy': acc_top3,\n",
        "            'top5_accuracy': acc_top5\n",
        "        },\n",
        "        'macro_averaged': {\n",
        "            'precision': macro_precision,\n",
        "            'recall': macro_recall,\n",
        "            'f1_score': macro_f1\n",
        "        },\n",
        "        'micro_averaged': {\n",
        "            'precision': micro_precision,\n",
        "            'recall': micro_recall,\n",
        "            'f1_score': micro_f1\n",
        "        },\n",
        "        'weighted_averaged': {\n",
        "            'precision': weighted_precision,\n",
        "            'recall': weighted_recall,\n",
        "            'f1_score': weighted_f1\n",
        "        },\n",
        "        'per_class': class_performance,\n",
        "        'class_imbalance': {\n",
        "            'mean_support': mean_support,\n",
        "            'median_support': median_support,\n",
        "            'std_support': std_support,\n",
        "            'correlation_with_f1': correlation\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save to file\n",
        "    metrics_path = os.path.join(DRIVE_ANALYSIS_DIR, 'stage2_precision_recall_f1_analysis.json')\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        json.dump(metrics_summary, f, indent=2, default=str)\n",
        "\n",
        "    print(f\"\\nDetailed metrics saved to: {metrics_path}\")\n",
        "\n",
        "    return metrics_summary\n",
        "\n",
        "def create_precision_recall_f1_visualizations(\n",
        "    class_performance, per_class_precision, per_class_recall, per_class_f1,\n",
        "    per_class_support, macro_precision, macro_recall, macro_f1,\n",
        "    micro_precision, micro_recall, micro_f1,\n",
        "    weighted_precision, weighted_recall, weighted_f1,\n",
        "    acc_test, acc_top3, acc_top5, correlation,\n",
        "    mean_support, median_support, support_values  # Added these parameters\n",
        "):\n",
        "    \"\"\"Create comprehensive visualizations for precision, recall, and F1-score analysis\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" CREATING PRECISION, RECALL, F1-SCORE VISUALIZATIONS\")\n",
        "    print(\" Multi-Class Disease Classification Performance Analysis\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Set up the plotting style\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "\n",
        "    # Create a comprehensive figure\n",
        "    fig = plt.figure(figsize=(20, 16))\n",
        "    fig.suptitle('Stage 2: Multi-Class Disease Classification Performance Analysis\\n'\n",
        "                 'Precision, Recall, and F1-Score Comprehensive Analysis',\n",
        "                 fontsize=18, fontweight='bold')\n",
        "\n",
        "    # Plot 1: Overall Performance Metrics Comparison\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    overall_metrics = ['Accuracy', 'Top-3', 'Top-5']\n",
        "    overall_values = [acc_test, acc_top3, acc_top5]\n",
        "    colors = ['skyblue', 'lightgreen', 'plum']\n",
        "\n",
        "    bars = ax1.bar(overall_metrics, overall_values, color=colors, alpha=0.8)\n",
        "    ax1.set_ylabel('Score')\n",
        "    ax1.set_title('Overall Performance Metrics')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim(0, 1)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, overall_values):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{value:.3f}',\n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 2: Averaging Methods Comparison\n",
        "    ax2 = plt.subplot(3, 3, 2)\n",
        "    averaging_methods = ['Macro', 'Micro', 'Weighted']\n",
        "    precision_values = [macro_precision, micro_precision, weighted_precision]\n",
        "    recall_values = [macro_recall, micro_recall, weighted_recall]\n",
        "    f1_values = [macro_f1, micro_f1, weighted_f1]\n",
        "\n",
        "    x = np.arange(len(averaging_methods))\n",
        "    width = 0.25\n",
        "\n",
        "    bars1 = ax2.bar(x - width, precision_values, width, label='Precision', alpha=0.8, color='skyblue')\n",
        "    bars2 = ax2.bar(x, recall_values, width, label='Recall', alpha=0.8, color='lightcoral')\n",
        "    bars3 = ax2.bar(x + width, f1_values, width, label='F1-Score', alpha=0.8, color='lightgreen')\n",
        "\n",
        "    ax2.set_xlabel('Averaging Method')\n",
        "    ax2.set_ylabel('Score')\n",
        "    ax2.set_title('Performance Metrics by Averaging Method')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels(averaging_methods)\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_ylim(0, 1)\n",
        "\n",
        "    # Plot 3: Top 20 Classes Performance\n",
        "    ax3 = plt.subplot(3, 3, 3)\n",
        "    top_20 = class_performance[:20]\n",
        "    class_names = [perf['class_name'][:15] for perf in top_20]\n",
        "    f1_scores = [perf['f1_score'] for perf in top_20]\n",
        "\n",
        "    bars = ax3.barh(range(len(class_names)), f1_scores, color='lightgreen', alpha=0.8)\n",
        "    ax3.set_yticks(range(len(class_names)))\n",
        "    ax3.set_yticklabels(class_names)\n",
        "    ax3.set_xlabel('F1-Score')\n",
        "    ax3.set_title('Top 20 Performing Classes')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    ax3.set_xlim(0, 1)\n",
        "\n",
        "    # Plot 4: Bottom 20 Classes Performance\n",
        "    ax4 = plt.subplot(3, 3, 4)\n",
        "    bottom_20 = class_performance[-20:]\n",
        "    class_names_bottom = [perf['class_name'][:15] for perf in bottom_20]\n",
        "    f1_scores_bottom = [perf['f1_score'] for perf in bottom_20]\n",
        "\n",
        "    bars = ax4.barh(range(len(class_names_bottom)), f1_scores_bottom, color='lightcoral', alpha=0.8)\n",
        "    ax4.set_yticks(range(len(class_names_bottom)))\n",
        "    ax4.set_yticklabels(class_names_bottom)\n",
        "    ax4.set_xlabel('F1-Score')\n",
        "    ax4.set_title('Bottom 20 Performing Classes')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    ax4.set_xlim(0, 1)\n",
        "\n",
        "    # Plot 5: Precision vs Recall Scatter Plot\n",
        "    ax5 = plt.subplot(3, 3, 5)\n",
        "    ax5.scatter(per_class_precision, per_class_recall, alpha=0.6, s=30, c=per_class_f1, cmap='RdYlGn')\n",
        "\n",
        "    # Add diagonal line for equal precision/recall\n",
        "    ax5.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Equal Precision/Recall')\n",
        "\n",
        "    ax5.set_xlabel('Precision')\n",
        "    ax5.set_ylabel('Recall')\n",
        "    ax5.set_title('Precision vs Recall by Class (colored by F1-Score)')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add colorbar\n",
        "    scatter = ax5.scatter(per_class_precision, per_class_recall, alpha=0.6, s=30, c=per_class_f1, cmap='RdYlGn')\n",
        "    plt.colorbar(scatter, ax=ax5, shrink=0.8, label='F1-Score')\n",
        "\n",
        "    # Plot 6: Class Support Distribution\n",
        "    ax6 = plt.subplot(3, 3, 6)\n",
        "    ax6.hist(per_class_support, bins=30, alpha=0.7, color='lightsteelblue', edgecolor='black')\n",
        "    ax6.axvline(mean_support, color='red', linestyle='--', linewidth=2,\n",
        "                label=f'Mean: {mean_support:.1f}')\n",
        "    ax6.axvline(median_support, color='orange', linestyle='--', linewidth=2,\n",
        "                label=f'Median: {median_support:.1f}')\n",
        "\n",
        "    ax6.set_xlabel('Samples per Class')\n",
        "    ax6.set_ylabel('Number of Classes')\n",
        "    ax6.set_title('Class Support Distribution')\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 7: Performance vs Class Support\n",
        "    ax7 = plt.subplot(3, 3, 7)\n",
        "    ax7.scatter(per_class_support, per_class_f1, alpha=0.6, s=30, color='purple')\n",
        "\n",
        "    # Add trend line\n",
        "    z = np.polyfit(per_class_support, per_class_f1, 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax7.plot(per_class_support, p(per_class_support), \"r--\", alpha=0.8,\n",
        "             label=f'Correlation: {correlation:.3f}')\n",
        "\n",
        "    ax7.set_xlabel('Class Support (Number of Samples)')\n",
        "    ax7.set_ylabel('F1-Score')\n",
        "    ax7.set_title('Performance vs Class Support')\n",
        "    ax7.legend()\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 8: Metrics Distribution by Class\n",
        "    ax8 = plt.subplot(3, 3, 8)\n",
        "    metrics_data = [per_class_precision, per_class_recall, per_class_f1]\n",
        "    metric_labels = ['Precision', 'Recall', 'F1-Score']\n",
        "    colors_metrics = ['skyblue', 'lightcoral', 'lightgreen']\n",
        "\n",
        "    bp = ax8.boxplot(metrics_data, labels=metric_labels, patch_artist=True)\n",
        "    for patch, color in zip(bp['boxes'], colors_metrics):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "    ax8.set_ylabel('Score')\n",
        "    ax8.set_title('Distribution of Metrics Across Classes')\n",
        "    ax8.grid(True, alpha=0.3)\n",
        "    ax8.set_ylim(0, 1)\n",
        "\n",
        "    # Plot 9: Statistical Summary\n",
        "    ax9 = plt.subplot(3, 3, 9)\n",
        "    ax9.axis('off')\n",
        "\n",
        "    summary_text = \"PERFORMANCE ANALYSIS SUMMARY:\\n\\n\"\n",
        "    summary_text += f\"Overall Accuracy: {acc_test:.4f}\\n\"\n",
        "    summary_text += f\"Top-3 Accuracy: {acc_top3:.4f}\\n\"\n",
        "    summary_text += f\"Top-5 Accuracy: {acc_top5:.4f}\\n\\n\"\n",
        "\n",
        "    summary_text += \"AVERAGING METHODS:\\n\"\n",
        "    summary_text += f\"Macro F1: {macro_f1:.4f}\\n\"\n",
        "    summary_text += f\"Micro F1: {micro_f1:.4f}\\n\"\n",
        "    summary_text += f\"Weighted F1: {weighted_f1:.4f}\\n\\n\"\n",
        "\n",
        "    summary_text += \"CLASS IMBALANCE:\\n\"\n",
        "    summary_text += f\"Mean Support: {mean_support:.1f}\\n\"\n",
        "    summary_text += f\"Classes < 10: {(support_values < 10).sum()}\\n\"\n",
        "    summary_text += f\"Classes > 100: {(support_values > 100).sum()}\\n\\n\"\n",
        "\n",
        "    summary_text += f\"Correlation: {correlation:.3f}\"\n",
        "\n",
        "    ax9.text(0.05, 0.95, summary_text, transform=ax9.transAxes, fontsize=9,\n",
        "             verticalalignment='top', fontfamily='monospace',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the comprehensive visualization\n",
        "    save_path = '/content/drive/MyDrive/plantwild_stage1_models/stage2_precision_recall_f1_analysis_comprehensive.png'\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\" Comprehensive precision/recall/F1 visualization saved to: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Run enhanced precision, recall, F1 analysis with plots\n",
        "precision_recall_f1_analysis = comprehensive_precision_recall_f1_analysis()"
      ],
      "metadata": {
        "id": "RUPH6fUiZzoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QLXz796ONQA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **STATISTICAL SIGNIFICANCE ANALYSIS FOR MULTI-CLASS + PLOTS**\n",
        "\n",
        "def statistical_significance_analysis_multi_class():\n",
        "    \"\"\"Perform statistical significance analysis for multi-class classification with visualizations\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" STATISTICAL SIGNIFICANCE ANALYSIS - MULTI-CLASS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Calculate per-class metrics\n",
        "    per_class_precision, per_class_recall, per_class_f1, per_class_support = precision_recall_fscore_support(\n",
        "        y_test, yhat_test, average=None, zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate macro-averaged metrics\n",
        "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
        "        y_test, yhat_test, average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate micro-averaged metrics\n",
        "    micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
        "        y_test, yhat_test, average='micro', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate per-class accuracy\n",
        "    per_class_acc = []\n",
        "    for i in range(num_classes):\n",
        "        class_mask = (y_test == i)\n",
        "        if class_mask.sum() > 0:\n",
        "            class_acc = (yhat_test[class_mask] == y_test[class_mask]).mean()\n",
        "            per_class_acc.append(class_acc)\n",
        "        else:\n",
        "            per_class_acc.append(0.0)\n",
        "\n",
        "    per_class_acc = np.array(per_class_acc)\n",
        "\n",
        "    print(f\"PER-CLASS PERFORMANCE ANALYSIS:\")\n",
        "    print(f\"  Number of classes: {num_classes}\")\n",
        "    print(f\"  Mean F1-Score: {per_class_f1.mean():.4f}\")\n",
        "    print(f\"  Std F1-Score: {per_class_f1.std():.4f}\")\n",
        "    print(f\"  Mean Precision: {per_class_precision.mean():.4f}\")\n",
        "    print(f\"  Std Precision: {per_class_precision.std():.4f}\")\n",
        "    print(f\"  Mean Recall: {per_class_recall.mean():.4f}\")\n",
        "    print(f\"  Std Recall: {per_class_recall.std():.4f}\")\n",
        "    print(f\"  Mean Accuracy: {per_class_acc.mean():.4f}\")\n",
        "    print(f\"  Std Accuracy: {per_class_acc.std():.4f}\")\n",
        "\n",
        "    print(f\"\\nAVERAGING METHODS COMPARISON:\")\n",
        "    print(f\"  Macro-Averaged:\")\n",
        "    print(f\"    Precision: {macro_precision:.4f}\")\n",
        "    print(f\"    Recall: {macro_recall:.4f}\")\n",
        "    print(f\"    F1-Score: {macro_f1:.4f}\")\n",
        "    print(f\"  Micro-Averaged:\")\n",
        "    print(f\"    Precision: {micro_precision:.4f}\")\n",
        "    print(f\"    Recall: {micro_recall:.4f}\")\n",
        "    print(f\"    F1-Score: {micro_f1:.4f}\")\n",
        "\n",
        "    # Statistical tests\n",
        "    print(f\"\\nSTATISTICAL ANALYSIS:\")\n",
        "\n",
        "    # Test for normality (Shapiro-Wilk)\n",
        "    try:\n",
        "        f1_stat, f1_p = stats.shapiro(per_class_f1)\n",
        "        print(f\"  F1-Score normality test (Shapiro-Wilk):\")\n",
        "        print(f\"    Statistic: {f1_stat:.4f}\")\n",
        "        print(f\"    P-value: {f1_p:.6f}\")\n",
        "        print(f\"    Normal distribution: {'Yes' if f1_p > 0.05 else 'No'}\")\n",
        "    except:\n",
        "        print(f\"  F1-Score normality test failed\")\n",
        "\n",
        "    # Confidence intervals for mean performance\n",
        "    f1_ci = stats.t.interval(0.95, len(per_class_f1)-1,\n",
        "                            loc=per_class_f1.mean(),\n",
        "                            scale=stats.sem(per_class_f1))\n",
        "\n",
        "    prec_ci = stats.t.interval(0.95, len(per_class_precision)-1,\n",
        "                              loc=per_class_precision.mean(),\n",
        "                              scale=stats.sem(per_class_precision))\n",
        "\n",
        "    rec_ci = stats.t.interval(0.95, len(per_class_recall)-1,\n",
        "                             loc=per_class_recall.mean(),\n",
        "                             scale=stats.sem(per_class_recall))\n",
        "\n",
        "    acc_ci = stats.t.interval(0.95, len(per_class_acc)-1,\n",
        "                             loc=per_class_acc.mean(),\n",
        "                             scale=stats.sem(per_class_acc))\n",
        "\n",
        "    print(f\"\\nCONFIDENCE INTERVALS (95%):\")\n",
        "    print(f\"  F1-Score: [{f1_ci[0]:.4f}, {f1_ci[1]:.4f}]\")\n",
        "    print(f\"  Precision: [{prec_ci[0]:.4f}, {prec_ci[1]:.4f}]\")\n",
        "    print(f\"  Recall: [{rec_ci[0]:.4f}, {rec_ci[1]:.4f}]\")\n",
        "    print(f\"  Accuracy: [{acc_ci[0]:.4f}, {acc_ci[1]:.4f}]\")\n",
        "\n",
        "    # Effect size analysis\n",
        "    print(f\"\\nEFFECT SIZE ANALYSIS:\")\n",
        "\n",
        "    # Calculate Cohen's d for class performance variation\n",
        "    f1_range = per_class_f1.max() - per_class_f1.min()\n",
        "    f1_cohens_d = f1_range / per_class_f1.std()\n",
        "\n",
        "    print(f\"  F1-Score variation (Cohen's d): {f1_cohens_d:.4f}\")\n",
        "\n",
        "    if f1_cohens_d < 0.2:\n",
        "        effect_size = \"Negligible\"\n",
        "    elif f1_cohens_d < 0.5:\n",
        "        effect_size = \"Small\"\n",
        "    elif f1_cohens_d < 0.8:\n",
        "        effect_size = \"Medium\"\n",
        "    else:\n",
        "        effect_size = \"Large\"\n",
        "\n",
        "    print(f\"  Effect Size: {effect_size}\")\n",
        "\n",
        "    # ===== CREATE VISUALIZATIONS =====\n",
        "    print(f\"\\nCreating statistical analysis visualizations...\")\n",
        "\n",
        "    # Create comprehensive subplot grid\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "\n",
        "    # 1. Per-Class F1-Score Distribution\n",
        "    axes[0, 0].hist(per_class_f1, bins=20, color='skyblue', alpha=0.7, edgecolor='black')\n",
        "    axes[0, 0].axvline(per_class_f1.mean(), color='red', linestyle='--',\n",
        "                       label=f'Mean: {per_class_f1.mean():.3f}')\n",
        "    axes[0, 0].axvline(per_class_f1.mean() + per_class_f1.std(), color='orange', linestyle='--',\n",
        "                       label=f'+1 Std: {per_class_f1.mean() + per_class_f1.std():.3f}')\n",
        "    axes[0, 0].axvline(per_class_f1.mean() - per_class_f1.std(), color='orange', linestyle='--',\n",
        "                       label=f'-1 Std: {per_class_f1.mean() - per_class_f1.std():.3f}')\n",
        "    axes[0, 0].set_title('Per-Class F1-Score Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('F1-Score', fontsize=12)\n",
        "    axes[0, 0].set_ylabel('Number of Classes', fontsize=12)\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Per-Class Precision Distribution\n",
        "    axes[0, 1].hist(per_class_precision, bins=20, color='lightgreen', alpha=0.7, edgecolor='black')\n",
        "    axes[0, 1].axvline(per_class_precision.mean(), color='red', linestyle='--',\n",
        "                       label=f'Mean: {per_class_precision.mean():.3f}')\n",
        "    axes[0, 1].set_title('Per-Class Precision Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Precision', fontsize=12)\n",
        "    axes[0, 1].set_ylabel('Number of Classes', fontsize=12)\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Per-Class Recall Distribution\n",
        "    axes[0, 2].hist(per_class_recall, bins=20, color='lightcoral', alpha=0.7, edgecolor='black')\n",
        "    axes[0, 2].axvline(per_class_recall.mean(), color='red', linestyle='--',\n",
        "                       label=f'Mean: {per_class_recall.mean():.3f}')\n",
        "    axes[0, 2].set_title('Per-Class Recall Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[0, 2].set_xlabel('Recall', fontsize=12)\n",
        "    axes[0, 2].set_ylabel('Number of Classes', fontsize=12)\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Averaging Methods Comparison\n",
        "    averaging_methods = ['Macro', 'Micro']\n",
        "    precision_values = [macro_precision, micro_precision]\n",
        "    recall_values = [macro_recall, micro_recall]\n",
        "    f1_values = [macro_f1, micro_f1]\n",
        "\n",
        "    x_pos = np.arange(len(averaging_methods))\n",
        "    width = 0.25\n",
        "\n",
        "    axes[1, 0].bar(x_pos - width, precision_values, width, label='Precision', color='#FF6B6B', alpha=0.8)\n",
        "    axes[1, 0].bar(x_pos, recall_values, width, label='Recall', color='#4ECDC4', alpha=0.8)\n",
        "    axes[1, 0].bar(x_pos + width, f1_values, width, label='F1-Score', color='#45B7D1', alpha=0.8)\n",
        "\n",
        "    axes[1, 0].set_title('Averaging Methods Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Averaging Method', fontsize=12)\n",
        "    axes[1, 0].set_ylabel('Score', fontsize=12)\n",
        "    axes[1, 0].set_xticks(x_pos)\n",
        "    axes[1, 0].set_xticklabels(averaging_methods)\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 5. Performance vs Class Support Scatter\n",
        "    support_values = np.array(per_class_support)\n",
        "    axes[1, 1].scatter(support_values, per_class_f1, alpha=0.7, color='purple', s=50)\n",
        "    axes[1, 1].set_title('F1-Score vs Class Support', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Number of Samples (Support)', fontsize=12)\n",
        "    axes[1, 1].set_ylabel('F1-Score', fontsize=12)\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add trend line\n",
        "    z = np.polyfit(support_values, per_class_f1, 1)\n",
        "    p = np.poly1d(z)\n",
        "    correlation = np.corrcoef(support_values, per_class_f1)[0, 1]\n",
        "    axes[1, 1].plot(support_values, p(support_values), \"r--\", alpha=0.8,\n",
        "                     label=f\"Trend (r={correlation:.3f})\")\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    # 6. Confidence Intervals Visualization\n",
        "    metrics = ['F1-Score', 'Precision', 'Recall', 'Accuracy']\n",
        "    means = [per_class_f1.mean(), per_class_precision.mean(), per_class_recall.mean(), per_class_acc.mean()]\n",
        "    ci_lower = [f1_ci[0], prec_ci[0], rec_ci[0], acc_ci[0]]\n",
        "    ci_upper = [f1_ci[1], prec_ci[1], rec_ci[1], acc_ci[1]]\n",
        "\n",
        "    x_pos = np.arange(len(metrics))\n",
        "    yerr = np.array([np.array(means) - np.array(ci_lower), np.array(ci_upper) - np.array(means)])\n",
        "\n",
        "    axes[1, 2].errorbar(x_pos, means, yerr=yerr, fmt='o', capsize=5, capthick=2,\n",
        "                        markersize=8, color='blue', alpha=0.8)\n",
        "    axes[1, 2].set_title('95% Confidence Intervals', fontsize=14, fontweight='bold')\n",
        "    axes[1, 2].set_xlabel('Metrics', fontsize=12)\n",
        "    axes[1, 2].set_ylabel('Score', fontsize=12)\n",
        "    axes[1, 2].set_xticks(x_pos)\n",
        "    axes[1, 2].set_xticklabels(metrics, rotation=45)\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, (mean, ci_l, ci_u) in enumerate(zip(means, ci_lower, ci_upper)):\n",
        "        axes[1, 2].text(i, mean + yerr[1, i] + 0.01, f'{mean:.3f}',\n",
        "                        ha='center', va='bottom', fontweight='bold')\n",
        "        axes[1, 2].text(i, mean - yerr[0, i] - 0.01, f'[{ci_l:.3f}, {ci_u:.3f}]',\n",
        "                        ha='center', va='top', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save visualization\n",
        "    stats_viz_path = os.path.join(DRIVE_VISUALIZATIONS_DIR, 'stage2_statistical_analysis.png')\n",
        "    plt.savefig(stats_viz_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Statistical analysis visualizations saved to: {stats_viz_path}\")\n",
        "\n",
        "    return {\n",
        "        'per_class_f1': per_class_f1,\n",
        "        'per_class_precision': per_class_precision,\n",
        "        'per_class_recall': per_class_recall,\n",
        "        'per_class_acc': per_class_acc,\n",
        "        'macro_metrics': {'precision': macro_precision, 'recall': macro_recall, 'f1': macro_f1},\n",
        "        'micro_metrics': {'precision': micro_precision, 'recall': micro_recall, 'f1': micro_f1},\n",
        "        'f1_ci': f1_ci,\n",
        "        'prec_ci': prec_ci,\n",
        "        'rec_ci': rec_ci,\n",
        "        'acc_ci': acc_ci,\n",
        "        'cohens_d': f1_cohens_d\n",
        "    }\n",
        "\n",
        "# Run statistical analysis with plots\n",
        "stats_results = statistical_significance_analysis_multi_class()"
      ],
      "metadata": {
        "id": "qM29oX9WI6vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **CROSS-VALIDATION ANALYSIS FOR MULTI-CLASS + PLOTS**\n",
        "\n",
        "def cross_validation_analysis_multi_class(k_folds=5):\n",
        "    \"\"\"Perform k-fold cross-validation for multi-class classification with visualizations\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" CROSS-VALIDATION ANALYSIS - MULTI-CLASS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "    # Prepare data for cross-validation\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    print(f\"Preparing {len(test_df)} test samples for cross-validation...\")\n",
        "\n",
        "    for _, sample in test_df.iterrows():\n",
        "        try:\n",
        "            img = tf.keras.preprocessing.image.load_img(\n",
        "                sample['image_path_absolute'], target_size=(224, 224)\n",
        "            )\n",
        "            img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "            img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
        "            X.append(img_array)\n",
        "            y.append(sample['encoded_class_id'])\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {sample['image_path_absolute']}: {e}\")\n",
        "            continue\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    print(f\"Data prepared: X shape {X.shape}, y shape {y.shape}\")\n",
        "\n",
        "    # K-fold cross-validation\n",
        "    kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    cv_scores = []\n",
        "\n",
        "    print(f\"\\nPerforming {k_folds}-fold cross-validation...\")\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
        "\n",
        "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Evaluate on validation fold\n",
        "        predictions = model.predict(X_val_fold, verbose=0)\n",
        "        pred_binary = predictions.argmax(axis=1)\n",
        "\n",
        "        # Calculate all metrics\n",
        "        accuracy = (pred_binary == y_val_fold).mean()\n",
        "\n",
        "        # Calculate precision, recall, F1 for this fold\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            y_val_fold, pred_binary, average='macro', zero_division=0\n",
        "        )\n",
        "\n",
        "        # Calculate micro-averaged metrics\n",
        "        micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
        "            y_val_fold, pred_binary, average='micro', zero_division=0\n",
        "        )\n",
        "\n",
        "        cv_scores.append({\n",
        "            'fold': fold + 1,\n",
        "            'accuracy': accuracy,\n",
        "            'f1_score': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'micro_precision': micro_precision,\n",
        "            'micro_recall': micro_recall,\n",
        "            'micro_f1': micro_f1\n",
        "        })\n",
        "\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"  F1-Score (Macro): {f1:.4f}\")\n",
        "        print(f\"  Precision (Macro): {precision:.4f}\")\n",
        "        print(f\"  Recall (Macro): {recall:.4f}\")\n",
        "        print(f\"  F1-Score (Micro): {micro_f1:.4f}\")\n",
        "\n",
        "    # Cross-validation summary\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\" CROSS-VALIDATION SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    accuracies = [score['accuracy'] for score in cv_scores]\n",
        "    f1_scores = [score['f1_score'] for score in cv_scores]\n",
        "    precisions = [score['precision'] for score in cv_scores]\n",
        "    recalls = [score['recall'] for score in cv_scores]\n",
        "    micro_f1s = [score['micro_f1'] for score in cv_scores]\n",
        "\n",
        "    print(f\"Accuracy: {np.mean(accuracies):.4f}  {np.std(accuracies):.4f}\")\n",
        "    print(f\"F1-Score (Macro): {np.mean(f1_scores):.4f}  {np.std(f1_scores):.4f}\")\n",
        "    print(f\"Precision (Macro): {np.mean(precisions):.4f}  {np.std(precisions):.4f}\")\n",
        "    print(f\"Recall (Macro): {np.mean(recalls):.4f}  {np.std(recalls):.4f}\")\n",
        "    print(f\"F1-Score (Micro): {np.mean(micro_f1s):.4f}  {np.std(micro_f1s):.4f}\")\n",
        "\n",
        "    # Stability assessment\n",
        "    stability = 'Good' if np.std(accuracies) < 0.05 else 'Moderate' if np.std(accuracies) < 0.1 else 'Poor'\n",
        "    print(f\"Stability: {stability}\")\n",
        "\n",
        "    # ===== CREATE VISUALIZATIONS =====\n",
        "    print(f\"\\nCreating cross-validation visualizations...\")\n",
        "\n",
        "    # Create comprehensive subplot grid\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # 1. Fold-by-Fold Performance Comparison\n",
        "    fold_numbers = [score['fold'] for score in cv_scores]\n",
        "\n",
        "    axes[0, 0].plot(fold_numbers, accuracies, 'o-', linewidth=2, markersize=8,\n",
        "                     color='blue', label='Accuracy', alpha=0.8)\n",
        "    axes[0, 0].plot(fold_numbers, f1_scores, 's-', linewidth=2, markersize=8,\n",
        "                     color='red', label='F1-Score (Macro)', alpha=0.8)\n",
        "    axes[0, 0].plot(fold_numbers, micro_f1s, '^-', linewidth=2, markersize=8,\n",
        "                     color='green', label='F1-Score (Micro)', alpha=0.8)\n",
        "\n",
        "    axes[0, 0].set_title('Fold-by-Fold Performance Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Fold Number', fontsize=12)\n",
        "    axes[0, 0].set_ylabel('Score', fontsize=12)\n",
        "    axes[0, 0].set_xticks(fold_numbers)\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add mean lines\n",
        "    axes[0, 0].axhline(np.mean(accuracies), color='blue', linestyle='--', alpha=0.5,\n",
        "                       label=f'Mean Acc: {np.mean(accuracies):.3f}')\n",
        "    axes[0, 0].axhline(np.mean(f1_scores), color='red', linestyle='--', alpha=0.5,\n",
        "                       label=f'Mean F1: {np.mean(f1_scores):.3f}')\n",
        "\n",
        "    # 2. Precision vs Recall by Fold\n",
        "    axes[0, 1].scatter(precisions, recalls, s=100, c=fold_numbers, cmap='viridis',\n",
        "                        alpha=0.8, edgecolors='black', linewidth=2)\n",
        "\n",
        "    # Add fold labels\n",
        "    for i, fold in enumerate(fold_numbers):\n",
        "        axes[0, 1].annotate(f'Fold {fold}', (precisions[i], recalls[i]),\n",
        "                           xytext=(5, 5), textcoords='offset points',\n",
        "                           fontsize=10, fontweight='bold')\n",
        "\n",
        "    axes[0, 1].set_title('Precision vs Recall by Fold', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Precision (Macro)', fontsize=12)\n",
        "    axes[0, 1].set_ylabel('Recall (Macro)', fontsize=12)\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add colorbar\n",
        "    scatter = axes[0, 1].scatter(precisions, recalls, s=100, c=fold_numbers, cmap='viridis',\n",
        "                                 alpha=0.8, edgecolors='black', linewidth=2)\n",
        "    cbar = plt.colorbar(scatter, ax=axes[0, 1])\n",
        "    cbar.set_label('Fold Number', fontsize=12)\n",
        "\n",
        "    # 3. Performance Stability Analysis\n",
        "    metrics_names = ['Accuracy', 'F1-Score\\n(Macro)', 'Precision\\n(Macro)', 'Recall\\n(Macro)', 'F1-Score\\n(Micro)']\n",
        "    metrics_values = [accuracies, f1_scores, precisions, recalls, micro_f1s]\n",
        "    metrics_means = [np.mean(values) for values in metrics_values]\n",
        "    metrics_stds = [np.std(values) for values in metrics_values]\n",
        "\n",
        "    x_pos = np.arange(len(metrics_names))\n",
        "    bars = axes[1, 0].bar(x_pos, metrics_means, yerr=metrics_stds, capsize=5,\n",
        "                           color=['blue', 'red', 'green', 'orange', 'purple'], alpha=0.8)\n",
        "\n",
        "    axes[1, 0].set_title('Performance Stability Across Folds', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Metrics', fontsize=12)\n",
        "    axes[1, 0].set_ylabel('Score', fontsize=12)\n",
        "    axes[1, 0].set_xticks(x_pos)\n",
        "    axes[1, 0].set_xticklabels(metrics_names, rotation=45, ha='right')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, mean, std in zip(bars, metrics_means, metrics_stds):\n",
        "        height = bar.get_height()\n",
        "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + std + 0.01,\n",
        "                        f'{mean:.3f}\\n{std:.3f}', ha='center', va='bottom',\n",
        "                        fontweight='bold', fontsize=10)\n",
        "\n",
        "    # 4. Fold Performance Heatmap\n",
        "    # Prepare data for heatmap\n",
        "    heatmap_data = np.array([accuracies, f1_scores, precisions, recalls, micro_f1s])\n",
        "    heatmap_labels = ['Accuracy', 'F1-Score\\n(Macro)', 'Precision\\n(Macro)', 'Recall\\n(Macro)', 'F1-Score\\n(Micro)']\n",
        "\n",
        "    im = axes[1, 1].imshow(heatmap_data, cmap='YlOrRd', aspect='auto', alpha=0.8)\n",
        "\n",
        "    # Set labels\n",
        "    axes[1, 1].set_xticks(range(len(fold_numbers)))\n",
        "    axes[1, 1].set_xticklabels([f'Fold {f}' for f in fold_numbers])\n",
        "    axes[1, 1].set_yticks(range(len(heatmap_labels)))\n",
        "    axes[1, 1].set_yticklabels(heatmap_labels)\n",
        "\n",
        "    # Add text annotations\n",
        "    for i in range(len(heatmap_labels)):\n",
        "        for j in range(len(fold_numbers)):\n",
        "            text = axes[1, 1].text(j, i, f'{heatmap_data[i, j]:.3f}',\n",
        "                                  ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "\n",
        "    axes[1, 1].set_title('Cross-Validation Performance Heatmap', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(im, ax=axes[1, 1])\n",
        "    cbar.set_label('Score Value', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save visualization\n",
        "    cv_viz_path = os.path.join(DRIVE_VISUALIZATIONS_DIR, 'stage2_cross_validation_analysis.png')\n",
        "    plt.savefig(cv_viz_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Cross-validation visualizations saved to: {cv_viz_path}\")\n",
        "\n",
        "    return cv_scores\n",
        "\n",
        "# Run cross-validation with plots\n",
        "cv_results = cross_validation_analysis_multi_class()"
      ],
      "metadata": {
        "id": "0eHM-punI6pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **ERROR ANALYSIS AND MISCLASSIFICATION STUDY - MULTI-CLASS + PLOTS**\n",
        "\n",
        "def error_analysis_multi_class():\n",
        "    \"\"\"Analyze errors and misclassifications for multi-class classification with visualizations\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" ERROR ANALYSIS AND MISCLASSIFICATION STUDY - MULTI-CLASS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Analyze test predictions\n",
        "    total_samples = len(y_test)\n",
        "    correct_predictions = (yhat_test == y_test).sum()\n",
        "    misclassifications = total_samples - correct_predictions\n",
        "\n",
        "    print(f\"Total test samples: {total_samples}\")\n",
        "    print(f\"Correct predictions: {correct_predictions} ({correct_predictions/total_samples*100:.1f}%)\")\n",
        "    print(f\"Misclassifications: {misclassifications} ({misclassifications/total_samples*100:.1f}%)\")\n",
        "\n",
        "    # Calculate per-class error rates\n",
        "    print(f\"\\nPER-CLASS ERROR ANALYSIS:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    class_errors = {}\n",
        "    for i in range(num_classes):\n",
        "        class_mask = (y_test == i)\n",
        "        if class_mask.sum() > 0:\n",
        "            class_correct = (yhat_test[class_mask] == y_test[class_mask]).sum()\n",
        "            class_total = class_mask.sum()\n",
        "            class_error_rate = (class_total - class_correct) / class_total\n",
        "\n",
        "            # Calculate per-class precision, recall, F1\n",
        "            class_precision, class_recall, class_f1, _ = precision_recall_fscore_support(\n",
        "                y_test, yhat_test, labels=[i], average=None, zero_division=0\n",
        "            )\n",
        "\n",
        "            class_errors[i] = {\n",
        "                'total': class_total,\n",
        "                'correct': class_correct,\n",
        "                'errors': class_total - class_correct,\n",
        "                'error_rate': class_error_rate,\n",
        "                'precision': class_precision[0],\n",
        "                'recall': class_recall[0],\n",
        "                'f1_score': class_f1[0]\n",
        "            }\n",
        "\n",
        "    # Sort by error rate\n",
        "    sorted_errors = sorted(class_errors.items(), key=lambda x: x[1]['error_rate'], reverse=True)\n",
        "\n",
        "    print(f\"Top 10 Classes with Highest Error Rates:\")\n",
        "    print(f\"{'Class ID':<8} {'Class Name':<35} {'Error Rate':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for i, (class_id, error_info) in enumerate(sorted_errors[:10]):\n",
        "        class_name = idx2name.get(class_id, f\"Class_{class_id}\")\n",
        "        print(f\"{class_id:<8} {class_name[:34]:<35} {error_info['error_rate']:<12.1%} {error_info['precision']:<12.4f} {error_info['recall']:<12.4f} {error_info['f1_score']:<12.4f}\")\n",
        "\n",
        "    # Analyze misclassifications by class\n",
        "    print(f\"\\nMISCLASSIFICATION PATTERN ANALYSIS:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Create confusion matrix for detailed analysis\n",
        "    cm = confusion_matrix(y_test, yhat_test, labels=range(num_classes))\n",
        "\n",
        "    # Find most confused class pairs\n",
        "    most_confused = []\n",
        "    for i in range(num_classes):\n",
        "        for j in range(num_classes):\n",
        "            if i != j and cm[i, j] > 0:\n",
        "                most_confused.append({\n",
        "                    'true_class': i,\n",
        "                    'predicted_class': j,\n",
        "                    'count': cm[i, j],\n",
        "                    'true_class_name': idx2name.get(i, f\"Class_{i}\"),\n",
        "                    'predicted_class_name': idx2name.get(j, f\"Class_{j}\"),\n",
        "                    'error_rate': cm[i, j] / cm[i, :].sum() if cm[i, :].sum() > 0 else 0\n",
        "                })\n",
        "\n",
        "    # Sort by confusion count\n",
        "    most_confused.sort(key=lambda x: x['count'], reverse=True)\n",
        "\n",
        "    print(f\"Top 10 Most Confused Class Pairs:\")\n",
        "    for i, confusion in enumerate(most_confused[:10]):\n",
        "        print(f\"  {i+1:2d}. {confusion['true_class_name'][:25]:<25}  {confusion['predicted_class_name'][:25]:<25}\")\n",
        "        print(f\"      Count: {confusion['count']} samples, Error Rate: {confusion['error_rate']:.1%}\")\n",
        "\n",
        "    # Analyze confidence vs accuracy\n",
        "    print(f\"\\nCONFIDENCE ANALYSIS:\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Get prediction confidences\n",
        "    confidences = p_test.max(axis=1)\n",
        "    correct_mask = (yhat_test == y_test)\n",
        "\n",
        "    # High confidence errors\n",
        "    high_conf_threshold = 0.8\n",
        "    high_conf_mask = confidences >= high_conf_threshold\n",
        "    high_conf_errors = high_conf_mask & ~correct_mask\n",
        "\n",
        "    print(f\"High confidence errors ({high_conf_threshold}): {high_conf_errors.sum()}\")\n",
        "    print(f\"High confidence accuracy: {(high_conf_mask & correct_mask).sum() / high_conf_mask.sum():.1%}\")\n",
        "\n",
        "    # Low confidence errors\n",
        "    low_conf_threshold = 0.5\n",
        "    low_conf_mask = confidences < low_conf_threshold\n",
        "    low_conf_errors = low_conf_mask & ~correct_mask\n",
        "\n",
        "    print(f\"Low confidence errors (<{low_conf_threshold}): {low_conf_errors.sum()}\")\n",
        "    print(f\"Low confidence accuracy: {(low_conf_mask & correct_mask).sum() / low_conf_mask.sum():.1%}\")\n",
        "\n",
        "    # Performance correlation analysis\n",
        "    print(f\"\\nPERFORMANCE CORRELATION ANALYSIS:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Correlation between class support and performance\n",
        "    support_values = np.array([class_errors[i]['total'] for i in range(num_classes)])\n",
        "    f1_values = np.array([class_errors[i]['f1_score'] for i in range(num_classes)])\n",
        "\n",
        "    support_f1_corr = np.corrcoef(support_values, f1_values)[0, 1]\n",
        "    print(f\"Correlation (Support vs F1-Score): {support_f1_corr:.4f}\")\n",
        "\n",
        "    # Correlation between class support and error rate\n",
        "    error_rates = np.array([class_errors[i]['error_rate'] for i in range(num_classes)])\n",
        "    support_error_corr = np.corrcoef(support_values, error_rates)[0, 1]\n",
        "    print(f\"Correlation (Support vs Error Rate): {support_error_corr:.4f}\")\n",
        "\n",
        "    # ===== CREATE VISUALIZATIONS =====\n",
        "    print(f\"\\nCreating error analysis visualizations...\")\n",
        "\n",
        "    # Create comprehensive subplot grid\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "\n",
        "    # 1. Top Error Classes Bar Chart\n",
        "    top_10_errors = sorted_errors[:10]\n",
        "    class_names = [idx2name.get(cls_id, f\"Class_{cls_id}\")[:20] + \"...\" for cls_id, _ in top_10_errors]\n",
        "    error_rates = [error_info['error_rate'] for _, error_info in top_10_errors]\n",
        "\n",
        "    bars = axes[0, 0].bar(range(len(class_names)), error_rates, color='red', alpha=0.7)\n",
        "    axes[0, 0].set_title('Top 10 Classes with Highest Error Rates', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Classes', fontsize=12)\n",
        "    axes[0, 0].set_ylabel('Error Rate', fontsize=12)\n",
        "    axes[0, 0].set_xticks(range(len(class_names)))\n",
        "    axes[0, 0].set_xticklabels(class_names, rotation=45, ha='right')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, rate in zip(bars, error_rates):\n",
        "        height = bar.get_height()\n",
        "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                        f'{rate:.1%}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # 2. Error Rate vs F1-Score Scatter\n",
        "    all_error_rates = [error_info['error_rate'] for _, error_info in sorted_errors]\n",
        "    all_f1_scores = [error_info['f1_score'] for _, error_info in sorted_errors]\n",
        "\n",
        "    axes[0, 1].scatter(all_error_rates, all_f1_scores, alpha=0.7, color='blue', s=50)\n",
        "    axes[0, 1].set_title('Error Rate vs F1-Score Correlation', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Error Rate', fontsize=12)\n",
        "    axes[0, 1].set_ylabel('F1-Score', fontsize=12)\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add trend line\n",
        "    z = np.polyfit(all_error_rates, all_f1_scores, 1)\n",
        "    p = np.poly1d(z)\n",
        "    correlation = np.corrcoef(all_error_rates, all_f1_scores)[0, 1]\n",
        "    axes[0, 1].plot(all_error_rates, p(all_error_rates), \"r--\", alpha=0.8,\n",
        "                     label=f\"Trend (r={correlation:.3f})\")\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "    # 3. Class Support vs Error Rate\n",
        "    all_supports = [error_info['total'] for _, error_info in sorted_errors]\n",
        "\n",
        "    axes[0, 2].scatter(all_supports, all_error_rates, alpha=0.7, color='green', s=50)\n",
        "    axes[0, 2].set_title('Class Support vs Error Rate', fontsize=14, fontweight='bold')\n",
        "    axes[0, 2].set_xlabel('Number of Samples (Support)', fontsize=12)\n",
        "    axes[0, 2].set_ylabel('Error Rate', fontsize=12)\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add trend line\n",
        "    z = np.polyfit(all_supports, all_error_rates, 1)\n",
        "    p = np.poly1d(z)\n",
        "    correlation = np.corrcoef(all_supports, all_error_rates)[0, 1]\n",
        "    axes[0, 2].plot(all_supports, p(all_supports), \"r--\", alpha=0.8,\n",
        "                     label=f\"Trend (r={correlation:.3f})\")\n",
        "    axes[0, 2].legend()\n",
        "\n",
        "    # 4. Most Confused Class Pairs\n",
        "    top_confused = most_confused[:10]\n",
        "    confusion_counts = [confusion['count'] for confusion in top_confused]\n",
        "    confusion_labels = [f\"{confusion['true_class_name'][:15]}...\\n {confusion['predicted_class_name'][:15]}...\"\n",
        "                       for confusion in top_confused]\n",
        "\n",
        "    bars = axes[1, 0].bar(range(len(confusion_counts)), confusion_counts, color='orange', alpha=0.7)\n",
        "    axes[1, 0].set_title('Top 10 Most Confused Class Pairs', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Class Pairs', fontsize=12)\n",
        "    axes[1, 0].set_ylabel('Number of Confusions', fontsize=12)\n",
        "    axes[1, 0].set_xticks(range(len(confusion_labels)))\n",
        "    axes[1, 0].set_xticklabels(confusion_labels, rotation=45, ha='right')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, count in zip(bars, confusion_counts):\n",
        "        height = bar.get_height()\n",
        "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                        str(count), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # 5. Confidence vs Accuracy Analysis\n",
        "    confidence_bins = np.linspace(0, 1, 11)\n",
        "    bin_accuracies = []\n",
        "    bin_centers = []\n",
        "\n",
        "    for i in range(len(confidence_bins) - 1):\n",
        "        bin_mask = (confidences >= confidence_bins[i]) & (confidences < confidence_bins[i + 1])\n",
        "        if bin_mask.sum() > 0:\n",
        "            bin_accuracy = correct_mask[bin_mask].mean()\n",
        "            bin_accuracies.append(bin_accuracy)\n",
        "            bin_centers.append((confidence_bins[i] + confidence_bins[i + 1]) / 2)\n",
        "\n",
        "    axes[1, 1].plot(bin_centers, bin_accuracies, 'o-', linewidth=2, markersize=8,\n",
        "                     color='purple', alpha=0.8)\n",
        "    axes[1, 1].plot([0, 1], [0, 1], '--', color='red', alpha=0.5, label='Perfect Calibration')\n",
        "    axes[1, 1].set_title('Confidence vs Accuracy Analysis', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Confidence', fontsize=12)\n",
        "    axes[1, 1].set_ylabel('Accuracy', fontsize=12)\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    # 6. Error Distribution by Performance Level\n",
        "    performance_levels = ['High (F1 > 0.8)', 'Medium (0.5 < F1  0.8)', 'Low (F1  0.5)']\n",
        "    high_perf = [error_info for _, error_info in sorted_errors if error_info['f1_score'] > 0.8]\n",
        "    medium_perf = [error_info for _, error_info in sorted_errors if 0.5 < error_info['f1_score'] <= 0.8]\n",
        "    low_perf = [error_info for _, error_info in sorted_errors if error_info['f1_score'] <= 0.5]\n",
        "\n",
        "    performance_counts = [len(high_perf), len(medium_perf), len(low_perf)]\n",
        "    performance_colors = ['green', 'orange', 'red']\n",
        "\n",
        "    bars = axes[1, 2].bar(performance_levels, performance_counts, color=performance_colors, alpha=0.7)\n",
        "    axes[1, 2].set_title('Error Distribution by Performance Level', fontsize=14, fontweight='bold')\n",
        "    axes[1, 2].set_xlabel('Performance Level', fontsize=12)\n",
        "    axes[1, 2].set_ylabel('Number of Classes', fontsize=12)\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, count in zip(bars, performance_counts):\n",
        "        height = bar.get_height()\n",
        "        axes[1, 2].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                        str(count), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save visualization\n",
        "    error_viz_path = os.path.join(DRIVE_VISUALIZATIONS_DIR, 'stage2_error_analysis.png')\n",
        "    plt.savefig(error_viz_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Error analysis visualizations saved to: {error_viz_path}\")\n",
        "\n",
        "    return {\n",
        "        'class_errors': class_errors,\n",
        "        'most_confused_pairs': most_confused,\n",
        "        'high_conf_errors': high_conf_errors.sum(),\n",
        "        'low_conf_errors': low_conf_errors.sum(),\n",
        "        'overall_error_rate': misclassifications / total_samples,\n",
        "        'correlations': {\n",
        "            'support_vs_f1': support_f1_corr,\n",
        "            'support_vs_error': support_error_corr\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Run error analysis with plots\n",
        "error_results = error_analysis_multi_class()"
      ],
      "metadata": {
        "id": "pvMIqRojJ2mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rEgcX8F2I6fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **CONFUSION MATRIX ANALYSIS - MULTI-CLASS**\n",
        "\n",
        "def confusion_matrix_analysis_multi_class():\n",
        "    \"\"\"Comprehensive confusion matrix analysis for multi-class disease classification\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" CONFUSION MATRIX ANALYSIS - MULTI-CLASS\")\n",
        "    print(\" Disease Classification Confusion Analysis\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_test, yhat_test, labels=range(num_classes))\n",
        "\n",
        "    print(f\"CONFUSION MATRIX OVERVIEW:\")\n",
        "    print(f\"  Matrix shape: {cm.shape} ({num_classes} x {num_classes})\")\n",
        "    print(f\"  Total samples: {cm.sum()}\")\n",
        "    print(f\"  Correct predictions (diagonal): {np.trace(cm)}\")\n",
        "    print(f\"  Incorrect predictions (off-diagonal): {cm.sum() - np.trace(cm)}\")\n",
        "\n",
        "    # Basic confusion matrix statistics\n",
        "    print(f\"\\nBASIC STATISTICS:\")\n",
        "    print(f\"  Overall Accuracy: {np.trace(cm) / cm.sum():.4f}\")\n",
        "    print(f\"  Error Rate: {(cm.sum() - np.trace(cm)) / cm.sum():.4f}\")\n",
        "\n",
        "    # Per-class confusion analysis\n",
        "    print(f\"\\nPER-CLASS CONFUSION ANALYSIS:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Class ID':<8} {'Class Name':<35} {'TP':<8} {'FP':<8} {'FN':<8} {'TN':<8} {'Precision':<12} {'Recall':<12}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    class_confusion_analysis = []\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        # True Positives (correctly predicted as class i)\n",
        "        tp = cm[i, i]\n",
        "\n",
        "        # False Positives (incorrectly predicted as class i)\n",
        "        fp = cm[:, i].sum() - tp\n",
        "\n",
        "        # False Negatives (class i incorrectly predicted as other classes)\n",
        "        fn = cm[i, :].sum() - tp\n",
        "\n",
        "        # True Negatives (correctly not predicted as class i)\n",
        "        tn = cm.sum() - tp - fp - fn\n",
        "\n",
        "        # Calculate precision and recall\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "        class_name = idx2name.get(i, f\"Class_{i}\")\n",
        "\n",
        "        print(f\"{i:<8} {class_name[:34]:<35} {tp:<8} {fp:<8} {fn:<8} {tn:<8} {precision:<12.4f} {recall:<12.4f}\")\n",
        "\n",
        "        class_confusion_analysis.append({\n",
        "            'class_id': i,\n",
        "            'class_name': class_name,\n",
        "            'tp': tp,\n",
        "            'fp': fp,\n",
        "            'fn': fn,\n",
        "            'tn': tn,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        })\n",
        "\n",
        "    # Most confused class pairs analysis\n",
        "    print(f\"\\nMOST CONFUSED CLASS PAIRS:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    most_confused = []\n",
        "    for i in range(num_classes):\n",
        "        for j in range(num_classes):\n",
        "            if i != j and cm[i, j] > 0:\n",
        "                most_confused.append({\n",
        "                    'true_class': i,\n",
        "                    'predicted_class': j,\n",
        "                    'count': cm[i, j],\n",
        "                    'true_class_name': idx2name.get(i, f\"Class_{i}\"),\n",
        "                    'predicted_class_name': idx2name.get(j, f\"Class_{j}\"),\n",
        "                    'error_rate': cm[i, j] / cm[i, :].sum() if cm[i, :].sum() > 0 else 0\n",
        "                })\n",
        "\n",
        "    # Sort by confusion count\n",
        "    most_confused.sort(key=lambda x: x['count'], reverse=True)\n",
        "\n",
        "    print(f\"Top 15 Most Confused Class Pairs:\")\n",
        "    for i, confusion in enumerate(most_confused[:15]):\n",
        "        print(f\"  {i+1:2d}. {confusion['true_class_name'][:25]:<25}  {confusion['predicted_class_name'][:25]:<25}\")\n",
        "        print(f\"      Count: {confusion['count']} samples, Error Rate: {confusion['error_rate']:.1%}\")\n",
        "\n",
        "    # Class-wise error analysis\n",
        "    print(f\"\\nCLASS-WISE ERROR ANALYSIS:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Sort classes by error rate\n",
        "    class_confusion_analysis.sort(key=lambda x: (x['fp'] + x['fn']) / (x['tp'] + x['fp'] + x['fn'] + x['tn']), reverse=True)\n",
        "\n",
        "    print(f\"Top 10 Classes with Highest Error Rates:\")\n",
        "    for i, analysis in enumerate(class_confusion_analysis[:10]):\n",
        "        total_samples = analysis['tp'] + analysis['fp'] + analysis['fn'] + analysis['tn']\n",
        "        error_rate = (analysis['fp'] + analysis['fn']) / total_samples if total_samples > 0 else 0\n",
        "        print(f\"  {i+1:2d}. {analysis['class_name'][:30]:<30}\")\n",
        "        print(f\"      Error Rate: {error_rate:.1%}, FP: {analysis['fp']}, FN: {analysis['fn']}\")\n",
        "\n",
        "    # Confusion matrix visualization\n",
        "    print(f\"\\nCREATING CONFUSION MATRIX VISUALIZATIONS...\")\n",
        "\n",
        "    # 1. Full confusion matrix heatmap\n",
        "    plt.figure(figsize=(16, 14))\n",
        "\n",
        "    # Normalize confusion matrix for better visualization\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    cm_normalized = np.nan_to_num(cm_normalized)\n",
        "\n",
        "    # Create heatmap\n",
        "    sns.heatmap(cm_normalized,\n",
        "                annot=True,\n",
        "                fmt='.2f',\n",
        "                cmap='Blues',\n",
        "                xticklabels=[f\"{i}\\n{idx2name.get(i, f'Class_{i}')[:15]}...\" for i in range(num_classes)],\n",
        "                yticklabels=[f\"{i}\\n{idx2name.get(i, f'Class_{i}')[:15]}...\" for i in range(num_classes)],\n",
        "                cbar_kws={'label': 'Normalized Confusion Rate'})\n",
        "\n",
        "    plt.title('Stage 2 - Multi-Class Disease Classification Confusion Matrix\\n(Normalized by True Class)',\n",
        "              fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Predicted Class', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('True Class', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save full confusion matrix\n",
        "    full_cm_path = os.path.join(DRIVE_VISUALIZATIONS_DIR, 'stage2_full_confusion_matrix.png')\n",
        "    plt.savefig(full_cm_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Top confused classes heatmap (focus on most problematic classes)\n",
        "    print(f\"Creating focused confusion matrix for top confused classes...\")\n",
        "\n",
        "    # Get top 20 classes with most errors\n",
        "    top_error_classes = sorted(class_confusion_analysis,\n",
        "                              key=lambda x: (x['fp'] + x['fn']),\n",
        "                              reverse=True)[:20]\n",
        "\n",
        "    top_class_ids = [cls['class_id'] for cls in top_error_classes]\n",
        "\n",
        "    # Extract sub-matrix for top confused classes\n",
        "    cm_top = cm[np.ix_(top_class_ids, top_class_ids)]\n",
        "\n",
        "    plt.figure(figsize=(14, 12))\n",
        "\n",
        "    # Normalize\n",
        "    cm_top_normalized = cm_top.astype('float') / cm_top.sum(axis=1)[:, np.newaxis]\n",
        "    cm_top_normalized = np.nan_to_num(cm_top_normalized)\n",
        "\n",
        "    # Create heatmap\n",
        "    sns.heatmap(cm_top_normalized,\n",
        "                annot=True,\n",
        "                fmt='.2f',\n",
        "                cmap='Reds',\n",
        "                xticklabels=[f\"{i}\\n{idx2name.get(i, f'Class_{i}')[:20]}\" for i in top_class_ids],\n",
        "                yticklabels=[f\"{i}\\n{idx2name.get(i, f'Class_{i}')[:20]}\" for i in top_class_ids],\n",
        "                cbar_kws={'label': 'Normalized Confusion Rate'})\n",
        "\n",
        "    plt.title('Stage 2 - Top Confused Classes Confusion Matrix\\n(Focus on Most Problematic Classes)',\n",
        "              fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Predicted Class', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('True Class', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save focused confusion matrix\n",
        "    focused_cm_path = os.path.join(DRIVE_VISUALIZATIONS_DIR, 'stage2_focused_confusion_matrix.png')\n",
        "    plt.savefig(focused_cm_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Confusion pattern analysis\n",
        "    print(f\"Creating confusion pattern analysis...\")\n",
        "\n",
        "    # Analyze confusion patterns\n",
        "    confusion_patterns = {}\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        class_errors = []\n",
        "        for j in range(num_classes):\n",
        "            if i != j and cm[i, j] > 0:\n",
        "                class_errors.append({\n",
        "                    'predicted_as': j,\n",
        "                    'count': cm[i, j],\n",
        "                    'error_rate': cm[i, j] / cm[i, :].sum() if cm[i, :].sum() > 0 else 0,\n",
        "                    'predicted_class_name': idx2name.get(j, f\"Class_{j}\")\n",
        "                })\n",
        "\n",
        "        if class_errors:\n",
        "            # Sort by error count\n",
        "            class_errors.sort(key=lambda x: x['count'], reverse=True)\n",
        "            confusion_patterns[i] = class_errors\n",
        "\n",
        "    # Save confusion analysis results\n",
        "    confusion_analysis_results = {\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'normalized_confusion_matrix': cm_normalized.tolist(),\n",
        "        'class_confusion_analysis': class_confusion_analysis,\n",
        "        'most_confused_pairs': most_confused[:20],  # Top 20\n",
        "        'confusion_patterns': confusion_patterns,\n",
        "        'matrix_statistics': {\n",
        "            'total_samples': int(cm.sum()),\n",
        "            'correct_predictions': int(np.trace(cm)),\n",
        "            'incorrect_predictions': int(cm.sum() - np.trace(cm)),\n",
        "            'overall_accuracy': float(np.trace(cm) / cm.sum()),\n",
        "            'overall_error_rate': float((cm.sum() - np.trace(cm)) / cm.sum())\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save to file\n",
        "    confusion_analysis_path = os.path.join(DRIVE_ANALYSIS_DIR, 'stage2_confusion_matrix_analysis.json')\n",
        "    with open(confusion_analysis_path, 'w') as f:\n",
        "        json.dump(confusion_analysis_results, f, indent=2, default=str)\n",
        "\n",
        "    print(f\"\\nConfusion matrix analysis completed!\")\n",
        "    print(f\"Results saved to: {confusion_analysis_path}\")\n",
        "    print(f\"Visualizations saved to:\")\n",
        "    print(f\"  - Full confusion matrix: {full_cm_path}\")\n",
        "    print(f\"  - Focused confusion matrix: {focused_cm_path}\")\n",
        "\n",
        "    return confusion_analysis_results\n",
        "\n",
        "# Run confusion matrix analysis\n",
        "confusion_matrix_results = confusion_matrix_analysis_multi_class()"
      ],
      "metadata": {
        "id": "AtRT3GG6Hva7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "krdSivX5KhF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL TESTING WITH GRAD-CAM VISUALIZATION**"
      ],
      "metadata": {
        "id": "jtJg9wTGP3AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grad-CAM core for Stage 2 (MobileNetV2 head rebuilt in one graph)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "\n",
        "# Cache to avoid rebuilding the feature+prediction model each call\n",
        "_GRADCAM_FEATPRED_CACHE = {}\n",
        "\n",
        "def _build_featpred_model_from_existing(model):\n",
        "    # Rebuild: inputs -> base(out_relu) -> replay the head -> outputs\n",
        "    inputs = model.input                                   # original input tensor\n",
        "    base = model.get_layer('mobilenetv2_1.00_224')         # nested MobileNetV2 (seen in your summary)\n",
        "    feat = base(inputs)                                    # last conv feature (7x7x1280)\n",
        "\n",
        "    x = feat                                               # replay layers 2..end (head)\n",
        "    for layer in model.layers[2:]:\n",
        "        x = layer(x)\n",
        "\n",
        "    # outputs: (features, logits_or_probs)\n",
        "    featpred = tf.keras.Model(inputs=inputs, outputs=[feat, x])\n",
        "    return featpred\n",
        "\n",
        "def create_stage2_gradcam(model, img_path, target_class=None, target_size=(224, 224), idx2name=None):\n",
        "    # Get or build the single-graph (features, predictions) model\n",
        "    key = id(model)\n",
        "    featpred = _GRADCAM_FEATPRED_CACHE.get(key)\n",
        "    if featpred is None:\n",
        "        featpred = _build_featpred_model_from_existing(model)\n",
        "        _GRADCAM_FEATPRED_CACHE[key] = featpred\n",
        "\n",
        "    # Load + preprocess image\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=target_size)\n",
        "    img_arr = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    orig = img_arr.astype(np.uint8)\n",
        "    img_proc = tf.keras.applications.mobilenet_v2.preprocess_input(img_arr.copy())\n",
        "    inp = tf.convert_to_tensor(np.expand_dims(img_proc, 0), dtype=tf.float32)\n",
        "\n",
        "    # Forward for prediction (use softmax for safety)\n",
        "    _, preds = featpred(inp, training=False)\n",
        "    probs = tf.nn.softmax(preds, axis=-1).numpy()[0]\n",
        "    pred_cls = int(np.argmax(probs))\n",
        "    pred_conf = float(probs[pred_cls])\n",
        "    target = int(target_class) if target_class is not None else pred_cls\n",
        "    pred_name = idx2name.get(target, f\"Class_{target}\") if isinstance(idx2name, dict) else f\"Class_{target}\"\n",
        "\n",
        "    # Gradients of target score w.r.t. conv features (same graph tensors)\n",
        "    with tf.GradientTape() as tape:\n",
        "        feat_map, preds2 = featpred(inp, training=False)\n",
        "        score = tf.nn.softmax(preds2, axis=-1)[:, target]\n",
        "    grads = tape.gradient(score, feat_map)\n",
        "    if grads is None:\n",
        "        return orig, None, None, pred_cls, pred_name, pred_conf\n",
        "\n",
        "    # Grad-CAM heatmap (global-average pool grads -> channel weights)\n",
        "    weights = tf.reduce_mean(grads, axis=(0, 1, 2))           # (C,)\n",
        "    cam = tf.reduce_sum(weights * feat_map[0], axis=-1)       # (Hf, Wf)\n",
        "    cam = tf.nn.relu(cam)\n",
        "    cam = cam / (tf.reduce_max(cam) + 1e-8)\n",
        "    heat = cam.numpy()\n",
        "\n",
        "    # Resize, colorize, overlay\n",
        "    heat_r = cv2.resize(heat, (target_size[1], target_size[0]), interpolation=cv2.INTER_CUBIC)\n",
        "    focus = np.where(heat_r > 0.30, heat_r, 0.0)\n",
        "    heat_rgb = cv2.applyColorMap(np.uint8(255 * focus), cv2.COLORMAP_JET)\n",
        "    heat_rgb = cv2.cvtColor(heat_rgb, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    overlay = orig.copy().astype(np.float32)\n",
        "    mask = focus > 0.40\n",
        "    if np.any(mask):\n",
        "        overlay[mask] = 0.6 * heat_rgb[mask] + 0.4 * orig[mask]\n",
        "    overlay = np.clip(overlay, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return orig, heat_rgb, overlay, pred_cls, pred_name, pred_conf\n",
        "\n",
        "def build_idx2name_from_df(df):\n",
        "    # Build {class_id: class_name} from a dataframe\n",
        "    if not {'encoded_class_id', 'class_name'}.issubset(df.columns):\n",
        "        return {}\n",
        "    return (df[['encoded_class_id', 'class_name']]\n",
        "            .drop_duplicates()\n",
        "            .sort_values('encoded_class_id')\n",
        "            .set_index('encoded_class_id')['class_name']\n",
        "            .to_dict())"
      ],
      "metadata": {
        "id": "F6QdioxEKhDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Grad-CAM panel visualizer (picks different samples each run unless seed is set)\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Output folder\n",
        "if 'DRIVE_GRADCAM_DIR' not in globals():\n",
        "    DRIVE_GRADCAM_DIR = '/content/drive/MyDrive/Stage2_Enhanced_GradCAM'\n",
        "os.makedirs(DRIVE_GRADCAM_DIR, exist_ok=True)\n",
        "\n",
        "def visualize_and_save_stage2_gradcam(\n",
        "    model,\n",
        "    test_df,\n",
        "    idx2name=None,\n",
        "    num_samples=6,\n",
        "    target_mode='true',    # 'true' -> explain true class, 'pred' -> explain predicted class\n",
        "    seed=None,             # None => different each run; set int for reproducibility\n",
        "    save_individual_overlays=True\n",
        "):\n",
        "    # Build idx2name if not provided\n",
        "    if idx2name is None:\n",
        "        idx2name = build_idx2name_from_df(test_df)\n",
        "\n",
        "    rng = np.random.default_rng(None if seed is None else int(seed))\n",
        "\n",
        "    # Pick distinct classes at random\n",
        "    unique_classes = test_df['encoded_class_id'].unique()\n",
        "    chosen = rng.choice(unique_classes, size=min(num_samples, len(unique_classes)), replace=False)\n",
        "\n",
        "    # Pick one random image per chosen class\n",
        "    samples = []\n",
        "    for cls in chosen:\n",
        "        sub = test_df[test_df['encoded_class_id'] == cls]\n",
        "        if len(sub) == 0:\n",
        "            continue\n",
        "        rand_idx = int(rng.integers(0, len(sub)))\n",
        "        samples.append(sub.iloc[rand_idx])\n",
        "\n",
        "    rows = len(samples)\n",
        "    fig, axes = plt.subplots(rows, 3, figsize=(18, 4 * rows))\n",
        "    if rows == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    saved_overlay_paths = []\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        img_path = sample['image_path_absolute']\n",
        "        true_id = int(sample['encoded_class_id'])\n",
        "        true_name = idx2name.get(true_id, f\"Class_{true_id}\")\n",
        "\n",
        "        target = true_id if target_mode == 'true' else None\n",
        "\n",
        "        original, heatmap, overlay, pred_id, pred_name, pred_conf = create_stage2_gradcam(\n",
        "            model=model,\n",
        "            img_path=img_path,\n",
        "            target_class=target,\n",
        "            target_size=(224, 224),\n",
        "            idx2name=idx2name\n",
        "        )\n",
        "        if target_mode == 'pred':\n",
        "            pred_name = idx2name.get(pred_id, f\"Class_{pred_id}\")\n",
        "\n",
        "        is_correct = (pred_id == true_id)\n",
        "        border_color = 'green' if is_correct else 'red'\n",
        "\n",
        "        axes[i, 0].imshow(original.astype(np.uint8))\n",
        "        axes[i, 0].set_title(f\"Original\\nTrue: {true_name[:28]}\", fontsize=11, fontweight='bold')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        if heatmap is not None:\n",
        "            axes[i, 1].imshow(heatmap)\n",
        "            axes[i, 1].set_title(\"Disease Attention Map\\nRed = High Attention\", fontsize=11, fontweight='bold')\n",
        "        else:\n",
        "            axes[i, 1].text(0.5, 0.5, 'Grad-CAM Unavailable', ha='center', va='center', fontsize=12, color='red', fontweight='bold')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        axes[i, 2].imshow((overlay if overlay is not None else original).astype(np.uint8))\n",
        "        axes[i, 2].set_title(f\"Pred: {pred_name[:28]}\\nConf: {pred_conf:.1%}\", fontsize=11, fontweight='bold')\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "        for col in range(3):\n",
        "            for spine in axes[i, col].spines.values():\n",
        "                spine.set_color(border_color)\n",
        "                spine.set_linewidth(3)\n",
        "                spine.set_visible(True)\n",
        "\n",
        "        if save_individual_overlays and overlay is not None:\n",
        "            base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "            tag = 'true' if target_mode == 'true' else 'pred'\n",
        "            overlay_name = f\"{base}_overlay_{tag}_cls{true_id}_pred{pred_id}.png\"\n",
        "            overlay_path = os.path.join(DRIVE_GRADCAM_DIR, overlay_name)\n",
        "            plt.imsave(overlay_path, overlay.astype(np.uint8))\n",
        "            saved_overlay_paths.append(overlay_path)\n",
        "\n",
        "    mode_label = 'TrueClass' if target_mode == 'true' else 'PredClass'\n",
        "    plt.suptitle(f\"Stage 2 Grad-CAM ({mode_label})  {rows} Classes\", fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.90)\n",
        "\n",
        "    ts = int(time.time())\n",
        "    panel_name = f\"stage2_gradcam_panel_{mode_label}_{rows}rows_{ts}.png\"\n",
        "    panel_path = os.path.join(DRIVE_GRADCAM_DIR, panel_name)\n",
        "    plt.savefig(panel_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Saved panel: {panel_path}\")\n",
        "    if saved_overlay_paths:\n",
        "        print(f\"Saved {len(saved_overlay_paths)} overlay(s) to: {DRIVE_GRADCAM_DIR}\")\n",
        "    return panel_path, saved_overlay_paths"
      ],
      "metadata": {
        "id": "ofdgMd11KhAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure you have: model (loaded Stage 2 model) and test_df (Stage 2 test split)\n",
        "\n",
        "# Build class-name map once (optional; the visualizer will build it if None)\n",
        "idx2name = build_idx2name_from_df(test_df)\n",
        "\n",
        "# Random samples every run (seed=None)\n",
        "panel_path, overlay_paths = visualize_and_save_stage2_gradcam(\n",
        "    model=model,\n",
        "    test_df=test_df,\n",
        "    idx2name=idx2name,\n",
        "    num_samples=6,\n",
        "    target_mode='true',          # use 'pred' to explain predicted class\n",
        "    seed=None,                   # None => different random selection each run\n",
        "    save_individual_overlays=True\n",
        ")\n",
        "print(panel_path)"
      ],
      "metadata": {
        "id": "921UXEecKg9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I6lYiW_e-uQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ztCFjcwznY4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTbs58a-PJGL"
      },
      "outputs": [],
      "source": [
        "# **COMPREHENSIVE RESULTS SUMMARY AND VISUALIZATION**\n",
        "\n",
        "def create_comprehensive_summary():\n",
        "    \"\"\"Create comprehensive summary of all evaluation results\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" COMPREHENSIVE EVALUATION SUMMARY - STAGE 2\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Compile all results\n",
        "    summary = {\n",
        "        'model_info': {\n",
        "            'model_path': BEST_MODEL_PATH,\n",
        "            'num_classes': num_classes,\n",
        "            'total_test_samples': len(y_test)\n",
        "        },\n",
        "        'performance_metrics': {\n",
        "            'accuracy': acc_test,\n",
        "            'top3_accuracy': acc_top3,\n",
        "            'top5_accuracy': acc_top5,\n",
        "            'f1_macro': precision_recall_fscore_support(\n",
        "                y_test, yhat_test, average='macro', zero_division=0\n",
        "            )[2]\n",
        "        },\n",
        "        'bootstrap_results': bootstrap_results,\n",
        "        'cross_validation': {\n",
        "            'mean_accuracy': np.mean([score['accuracy'] for score in cv_results]),\n",
        "            'std_accuracy': np.std([score['accuracy'] for score in cv_results]),\n",
        "            'mean_f1': np.mean([score['f1_score'] for score in cv_results]),\n",
        "            'std_f1': np.std([score['f1_score'] for score in cv_results])\n",
        "        },\n",
        "        'error_analysis': error_results,\n",
        "        'statistical_analysis': stats_results\n",
        "    }\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"MODEL PERFORMANCE SUMMARY:\")\n",
        "    print(f\"  Test Accuracy: {summary['performance_metrics']['accuracy']:.4f}\")\n",
        "    print(f\"  Top-3 Accuracy: {summary['performance_metrics']['top3_accuracy']:.4f}\")\n",
        "    print(f\"  Top-5 Accuracy: {summary['performance_metrics']['top5_accuracy']:.4f}\")\n",
        "    print(f\"  F1-Score (Macro): {summary['performance_metrics']['f1_macro']:.4f}\")\n",
        "\n",
        "    print(f\"\\nBOOTSTRAP CONFIDENCE INTERVALS (95%):\")\n",
        "    print(f\"  Accuracy: {bootstrap_results['accuracy']['mean']:.4f} [{bootstrap_results['accuracy']['ci'][0]:.4f}, {bootstrap_results['accuracy']['ci'][1]:.4f}]\")\n",
        "    print(f\"  F1-Score: {bootstrap_results['f1_score']['mean']:.4f} [{bootstrap_results['f1_score']['ci'][0]:.4f}, {bootstrap_results['f1_score']['ci'][1]:.4f}]\")\n",
        "\n",
        "    print(f\"\\nCROSS-VALIDATION RESULTS:\")\n",
        "    print(f\"  Accuracy: {summary['cross_validation']['mean_accuracy']:.4f}  {summary['cross_validation']['std_accuracy']:.4f}\")\n",
        "    print(f\"  F1-Score: {summary['cross_validation']['mean_f1']:.4f}  {summary['cross_validation']['std_f1']:.4f}\")\n",
        "\n",
        "    print(f\"\\nERROR ANALYSIS:\")\n",
        "    print(f\"  Overall Error Rate: {error_results['overall_error_rate']:.1%}\")\n",
        "    print(f\"  High Confidence Errors: {error_results['high_conf_errors']}\")\n",
        "    print(f\"  Low Confidence Errors: {error_results['low_conf_errors']}\")\n",
        "\n",
        "    # Save comprehensive summary\n",
        "    summary_path = os.path.join(DRIVE_ANALYSIS_DIR, 'stage2_comprehensive_evaluation_summary.json')\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary, f, indent=2, default=str)\n",
        "\n",
        "    print(f\"\\nComprehensive summary saved to: {summary_path}\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Create and display comprehensive summary\n",
        "final_summary = create_comprehensive_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" STAGE 2 COMPREHENSIVE EVALUATION COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"All evaluation metrics, visualizations, and analyses have been completed.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}