{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **AI DRIVEN PRECISION AGRICULTURE FOR EARLY DISEASE DETECTION AND SUSTAINABLE CROP PROTECTION**"
      ],
      "metadata": {
        "id": "0OuHl_0vRwhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STAGE-1-BINARY CLASSIFIER FOR 89 CLASSES (HEALTHY VS DISEASED)**"
      ],
      "metadata": {
        "id": "R8FHfniVSfZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATASET: PlantWild (Benchmarking In-the-Wild Multimodal Plant Disease Recognition and A Versatile Baseline)**\n",
        "# **LINK TO PAPER: https://tqwei05.github.io/PlantWild**\n",
        "# **LINK TO DATASET: https://huggingface.co/datasets/uqtwei2/PlantWild/tree/main**"
      ],
      "metadata": {
        "id": "6hIgknB6UxVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**"
      ],
      "metadata": {
        "id": "EXZtN9I_iOCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing standard Python libraries for file and data operations\n",
        "import matplotlib.patheffects\n",
        "import os  # For operating system interface and file path operations\n",
        "import io  # For input/output operations and file handling\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import numpy as np  # For numerical computations and array operations\n",
        "import matplotlib.pyplot as plt  # For creating plots and visualizations\n",
        "import matplotlib.cm as cm  # For color mapping in visualizations\n",
        "import cv2  # For computer vision and image processing operations\n",
        "from PIL import Image  # For image manipulation and processing\n",
        "\n",
        "# Importing scikit-learn utilities for machine learning preprocessing and evaluation\n",
        "from sklearn.utils import class_weight  # For computing balanced class weights\n",
        "from sklearn.preprocessing import LabelEncoder  # For encoding categorical labels\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay  # For model evaluation metrics\n",
        "\n",
        "# Importing TensorFlow and Keras components for deep learning\n",
        "import tensorflow as tf  # Main deep learning framework\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For image data augmentation and preprocessing\n",
        "from tensorflow.keras.models import Model, load_model  # For creating and loading neural network models\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization  # For building neural network layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # For training optimization and monitoring\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop  # For different optimization algorithms\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall  # For model performance evaluation metrics\n",
        "from tensorflow.keras.applications import MobileNetV2  # Pre-trained convolutional neural network architecture\n",
        "\n",
        "# Importing additional utilities for file handling and data transfer\n",
        "import base64  # For encoding/decoding binary data\n",
        "import zipfile  # For handling compressed zip files\n",
        "import gdown  # For downloading files from Google Drive\n",
        "\n",
        "# Suppress warnings for cleaner output during execution\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Hide warning messages to reduce output clutter\n",
        "\n",
        "print(\" All imports successful!\")  # Confirm all libraries imported successfully"
      ],
      "metadata": {
        "id": "tvajMstWibP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONFIGURATION AND SET-UP**"
      ],
      "metadata": {
        "id": "pSnaudAyiqOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "IMG_HEIGHT = 224  # Set image height for model input\n",
        "IMG_WIDTH = 224  # Set image width for model input\n",
        "BINARY_OUTPUT_NEURONS = 1  # Number of output neurons for binary classification\n",
        "\n",
        "# Improved training parameters\n",
        "EPOCHS_PHASE1 = 15  # Number of epochs for initial training phase\n",
        "EPOCHS_PHASE2 = 10  # Number of epochs for fine-tuning phase\n",
        "BATCH_SIZE = 32  # Number of samples per training batch\n",
        "\n",
        "# Create model directory\n",
        "MODEL_DIR = '/content/models'  # Define path for saving trained models\n",
        "if not os.path.exists(MODEL_DIR):  # Check if directory exists\n",
        "    os.makedirs(MODEL_DIR)  # Create directory if it doesn't exist\n",
        "\n",
        "# Improved ensemble configurations with better learning rates\n",
        "ENSEMBLE_CONFIGS = [\n",
        "    {\"optimizer\": Adam, \"learning_rate\": 5e-4, \"name\": \"model_adam\"},  # Adam optimizer configuration\n",
        "    {\"optimizer\": SGD, \"learning_rate\": 5e-3, \"name\": \"model_sgd\"},    # SGD optimizer configuration\n",
        "    {\"optimizer\": RMSprop, \"learning_rate\": 5e-4, \"name\": \"model_rmsprop\"},  # RMSprop optimizer configuration\n",
        "]\n",
        "\n",
        "print(\" Configuration set up successfully!\")  # Confirm configuration completion\n",
        "print(f\"Image size: {IMG_HEIGHT}x{IMG_WIDTH}\")  # Display image dimensions\n",
        "print(f\"Batch size: {BATCH_SIZE}\")  # Display batch size\n",
        "print(f\"Phase 1 epochs: {EPOCHS_PHASE1}\")  # Display phase 1 training epochs\n",
        "print(f\"Phase 2 epochs: {EPOCHS_PHASE2}\")  # Display phase 2 training epochs"
      ],
      "metadata": {
        "id": "UCxp1Irgc3T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GOOGLE DRIVE MOUNT AND DATASET DOWNLOAD**"
      ],
      "metadata": {
        "id": "uFCRIv06jQwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive  # Import Google Drive mounting functionality\n",
        "drive.mount('/content/drive')  # Mount Google Drive to access files\n",
        "print(\" Google Drive mounted successfully!\")  # Confirm successful mounting"
      ],
      "metadata": {
        "id": "aIOGh55hdTqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create working directory and download dataset\n",
        "import os  # Import operating system interface\n",
        "import zipfile  # Import zip file handling functionality\n",
        "import gdown  # Import Google Drive download utility\n",
        "\n",
        "# Create working directory\n",
        "os.makedirs(\"/content/plantwild\", exist_ok=True)  # Create directory for dataset storage\n",
        "\n",
        "# Google Drive File ID from your shared PlantWild link\n",
        "file_id = \"1TVvXiJIWvpOYUba78gm6ALuy52Ks6IwW\"  # Unique identifier for dataset file\n",
        "zip_path = \"/content/plantwild/plantwild.zip\"  # Local path for downloaded zip file\n",
        "\n",
        "# Download the file using gdown\n",
        "print(\"Downloading PlantWild dataset...\")  # Inform user of download start\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", zip_path, quiet=False)  # Download dataset from Google Drive\n",
        "\n",
        "# Unzip the dataset\n",
        "print(\" Extracting dataset...\")  # Inform user of extraction start\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:  # Open zip file for reading\n",
        "    zip_ref.extractall(\"/content/plantwild\")  # Extract all contents to plantwild directory\n",
        "\n",
        "print(\"PlantWild dataset downloaded and extracted to /content/plantwild\")  # Confirm successful extraction\n",
        "\n",
        "# Verify dataset structure\n",
        "DATASET_ROOT = \"/content/plantwild/plantwild\"  # Define path to extracted dataset\n",
        "if os.path.exists(DATASET_ROOT):  # Check if dataset directory exists\n",
        "    print(f\" Dataset found at: {DATASET_ROOT}\")  # Confirm dataset location\n",
        "    print(f\"Contents: {os.listdir(DATASET_ROOT)}\")  # Display dataset contents\n",
        "else:\n",
        "    print(\" Dataset not found!\")  # Error message if dataset not found"
      ],
      "metadata": {
        "id": "jbk7TJqIdsXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA LOADING FUNCTIONS**"
      ],
      "metadata": {
        "id": "2QNzR9Mujd7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_class_mapping(classes_file):\n",
        "    \"\"\"Load class ID to name mapping\"\"\"\n",
        "    mapping = {}  # Initialize empty dictionary for class mapping\n",
        "    with open(classes_file, 'r') as f:  # Open classes file for reading\n",
        "        for line in f:  # Iterate through each line in file\n",
        "            parts = line.strip().split(\" \", 1)  # Split line by first space\n",
        "            if len(parts) == 2:  # Check if line has both ID and name\n",
        "                mapping[int(parts[0])] = parts[1]  # Store ID as key, name as value\n",
        "    return mapping  # Return completed mapping dictionary\n",
        "\n",
        "def is_healthy(class_name):\n",
        "    \"\"\"Improved healthy/diseased classification logic\"\"\"\n",
        "    disease_keywords = [  # List of keywords indicating plant diseases\n",
        "        'rot', 'blight', 'rust', 'spot', 'virus', 'mildew', 'curl',\n",
        "        'scorch', 'canker', 'pocket', 'smut', 'greening', 'roll', 'anthracnose',\n",
        "        'mosaic', 'yellow', 'brown', 'black', 'gray', 'white'  # Added more disease indicators\n",
        "    ]\n",
        "\n",
        "    # More robust healthy classification\n",
        "    is_leaf = class_name.endswith('leaf')  # Check if class name ends with 'leaf'\n",
        "    has_disease = any(disease in class_name.lower() for disease in disease_keywords)  # Check for disease keywords\n",
        "\n",
        "    return is_leaf and not has_disease  # Return True if leaf without disease\n",
        "\n",
        "def load_plantwild_dataframe(dataset_root, binary_only=True, stage2_only=False):\n",
        "    \"\"\"Load and prepare PlantWild dataset with improved error handling\"\"\"\n",
        "    classes_file = os.path.join(dataset_root, \"classes.txt\")  # Path to class definitions file\n",
        "    trainval_file = os.path.join(dataset_root, \"trainval.txt\")  # Path to train/validation split file\n",
        "    image_dir = os.path.join(dataset_root, \"images\")  # Path to image directory\n",
        "\n",
        "    # Verify files exist\n",
        "    if not all(os.path.exists(f) for f in [classes_file, trainval_file, image_dir]):  # Check all required files exist\n",
        "        raise FileNotFoundError(\"Required dataset files not found!\")  # Raise error if files missing\n",
        "\n",
        "    class_map = load_class_mapping(classes_file)  # Load class ID to name mapping\n",
        "    data = []  # Initialize empty list for dataset\n",
        "    valid_images = 0  # Counter for valid images\n",
        "    invalid_images = 0  # Counter for invalid images\n",
        "\n",
        "    with open(trainval_file, \"r\") as f:  # Open train/validation file\n",
        "        for line_num, line in enumerate(f, 1):  # Iterate through each line with line number\n",
        "            parts = line.strip().split('=')  # Split line by equals sign\n",
        "            if len(parts) != 3:  # Skip lines that don't have 3 parts\n",
        "                continue\n",
        "\n",
        "            image_rel_path, class_id, mode = parts  # Extract image path, class ID, and split mode\n",
        "            class_id = int(class_id)  # Convert class ID to integer\n",
        "            mode = int(mode)  # Convert mode to integer\n",
        "\n",
        "            class_name = class_map.get(class_id, \"Unknown\")  # Get class name from mapping\n",
        "            binary_label = \"healthy\" if is_healthy(class_name) else \"diseased\"  # Determine binary label\n",
        "            image_path = os.path.join(image_dir, image_rel_path)  # Create full image path\n",
        "\n",
        "            # Verify image exists\n",
        "            if os.path.exists(image_path):  # Check if image file exists\n",
        "                data.append({  # Add image data to list\n",
        "                    \"image_path\": image_path,\n",
        "                    \"class_id\": class_id,\n",
        "                    \"class_name\": class_name,\n",
        "                    \"binary_label\": binary_label,\n",
        "                    \"split\": {0: \"test\", 1: \"train\", 2: \"val\"}.get(mode, \"unknown\")  # Map mode to split name\n",
        "                })\n",
        "                valid_images += 1  # Increment valid image counter\n",
        "            else:\n",
        "                invalid_images += 1  # Increment invalid image counter\n",
        "\n",
        "    df = pd.DataFrame(data)  # Convert data list to pandas DataFrame\n",
        "\n",
        "    if binary_only:  # Filter for binary classification only\n",
        "        df = df[df[\"binary_label\"].isin([\"healthy\", \"diseased\"])]\n",
        "    if stage2_only:  # Filter for diseased images only (stage 2)\n",
        "        df = df[df[\"binary_label\"] == \"diseased\"]\n",
        "\n",
        "    df[\"binary_label_verbose\"] = df[\"class_name\"].str.title() + \": \" + df[\"binary_label\"].str.title()  # Create verbose label\n",
        "\n",
        "    print(f\"Dataset loaded: {len(df)} valid images, {invalid_images} invalid images\")  # Print loading statistics\n",
        "    return df  # Return prepared DataFrame"
      ],
      "metadata": {
        "id": "IReETHdieKpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMAGE GENERATOR FUNCTIONS**"
      ],
      "metadata": {
        "id": "iixz79tXkRk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_generators(df, image_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE, aug_level='strong', label_column='binary_label'):\n",
        "    \"\"\"Create image generators with improved augmentation and error handling\"\"\"\n",
        "    df_train = df[df['split'] == 'train']  # Filter training data\n",
        "    df_val = df[df['split'] == 'val']  # Filter validation data\n",
        "    df_test = df[df['split'] == 'test']  # Filter test data\n",
        "\n",
        "    print(f\"Training samples: {len(df_train)}\")  # Display number of training samples\n",
        "    print(f\"Validation samples: {len(df_val)}\")  # Display number of validation samples\n",
        "    print(f\"Test samples: {len(df_test)}\")  # Display number of test samples\n",
        "\n",
        "    # Improved augmentation strategy\n",
        "    if aug_level == 'strong':  # Check if strong augmentation is requested\n",
        "        train_datagen = ImageDataGenerator(  # Create data generator with augmentation\n",
        "            preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,  # MobileNetV2 preprocessing\n",
        "            rotation_range=20,  # Random rotation up to 20 degrees\n",
        "            width_shift_range=0.15,  # Random horizontal shift up to 15%\n",
        "            height_shift_range=0.15,  # Random vertical shift up to 15%\n",
        "            shear_range=0.1,  # Random shear transformation up to 10%\n",
        "            zoom_range=[0.8, 1.2],  # Random zoom between 80% and 120%\n",
        "            horizontal_flip=True,  # Enable horizontal flipping\n",
        "            vertical_flip=False,  # Disable vertical flipping\n",
        "            brightness_range=[0.8, 1.2],  # Random brightness adjustment\n",
        "            channel_shift_range=30,  # Random color channel shifts\n",
        "            fill_mode='nearest'  # Fill mode for transformed pixels\n",
        "        )\n",
        "    else:  # Minimal augmentation\n",
        "        train_datagen = ImageDataGenerator(  # Create data generator without augmentation\n",
        "            preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input  # Only preprocessing\n",
        "        )\n",
        "\n",
        "    val_test_datagen = ImageDataGenerator(  # Create data generator for validation/test\n",
        "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input  # Only preprocessing, no augmentation\n",
        "    )\n",
        "\n",
        "    # Create generators with error handling\n",
        "    try:\n",
        "        train_generator = train_datagen.flow_from_dataframe(  # Create training data generator\n",
        "            df_train, x_col='image_path', y_col=label_column,  # Use image paths and labels\n",
        "            target_size=image_size, batch_size=batch_size,  # Set image size and batch size\n",
        "            class_mode='binary', shuffle=True  # Binary classification with shuffling\n",
        "        )\n",
        "\n",
        "        val_generator = val_test_datagen.flow_from_dataframe(  # Create validation data generator\n",
        "            df_val, x_col='image_path', y_col=label_column,  # Use image paths and labels\n",
        "            target_size=image_size, batch_size=batch_size,  # Set image size and batch size\n",
        "            class_mode='binary', shuffle=True  # Binary classification with shuffling\n",
        "        )\n",
        "\n",
        "        test_generator = val_test_datagen.flow_from_dataframe(  # Create test data generator\n",
        "            df_test, x_col='image_path', y_col=label_column,  # Use image paths and labels\n",
        "            target_size=image_size, batch_size=batch_size,  # Set image size and batch size\n",
        "            class_mode='binary', shuffle=False  # Binary classification without shuffling\n",
        "        )\n",
        "\n",
        "        print(\"Image generators created successfully!\")  # Confirm successful creation\n",
        "        return train_generator, val_generator, test_generator  # Return all generators\n",
        "\n",
        "    except Exception as e:  # Handle any errors during generator creation\n",
        "        print(f\"Error creating generators: {e}\")  # Print error message\n",
        "        raise  # Re-raise the exception\n",
        "\n",
        "def compute_class_weights(df, label_column=\"binary_label\"):\n",
        "    \"\"\"Compute balanced class weights\"\"\"\n",
        "    le = LabelEncoder()  # Create label encoder instance\n",
        "    y = le.fit_transform(df[df['split'] == 'train'][label_column])  # Encode training labels\n",
        "    class_weights = class_weight.compute_class_weight(  # Compute balanced class weights\n",
        "        class_weight='balanced',  # Use balanced weighting strategy\n",
        "        classes=np.unique(y),  # Get unique class labels\n",
        "        y=y  # Use encoded labels\n",
        "    )\n",
        "    weights_dict = dict(enumerate(class_weights))  # Convert to dictionary with class indices\n",
        "    print(f\"Class weights computed: {weights_dict}\")  # Display computed weights\n",
        "    return weights_dict  # Return weights dictionary"
      ],
      "metadata": {
        "id": "Srn4OjkxePNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ENHANCED MODEL ARCHITECTURE FUNCTION**"
      ],
      "metadata": {
        "id": "xd6CxR1JkjYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_hybrid_mobilenet_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=1, dropout_rate=0.5):\n",
        "    \"\"\"Create an improved MobileNetV2-based model with better architecture\"\"\"\n",
        "\n",
        "    # Load pre-trained MobileNetV2 with better initialization\n",
        "    base_model = MobileNetV2(  # Create MobileNetV2 base model\n",
        "        weights='imagenet',  # Use pre-trained ImageNet weights\n",
        "        include_top=False,  # Exclude classification head\n",
        "        input_shape=input_shape,  # Set input image dimensions\n",
        "        alpha=1.0  # Use full width for better performance\n",
        "    )\n",
        "\n",
        "    # Freeze base model initially\n",
        "    base_model.trainable = False  # Prevent base model weights from updating during initial training\n",
        "\n",
        "    # Create model with improved head\n",
        "    inputs = Input(shape=input_shape)  # Define input layer\n",
        "    x = base_model(inputs, training=False)  # Pass input through base model\n",
        "\n",
        "    # Enhanced classification head\n",
        "    x = GlobalAveragePooling2D()(x)  # Global average pooling to reduce spatial dimensions\n",
        "    x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # First dense layer with L2 regularization\n",
        "    x = BatchNormalization()(x)  # Batch normalization for training stability\n",
        "    x = Dropout(dropout_rate)(x)  # Dropout layer to prevent overfitting\n",
        "\n",
        "    x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # Second dense layer with L2 regularization\n",
        "    x = BatchNormalization()(x)  # Batch normalization for training stability\n",
        "    x = Dropout(dropout_rate * 0.8)(x)  # Reduced dropout rate for second layer\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Dense(num_classes, activation='sigmoid')(x)  # Final output layer for binary classification\n",
        "\n",
        "    model = Model(inputs, outputs)  # Create complete model from inputs to outputs\n",
        "\n",
        "    print(\"Model architecture created successfully!\")  # Confirm model creation\n",
        "    print(f\"Base model parameters: {base_model.count_params():,}\")  # Display base model parameter count\n",
        "    print(f\"Total model parameters: {model.count_params():,}\")  # Display total model parameter count\n",
        "\n",
        "    return model, base_model  # Return both complete model and base model\n",
        "\n",
        "def compile_model(model, optimizer, learning_rate, loss='binary_crossentropy'):\n",
        "    \"\"\"Compile model with improved settings\"\"\"\n",
        "    model.compile(  # Compile the model with specified settings\n",
        "        optimizer=optimizer(learning_rate=learning_rate),  # Set optimizer with learning rate\n",
        "        loss=loss,  # Set loss function\n",
        "        metrics=[  # Define evaluation metrics\n",
        "            'accuracy',  # Overall accuracy\n",
        "            Precision(name='precision'),  # Precision metric\n",
        "            Recall(name='recall'),  # Recall metric\n",
        "            AUC(name='auc')  # Area under curve metric\n",
        "        ]\n",
        "    )\n",
        "    print(f\" Model compiled with {optimizer.__name__} (lr={learning_rate})\")  # Confirm compilation with details"
      ],
      "metadata": {
        "id": "u_T7XhudghzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ENHANCED TRAINING FUNCTION**"
      ],
      "metadata": {
        "id": "fABPAR3Uk8_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ensemble_models(train_generator, val_generator, ensemble_configs, epochs_phase1, epochs_phase2, class_weights):\n",
        "    \"\"\"Train ensemble models with improved training strategy\"\"\"\n",
        "\n",
        "    trained_models = []  # Initialize list to store trained models\n",
        "    training_histories = []  # Initialize list to store training histories\n",
        "\n",
        "    for i, config in enumerate(ensemble_configs):  # Iterate through each model configuration\n",
        "        print(f\"\\n{'='*50}\")  # Print separator line\n",
        "        print(f\"Training Model {i+1}/{len(ensemble_configs)}: {config['name']}\")  # Display current model info\n",
        "        print(f\"{'='*50}\")  # Print separator line\n",
        "\n",
        "        # Create model\n",
        "        model, base_model = create_hybrid_mobilenet_model()  # Create new model instance\n",
        "\n",
        "        # Phase 1: Train only the head\n",
        "        print(f\"\\nPhase 1: Training head layers ({epochs_phase1} epochs)\")  # Display phase 1 info\n",
        "\n",
        "        # Compile for phase 1\n",
        "        compile_model(model, config['optimizer'], config['learning_rate'])  # Compile model with optimizer\n",
        "\n",
        "        # Enhanced callbacks for phase 1\n",
        "        callbacks_phase1 = [  # Define training callbacks for phase 1\n",
        "            EarlyStopping(  # Stop training if validation loss doesn't improve\n",
        "                monitor='val_loss',  # Monitor validation loss\n",
        "                patience=5,  # Wait 5 epochs before stopping\n",
        "                restore_best_weights=True,  # Restore best weights when stopping\n",
        "                verbose=1  # Show callback messages\n",
        "            ),\n",
        "            ModelCheckpoint(  # Save best model during training\n",
        "                filepath=os.path.join(MODEL_DIR, f\"{config['name']}_phase1_best.h5\"),  # Save path\n",
        "                monitor='val_loss',  # Monitor validation loss\n",
        "                save_best_only=True,  # Only save best model\n",
        "                verbose=1  # Show callback messages\n",
        "            ),\n",
        "            ReduceLROnPlateau(  # Reduce learning rate when loss plateaus\n",
        "                monitor='val_loss',  # Monitor validation loss\n",
        "                factor=0.5,  # Reduce learning rate by half\n",
        "                patience=3,  # Wait 3 epochs before reducing\n",
        "                min_lr=1e-7,  # Minimum learning rate\n",
        "                verbose=1  # Show callback messages\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train phase 1\n",
        "        history_phase1 = model.fit(  # Train the model\n",
        "            train_generator,  # Training data generator\n",
        "            epochs=epochs_phase1,  # Number of training epochs\n",
        "            validation_data=val_generator,  # Validation data generator\n",
        "            callbacks=callbacks_phase1,  # Training callbacks\n",
        "            class_weight=class_weights,  # Class weights for imbalanced data\n",
        "            verbose=1  # Show training progress\n",
        "        )\n",
        "\n",
        "        # Phase 2: Fine-tune entire model\n",
        "        print(f\"\\n Phase 2: Fine-tuning entire model ({epochs_phase2} epochs)\")  # Display phase 2 info\n",
        "\n",
        "        # Unfreeze base model layers\n",
        "        base_model.trainable = True  # Allow base model weights to be updated\n",
        "\n",
        "        # Freeze early layers to prevent catastrophic forgetting\n",
        "        for layer in base_model.layers[:100]:  # Iterate through first 100 layers\n",
        "            layer.trainable = False  # Freeze early layers\n",
        "\n",
        "        # Recompile with lower learning rate\n",
        "        fine_tune_lr = config['learning_rate'] * 0.1  # Reduce learning rate for fine-tuning\n",
        "        compile_model(model, config['optimizer'], fine_tune_lr)  # Recompile with new learning rate\n",
        "\n",
        "        # Enhanced callbacks for phase 2\n",
        "        callbacks_phase2 = [  # Define training callbacks for phase 2\n",
        "            EarlyStopping(  # Stop training if validation loss doesn't improve\n",
        "                monitor='val_loss',  # Monitor validation loss\n",
        "                patience=8,  # More patience for fine-tuning\n",
        "                restore_best_weights=True,  # Restore best weights when stopping\n",
        "                verbose=1  # Show callback messages\n",
        "            ),\n",
        "            ModelCheckpoint(  # Save best model during training\n",
        "                filepath=os.path.join(MODEL_DIR, f\"{config['name']}_final_best.h5\"),  # Save path\n",
        "                monitor='val_loss',  # Monitor validation loss\n",
        "                save_best_only=True,  # Only save best model\n",
        "                verbose=1  # Show callback messages\n",
        "            ),\n",
        "            ReduceLROnPlateau(  # Reduce learning rate when loss plateaus\n",
        "                monitor='val_loss',  # Monitor validation loss\n",
        "                factor=0.3,  # More aggressive learning rate reduction\n",
        "                patience=4,  # Wait 4 epochs before reducing\n",
        "                min_lr=1e-8,  # Minimum learning rate\n",
        "                verbose=1  # Show callback messages\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train phase 2\n",
        "        history_phase2 = model.fit(  # Fine-tune the model\n",
        "            train_generator,  # Training data generator\n",
        "            epochs=epochs_phase2,  # Number of fine-tuning epochs\n",
        "            validation_data=val_generator,  # Validation data generator\n",
        "            callbacks=callbacks_phase2,  # Training callbacks\n",
        "            class_weight=class_weights,  # Class weights for imbalanced data\n",
        "            verbose=1  # Show training progress\n",
        "        )\n",
        "\n",
        "        # Save final model\n",
        "        final_model_path = os.path.join(MODEL_DIR, f\"{config['name']}_final.h5\")  # Define final model path\n",
        "        model.save(final_model_path)  # Save the trained model\n",
        "\n",
        "        # Store results\n",
        "        trained_models.append({  # Add model info to results list\n",
        "            'model': model,  # Store model instance\n",
        "            'config': config,  # Store configuration\n",
        "            'path': final_model_path  # Store model file path\n",
        "        })\n",
        "\n",
        "        # Combine histories\n",
        "        combined_history = {  # Combine training histories from both phases\n",
        "            'loss': history_phase1.history['loss'] + history_phase2.history['loss'],  # Combine training losses\n",
        "            'val_loss': history_phase1.history['val_loss'] + history_phase2.history['val_loss'],  # Combine validation losses\n",
        "            'accuracy': history_phase1.history['accuracy'] + history_phase2.history['accuracy'],  # Combine training accuracies\n",
        "            'val_accuracy': history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy']  # Combine validation accuracies\n",
        "        }\n",
        "        training_histories.append(combined_history)  # Add combined history to list\n",
        "\n",
        "        print(f\" Model {config['name']} training completed!\")  # Confirm model training completion\n",
        "        print(f\"Final model saved to: {final_model_path}\")  # Display model save location\n",
        "\n",
        "    print(f\"\\n All {len(ensemble_configs)} models trained successfully!\")  # Confirm all models trained\n",
        "    return trained_models, training_histories  # Return trained models and histories"
      ],
      "metadata": {
        "id": "P1LrCYUmhBLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA LOADING AND MODEL TRAINING EXECUTION**"
      ],
      "metadata": {
        "id": "rOWvl_gKl4ZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LOAD DATASET AND CREATE GENERATORS**"
      ],
      "metadata": {
        "id": "BK9ejH9Wl_s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "print(\"Loading PlantWild dataset...\")  # Inform user of dataset loading start\n",
        "df = load_plantwild_dataframe(DATASET_ROOT, binary_only=True, stage2_only=False)  # Load dataset with binary classification\n",
        "\n",
        "# Display dataset statistics\n",
        "print(\"\\nDataset Statistics:\")  # Header for statistics section\n",
        "print(f\"Total samples: {len(df)}\")  # Display total number of samples\n",
        "print(f\"Healthy samples: {len(df[df['binary_label'] == 'healthy'])}\")  # Count healthy samples\n",
        "print(f\"Diseased samples: {len(df[df['binary_label'] == 'diseased'])}\")  # Count diseased samples\n",
        "print(f\"Train samples: {len(df[df['split'] == 'train'])}\")  # Count training samples\n",
        "print(f\"Validation samples: {len(df[df['split'] == 'val'])}\")  # Count validation samples\n",
        "print(f\"Test samples: {len(df[df['split'] == 'test'])}\")  # Count test samples\n",
        "\n",
        "# Show some examples\n",
        "print(\"\\nSample data:\")  # Header for sample data section\n",
        "print(df[['class_name', 'binary_label', 'split']].head(10))  # Display first 10 rows of key columns\n",
        "\n",
        "# Create image generators\n",
        "print(\"\\nCreating image generators...\")  # Inform user of generator creation\n",
        "train_generator, val_generator, test_generator = get_image_generators(  # Create data generators\n",
        "    df,  # Pass dataset DataFrame\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),  # Set image dimensions\n",
        "    batch_size=BATCH_SIZE,  # Set batch size\n",
        "    aug_level='strong'  # Use strong data augmentation\n",
        ")\n",
        "\n",
        "# Compute class weights\n",
        "print(\"\\nComputing class weights...\")  # Inform user of class weight computation\n",
        "class_weights = compute_class_weights(df)  # Calculate balanced class weights\n",
        "\n",
        "print(\"Dataset preparation completed!\")  # Confirm dataset preparation completion"
      ],
      "metadata": {
        "id": "5I8Vvnc-hKWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **VISUALIZE DATASET: TRAIN/VAL/TEST DISTRIBUTION, IMAGES PER CLASS AND CLASS BALANCE**"
      ],
      "metadata": {
        "id": "ogSCSx-masmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **VISUALIZE DATASET: IMAGES PER CLASS AND CLASS BALANCE**\n",
        "\n",
        "def visualize_dataset_overview(df, num_images_per_class=3, max_classes_to_show=20):\n",
        "    \"\"\"Visualize dataset with sample images per class and class distribution\"\"\"\n",
        "\n",
        "    print(\"=\"*60)  # Print separator line\n",
        "    print(\" DATASET VISUALIZATION\")  # Print section header\n",
        "    print(\"=\"*60)  # Print separator line\n",
        "\n",
        "    # 1. Class Distribution Analysis\n",
        "    print(\"\\n CLASS DISTRIBUTION ANALYSIS\")  # Print subsection header\n",
        "    print(\"-\" * 40)  # Print subsection separator\n",
        "\n",
        "    # Get class counts for each split\n",
        "    train_counts = df[df['split'] == 'train']['class_name'].value_counts()  # Count classes in training set\n",
        "    val_counts = df[df['split'] == 'val']['class_name'].value_counts()  # Count classes in validation set\n",
        "    test_counts = df[df['split'] == 'test']['class_name'].value_counts()  # Count classes in test set\n",
        "\n",
        "    print(f\"Total classes: {len(df['class_name'].unique())}\")  # Display total number of unique classes\n",
        "    print(f\"Total images: {len(df)}\")  # Display total number of images\n",
        "    print(f\"Train images: {len(df[df['split'] == 'train'])}\")  # Display number of training images\n",
        "    print(f\"Validation images: {len(df[df['split'] == 'val'])}\")  # Display number of validation images\n",
        "    print(f\"Test images: {len(df[df['split'] == 'test'])}\")  # Display number of test images\n",
        "\n",
        "    # 2. Class Balance Visualization\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(16, 12))  # Create figure with 2 subplots\n",
        "\n",
        "    # Top classes by total count\n",
        "    all_counts = df['class_name'].value_counts()  # Get counts for all classes\n",
        "    top_classes = all_counts.head(max_classes_to_show)  # Select top classes to display\n",
        "\n",
        "    # Create stacked bar chart\n",
        "    x_pos = np.arange(len(top_classes))  # Create x-axis positions\n",
        "    width = 0.25  # Set bar width\n",
        "\n",
        "    train_vals = [train_counts.get(cls, 0) for cls in top_classes.index]  # Get training counts for top classes\n",
        "    val_vals = [val_counts.get(cls, 0) for cls in top_classes.index]  # Get validation counts for top classes\n",
        "    test_vals = [test_counts.get(cls, 0) for cls in top_classes.index]  # Get test counts for top classes\n",
        "\n",
        "    axes[0].bar(x_pos - width, train_vals, width, label='Train', color='#2E86AB', alpha=0.8)  # Plot training bars\n",
        "    axes[0].bar(x_pos, val_vals, width, label='Validation', color='#A23B72', alpha=0.8)  # Plot validation bars\n",
        "    axes[0].bar(x_pos + width, test_vals, width, label='Test', color='#F18F01', alpha=0.8)  # Plot test bars\n",
        "\n",
        "    axes[0].set_xlabel('Classes', fontsize=12, fontweight='bold')  # Set x-axis label\n",
        "    axes[0].set_ylabel('Number of Images', fontsize=12, fontweight='bold')  # Set y-axis label\n",
        "    axes[0].set_title(f'Class Distribution - Top {max_classes_to_show} Classes', fontsize=14, fontweight='bold')  # Set title\n",
        "    axes[0].set_xticks(x_pos)  # Set x-axis tick positions\n",
        "    axes[0].set_xticklabels([cls[:20] + '...' if len(cls) > 20 else cls for cls in top_classes.index],  # Set x-axis labels\n",
        "                           rotation=45, ha='right')  # Rotate labels for readability\n",
        "    axes[0].legend()  # Add legend\n",
        "    axes[0].grid(True, alpha=0.3)  # Add grid\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, (train, val, test) in enumerate(zip(train_vals, val_vals, test_vals)):  # Iterate through bar values\n",
        "        if train > 0:  # If training count is greater than 0\n",
        "            axes[0].text(i - width, train + 5, str(train), ha='center', va='bottom', fontsize=8)  # Add training count label\n",
        "        if val > 0:  # If validation count is greater than 0\n",
        "            axes[0].text(i, val + 5, str(val), ha='center', va='bottom', fontsize=8)  # Add validation count label\n",
        "        if test > 0:  # If test count is greater than 0\n",
        "            axes[0].text(i + width, test + 5, str(test), ha='center', va='bottom', fontsize=8)  # Add test count label\n",
        "\n",
        "    # 3. Binary Label Distribution\n",
        "    binary_counts = df['binary_label'].value_counts()  # Count binary labels\n",
        "    colors = ['#4CAF50', '#F44336']  # Green for healthy, Red for diseased\n",
        "\n",
        "    axes[1].bar(binary_counts.index, binary_counts.values, color=colors, alpha=0.8)  # Plot binary label bars\n",
        "    axes[1].set_xlabel('Binary Labels', fontsize=12, fontweight='bold')  # Set x-axis label\n",
        "    axes[1].set_ylabel('Number of Images', fontsize=12, fontweight='bold')  # Set y-axis label\n",
        "    axes[1].set_title('Healthy vs Diseased Distribution', fontsize=14, fontweight='bold')  # Set title\n",
        "    axes[1].grid(True, alpha=0.3)  # Add grid\n",
        "\n",
        "    # Add value labels\n",
        "    for i, (label, count) in enumerate(binary_counts.items()):  # Iterate through binary label counts\n",
        "        axes[1].text(i, count + 50, str(count), ha='center', va='bottom', fontsize=12, fontweight='bold')  # Add count labels\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "    plt.savefig('/content/dataset_class_distribution.png', dpi=300, bbox_inches='tight')  # Save high-quality image\n",
        "    plt.show()  # Display the plot\n",
        "\n",
        "    # 4. Sample Images Per Class\n",
        "    print(f\"\\n SAMPLE IMAGES PER CLASS (showing first {max_classes_to_show} classes)\")  # Print subsection header\n",
        "    print(\"-\" * 60)  # Print subsection separator\n",
        "\n",
        "    # Get sample images for each class\n",
        "    sample_images = {}  # Initialize dictionary for sample images\n",
        "    for class_name in top_classes.index[:max_classes_to_show]:  # Iterate through top classes\n",
        "        class_df = df[df['class_name'] == class_name]  # Filter data for current class\n",
        "        if len(class_df) >= num_images_per_class:  # Check if enough images available\n",
        "            samples = class_df.sample(n=num_images_per_class)  # Randomly sample images\n",
        "            sample_images[class_name] = samples  # Store samples\n",
        "\n",
        "    # Create visualization grid\n",
        "    num_classes_to_show = min(len(sample_images), max_classes_to_show)  # Determine number of classes to show\n",
        "    fig, axes = plt.subplots(num_classes_to_show, num_images_per_class,  # Create subplot grid\n",
        "                            figsize=(4*num_images_per_class, 4*num_classes_to_show))\n",
        "\n",
        "    if num_classes_to_show == 1:  # Handle single class case\n",
        "        axes = axes.reshape(1, -1)  # Reshape axes for single row\n",
        "\n",
        "    for i, (class_name, samples) in enumerate(list(sample_images.items())[:num_classes_to_show]):  # Iterate through classes\n",
        "        for j, (_, sample) in enumerate(samples.iterrows()):  # Iterate through samples for each class\n",
        "            try:\n",
        "                # Load and display image\n",
        "                img = plt.imread(sample['image_path'])  # Load image from path\n",
        "                axes[i, j].imshow(img)  # Display image\n",
        "                axes[i, j].set_title(f\"{class_name[:15]}...\\n{sample['binary_label']}\",  # Set subplot title\n",
        "                                   fontsize=8, fontweight='bold')\n",
        "                axes[i, j].axis('off')  # Hide axes\n",
        "\n",
        "                # Add border color based on binary label\n",
        "                if sample['binary_label'] == 'healthy':  # If image is healthy\n",
        "                    axes[i, j].spines['bottom'].set_color('green')  # Set bottom border to green\n",
        "                    axes[i, j].spines['top'].set_color('green')  # Set top border to green\n",
        "                    axes[i, j].spines['right'].set_color('green')  # Set right border to green\n",
        "                    axes[i, j].spines['left'].set_color('green')  # Set left border to green\n",
        "                    axes[i, j].spines['bottom'].set_linewidth(3)  # Set bottom border width\n",
        "                    axes[i, j].spines['top'].set_linewidth(3)  # Set top border width\n",
        "                    axes[i, j].spines['right'].set_linewidth(3)  # Set right border width\n",
        "                    axes[i, j].spines['left'].set_linewidth(3)  # Set left border width\n",
        "                else:  # If image is diseased\n",
        "                    axes[i, j].spines['bottom'].set_color('red')  # Set bottom border to red\n",
        "                    axes[i, j].spines['top'].set_color('red')  # Set top border to red\n",
        "                    axes[i, j].spines['right'].set_color('red')  # Set right border to red\n",
        "                    axes[i, j].spines['left'].set_color('red')  # Set left border to red\n",
        "                    axes[i, j].spines['bottom'].set_linewidth(3)  # Set bottom border width\n",
        "                    axes[i, j].spines['top'].set_linewidth(3)  # Set top border width\n",
        "                    axes[i, j].spines['right'].set_linewidth(3)  # Set right border width\n",
        "                    axes[i, j].spines['left'].set_linewidth(3)  # Set left border width\n",
        "\n",
        "            except Exception as e:  # Handle image loading errors\n",
        "                axes[i, j].text(0.5, 0.5, 'Error\\nLoading Image',  # Display error message\n",
        "                               ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "                axes[i, j].axis('off')  # Hide axes\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "    plt.savefig('/content/sample_images_per_class.png', dpi=300, bbox_inches='tight')  # Save high-quality image\n",
        "    plt.show()  # Display the plot\n",
        "\n",
        "    # 5. Summary Statistics\n",
        "    print(f\"\\n SUMMARY STATISTICS\")  # Print subsection header\n",
        "    print(\"-\" * 30)  # Print subsection separator\n",
        "    print(f\"Most common class: {all_counts.index[0]} ({all_counts.iloc[0]} images)\")  # Display most common class\n",
        "    print(f\"Least common class: {all_counts.index[-1]} ({all_counts.iloc[-1]} images)\")  # Display least common class\n",
        "    print(f\"Average images per class: {all_counts.mean():.1f}\")  # Display average images per class\n",
        "    print(f\"Standard deviation: {all_counts.std():.1f}\")  # Display standard deviation\n",
        "    print(f\"Classes with < 10 images: {(all_counts < 10).sum()}\")  # Count classes with few images\n",
        "    print(f\"Classes with > 100 images: {(all_counts > 100).sum()}\")  # Count classes with many images\n",
        "\n",
        "    # Binary label statistics\n",
        "    print(f\"\\nHealthy images: {binary_counts.get('healthy', 0)} ({binary_counts.get('healthy', 0)/len(df)*100:.1f}%)\")  # Display healthy image statistics\n",
        "    print(f\"Diseased images: {binary_counts.get('diseased', 0)} ({binary_counts.get('diseased', 0)/len(df)*100:.1f}%)\")  # Display diseased image statistics\n",
        "\n",
        "    print(f\"\\n Visualizations saved:\")  # Print save confirmation\n",
        "    print(f\"   Class distribution: /content/dataset_class_distribution.png\")  # Display class distribution file path\n",
        "    print(f\"   Sample images: /content/sample_images_per_class.png\")  # Display sample images file path\n",
        "\n",
        "# **RUN THE VISUALIZATION**\n",
        "# You can call this function anywhere after loading your dataset\n",
        "visualize_dataset_overview(df, num_images_per_class=3, max_classes_to_show=20)  # Execute visualization function"
      ],
      "metadata": {
        "id": "pKO8beF4hu-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TRAINING ENSEMBLE MODELS**"
      ],
      "metadata": {
        "id": "ilEatEN-mZsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the ensemble models\n",
        "print(\" Starting ensemble model training...\")  # Inform user of training start\n",
        "print(f\"Training {len(ENSEMBLE_CONFIGS)} models with configurations:\")  # Display number of models to train\n",
        "for i, config in enumerate(ENSEMBLE_CONFIGS):  # Iterate through each model configuration\n",
        "    print(f\"  {i+1}. {config['name']}: {config['optimizer'].__name__} (lr={config['learning_rate']})\")  # Display model details\n",
        "\n",
        "# Start training\n",
        "trained_models, training_histories = train_ensemble_models(  # Execute ensemble training function\n",
        "    train_generator=train_generator,  # Pass training data generator\n",
        "    val_generator=val_generator,  # Pass validation data generator\n",
        "    ensemble_configs=ENSEMBLE_CONFIGS,  # Pass model configurations\n",
        "    epochs_phase1=EPOCHS_PHASE1,  # Pass phase 1 epoch count\n",
        "    epochs_phase2=EPOCHS_PHASE2,  # Pass phase 2 epoch count\n",
        "    class_weights=class_weights  # Pass balanced class weights\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed! Models saved to:\", MODEL_DIR)  # Confirm training completion and save location\n",
        "print(\"Available models:\")  # Header for model list\n",
        "for model_info in trained_models:  # Iterate through trained models\n",
        "    print(f\"  - {model_info['config']['name']}: {model_info['path']}\")  # Display each model's name and path\n",
        "\n",
        "# Save training histories for later analysis\n",
        "import pickle  # Import pickle for serialization\n",
        "histories_path = os.path.join(MODEL_DIR, 'training_histories.pkl')  # Define path for training histories\n",
        "with open(histories_path, 'wb') as f:  # Open file for binary writing\n",
        "    pickle.dump(training_histories, f)  # Save training histories to file\n",
        "print(f\" Training histories saved to: {histories_path}\")  # Confirm histories save location"
      ],
      "metadata": {
        "id": "LZgp7fsJiEgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL EVALUATION AND ANALYSIS**"
      ],
      "metadata": {
        "id": "Njxpi9pRnSvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Best Models and Evaluate**"
      ],
      "metadata": {
        "id": "dKm_ccjfnSXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_best_models(model_dir):\n",
        "    \"\"\"Load the best performing models from training\"\"\"\n",
        "    best_models = []  # Initialize empty list for best models\n",
        "\n",
        "    for config in ENSEMBLE_CONFIGS:  # Iterate through each model configuration\n",
        "        model_name = config['name']  # Get model name from configuration\n",
        "        best_path = os.path.join(model_dir, f\"{model_name}_final_best.h5\")  # Construct path to best model file\n",
        "\n",
        "        if os.path.exists(best_path):  # Check if model file exists\n",
        "            try:\n",
        "                model = load_model(best_path)  # Load the trained model\n",
        "                best_models.append({  # Add model information to list\n",
        "                    'model': model,  # Store model instance\n",
        "                    'config': config,  # Store configuration\n",
        "                    'path': best_path  # Store file path\n",
        "                })\n",
        "                print(f\"Loaded {model_name}: {best_path}\")  # Confirm successful loading\n",
        "            except Exception as e:  # Handle loading errors\n",
        "                print(f\"Failed to load {model_name}: {e}\")  # Display error message\n",
        "        else:  # If model file doesn't exist\n",
        "            print(f\"Model not found: {best_path}\")  # Display missing file message\n",
        "\n",
        "    return best_models  # Return list of loaded models\n",
        "\n",
        "def evaluate_models(models, test_generator):\n",
        "    \"\"\"Evaluate all models on test set\"\"\"\n",
        "    results = []  # Initialize empty list for evaluation results\n",
        "\n",
        "    for i, model_info in enumerate(models):  # Iterate through each model\n",
        "        model = model_info['model']  # Get model instance\n",
        "        config = model_info['config']  # Get model configuration\n",
        "\n",
        "        print(f\"\\n Evaluating {config['name']}...\")  # Display current model being evaluated\n",
        "\n",
        "        # Reset generator\n",
        "        test_generator.reset()  # Reset test generator to beginning\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = model.predict(test_generator, verbose=1)  # Generate predictions on test set\n",
        "        true_labels = test_generator.classes  # Get true labels from generator\n",
        "\n",
        "        # Ensure predictions and true_labels are numpy arrays\n",
        "        predictions = np.array(predictions).flatten()  # Convert to numpy array and flatten\n",
        "        true_labels = np.array(true_labels).flatten()  # Convert to numpy array and flatten\n",
        "\n",
        "        # Calculate metrics\n",
        "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score  # Import evaluation metrics\n",
        "\n",
        "        # Convert predictions to binary\n",
        "        pred_binary = (predictions > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "        accuracy = accuracy_score(true_labels, pred_binary)  # Calculate accuracy\n",
        "        precision = precision_score(true_labels, pred_binary)  # Calculate precision\n",
        "        recall = recall_score(true_labels, pred_binary)  # Calculate recall\n",
        "        f1 = f1_score(true_labels, pred_binary)  # Calculate F1 score\n",
        "        auc = roc_auc_score(true_labels, predictions)  # Calculate AUC\n",
        "\n",
        "        results.append({  # Add evaluation results to list\n",
        "            'model_name': config['name'],  # Store model name\n",
        "            'accuracy': accuracy,  # Store accuracy score\n",
        "            'precision': precision,  # Store precision score\n",
        "            'recall': recall,  # Store recall score\n",
        "            'f1_score': f1,  # Store F1 score\n",
        "            'auc': auc,  # Store AUC score\n",
        "            'predictions': predictions,  # Store raw predictions\n",
        "            'true_labels': true_labels  # Store true labels\n",
        "        })\n",
        "\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")  # Display accuracy\n",
        "        print(f\"  Precision: {precision:.4f}\")  # Display precision\n",
        "        print(f\"  Recall: {recall:.4f}\")  # Display recall\n",
        "        print(f\"  F1-Score: {f1:.4f}\")  # Display F1 score\n",
        "        print(f\"  AUC: {auc:.4f}\")  # Display AUC\n",
        "\n",
        "    return results  # Return evaluation results"
      ],
      "metadata": {
        "id": "_ozcJN6ZiXCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Comparison and Visualization**"
      ],
      "metadata": {
        "id": "5vLw-i-KqE8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(results):\n",
        "    \"\"\"Compare and visualize model performance with unique colors and proper saving\"\"\"\n",
        "\n",
        "    # Define unique colors for each optimizer\n",
        "    optimizer_colors = {  # Define color scheme for different optimizers\n",
        "        'model_adam': '#FF6B6B',      # Red color for Adam optimizer\n",
        "        'model_sgd': '#4ECDC4',       # Teal color for SGD optimizer\n",
        "        'model_rmsprop': '#45B7D1'    # Blue color for RMSprop optimizer\n",
        "    }\n",
        "\n",
        "    # Create comparison DataFrame\n",
        "    comparison_df = pd.DataFrame([  # Create DataFrame from evaluation results\n",
        "        {\n",
        "            'Model': r['model_name'],  # Extract model name\n",
        "            'Accuracy': r['accuracy'],  # Extract accuracy score\n",
        "            'Precision': r['precision'],  # Extract precision score\n",
        "            'Recall': r['recall'],  # Extract recall score\n",
        "            'F1-Score': r['f1_score'],  # Extract F1 score\n",
        "            'AUC': r['auc']  # Extract AUC score\n",
        "        }\n",
        "        for r in results  # Iterate through all results\n",
        "    ])\n",
        "\n",
        "    print(\" Model Performance Comparison:\")  # Print section header\n",
        "    print(comparison_df.round(4))  # Display comparison table with 4 decimal places\n",
        "\n",
        "    # Find best model\n",
        "    best_model_idx = comparison_df['F1-Score'].idxmax()  # Find index of model with highest F1 score\n",
        "    best_model = comparison_df.iloc[best_model_idx]  # Get best model row\n",
        "\n",
        "    print(f\"\\n Best Model: {best_model['Model']}\")  # Display best model name\n",
        "    print(f\"   F1-Score: {best_model['F1-Score']:.4f}\")  # Display best model F1 score\n",
        "    print(f\"   Accuracy: {best_model['Accuracy']:.4f}\")  # Display best model accuracy\n",
        "\n",
        "    # Visualization with unique colors\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # Create 2x2 subplot grid\n",
        "\n",
        "    # Get colors for each model\n",
        "    colors = [optimizer_colors.get(model, '#666666') for model in comparison_df['Model']]  # Assign colors to models\n",
        "\n",
        "    # Accuracy comparison\n",
        "    bars1 = axes[0,0].bar(comparison_df['Model'], comparison_df['Accuracy'], color=colors)  # Create accuracy bar chart\n",
        "    axes[0,0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')  # Set subplot title\n",
        "    axes[0,0].set_ylabel('Accuracy', fontsize=12)  # Set y-axis label\n",
        "    axes[0,0].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
        "    axes[0,0].grid(True, alpha=0.3)  # Add grid\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars1, comparison_df['Accuracy']):  # Iterate through bars and values\n",
        "        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,  # Position text above bar\n",
        "                      f'{value:.3f}', ha='center', va='bottom', fontweight='bold')  # Add value label\n",
        "\n",
        "    # F1-Score comparison\n",
        "    bars2 = axes[0,1].bar(comparison_df['Model'], comparison_df['F1-Score'], color=colors)  # Create F1-score bar chart\n",
        "    axes[0,1].set_title('Model F1-Score Comparison', fontsize=14, fontweight='bold')  # Set subplot title\n",
        "    axes[0,1].set_ylabel('F1-Score', fontsize=12)  # Set y-axis label\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
        "    axes[0,1].grid(True, alpha=0.3)  # Add grid\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars2, comparison_df['F1-Score']):  # Iterate through bars and values\n",
        "        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,  # Position text above bar\n",
        "                      f'{value:.3f}', ha='center', va='bottom', fontweight='bold')  # Add value label\n",
        "\n",
        "    # Precision vs Recall scatter\n",
        "    scatter = axes[1,0].scatter(comparison_df['Precision'], comparison_df['Recall'],  # Create scatter plot\n",
        "                               s=200, c=colors, alpha=0.7, edgecolors='black', linewidth=2)  # Set scatter properties\n",
        "    for i, model in enumerate(comparison_df['Model']):  # Iterate through models\n",
        "        axes[1,0].annotate(model, (comparison_df['Precision'].iloc[i], comparison_df['Recall'].iloc[i]),  # Position annotation\n",
        "                          xytext=(5, 5), textcoords='offset points', fontsize=10, fontweight='bold')  # Add model labels\n",
        "    axes[1,0].set_xlabel('Precision', fontsize=12)  # Set x-axis label\n",
        "    axes[1,0].set_ylabel('Recall', fontsize=12)  # Set y-axis label\n",
        "    axes[1,0].set_title('Precision vs Recall', fontsize=14, fontweight='bold')  # Set subplot title\n",
        "    axes[1,0].grid(True, alpha=0.3)  # Add grid\n",
        "\n",
        "    # AUC comparison\n",
        "    bars3 = axes[1,1].bar(comparison_df['Model'], comparison_df['AUC'], color=colors)  # Create AUC bar chart\n",
        "    axes[1,1].set_title('Model AUC Comparison', fontsize=14, fontweight='bold')  # Set subplot title\n",
        "    axes[1,1].set_ylabel('AUC', fontsize=12)  # Set y-axis label\n",
        "    axes[1,1].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
        "    axes[1,1].grid(True, alpha=0.3)  # Add grid\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars3, comparison_df['AUC']):  # Iterate through bars and values\n",
        "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,  # Position text above bar\n",
        "                      f'{value:.3f}', ha='center', va='bottom', fontweight='bold')  # Add value label\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "    plt.savefig('/content/model_comparison_analysis.png', dpi=300, bbox_inches='tight')  # Save high-quality image\n",
        "    plt.show()  # Display the plot\n",
        "\n",
        "    return comparison_df, best_model_idx  # Return comparison data and best model index"
      ],
      "metadata": {
        "id": "gClOJ0Ntib6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Confusion Matrix Analysis**"
      ],
      "metadata": {
        "id": "td3Czq3GrKtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_confusion_matrices(results, test_generator):\n",
        "    \"\"\"Create detailed confusion matrix analysis with unique colors and proper spacing\"\"\"\n",
        "\n",
        "    # Define unique colors for each optimizer\n",
        "    optimizer_colors = {  # Define color schemes for different optimizers\n",
        "        'model_adam': 'Reds',  # Red color scheme for Adam optimizer\n",
        "        'model_sgd': 'Blues',  # Blue color scheme for SGD optimizer\n",
        "        'model_rmsprop': 'Greens'  # Green color scheme for RMSprop optimizer\n",
        "    }\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(results), figsize=(6*len(results), 5))  # Create subplot grid for confusion matrices\n",
        "    if len(results) == 1:  # Handle single result case\n",
        "        axes = [axes]  # Convert single axis to list\n",
        "\n",
        "    for i, result in enumerate(results):  # Iterate through each model result\n",
        "        model_name = result['model_name']  # Get model name\n",
        "        predictions = result['predictions']  # Get model predictions\n",
        "        true_labels = result['true_labels']  # Get true labels\n",
        "\n",
        "        # Fix dimensionality issue - ensure predictions is 1D\n",
        "        if predictions.ndim > 1:  # Check if predictions has multiple dimensions\n",
        "            predictions = predictions.flatten()  # Flatten to 1D array\n",
        "        if true_labels.ndim > 1:  # Check if true labels has multiple dimensions\n",
        "            true_labels = true_labels.flatten()  # Flatten to 1D array\n",
        "\n",
        "        # Create confusion matrix\n",
        "        pred_binary = (predictions > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "        cm = confusion_matrix(true_labels, pred_binary)  # Calculate confusion matrix\n",
        "\n",
        "        # Plot confusion matrix with unique color\n",
        "        color_map = optimizer_colors.get(model_name, 'Blues')  # Get color scheme for model\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Healthy', 'Diseased'])  # Create confusion matrix display\n",
        "        disp.plot(ax=axes[i], cmap=color_map, values_format='d')  # Plot confusion matrix\n",
        "        axes[i].set_title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')  # Set subplot title\n",
        "\n",
        "        # Add text annotations with better positioning\n",
        "        for j in range(2):  # Iterate through rows (true labels)\n",
        "            for k in range(2):  # Iterate through columns (predicted labels)\n",
        "                text = axes[i].text(k, j, str(cm[j, k]), ha='center', va='center',  # Add count text\n",
        "                                  fontsize=16, fontweight='bold', color='white')  # Set text properties\n",
        "                # Add black outline for better visibility - FIXED VERSION\n",
        "                text.set_path_effects([matplotlib.patheffects.withStroke(linewidth=2, foreground='black')])  # Add text outline\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "    plt.savefig('/content/confusion_matrices_analysis.png', dpi=300, bbox_inches='tight')  # Save high-quality image\n",
        "    plt.show()  # Display the plot\n",
        "\n",
        "    # Print detailed analysis\n",
        "    for result in results:  # Iterate through each model result\n",
        "        print(f\"\\n Detailed Analysis - {result['model_name']}:\")  # Print model name header\n",
        "        predictions = result['predictions']  # Get model predictions\n",
        "        true_labels = result['true_labels']  # Get true labels\n",
        "\n",
        "        # Fix dimensionality\n",
        "        if predictions.ndim > 1:  # Check if predictions has multiple dimensions\n",
        "            predictions = predictions.flatten()  # Flatten to 1D array\n",
        "        if true_labels.ndim > 1:  # Check if true labels has multiple dimensions\n",
        "            true_labels = true_labels.flatten()  # Flatten to 1D array\n",
        "\n",
        "        pred_binary = (predictions > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        from sklearn.metrics import classification_report  # Import classification report\n",
        "        print(classification_report(true_labels, pred_binary, target_names=['Healthy', 'Diseased']))  # Print detailed metrics"
      ],
      "metadata": {
        "id": "Uea9FSRq_KH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prediction Distribution Analysis**"
      ],
      "metadata": {
        "id": "Cd9XG7EGsDHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_prediction_distributions(results):\n",
        "    \"\"\"Analyze prediction confidence distributions with unique colors and proper dimensionality handling\"\"\"\n",
        "\n",
        "    # Define unique colors for each optimizer\n",
        "    optimizer_colors = {  # Define color scheme for different optimizers\n",
        "        'model_adam': '#FF6B6B',      # Red color for Adam optimizer\n",
        "        'model_sgd': '#4ECDC4',       # Teal color for SGD optimizer\n",
        "        'model_rmsprop': '#45B7D1'    # Blue color for RMSprop optimizer\n",
        "    }\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(results), figsize=(6*len(results), 5))  # Create subplot grid for distributions\n",
        "    if len(results) == 1:  # Handle single result case\n",
        "        axes = [axes]  # Convert single axis to list\n",
        "\n",
        "    for i, result in enumerate(results):  # Iterate through each model result\n",
        "        model_name = result['model_name']  # Get model name\n",
        "        predictions = result['predictions']  # Get model predictions\n",
        "        true_labels = result['true_labels']  # Get true labels\n",
        "\n",
        "        # Fix dimensionality issue - ensure arrays are 1D\n",
        "        if predictions.ndim > 1:  # Check if predictions has multiple dimensions\n",
        "            predictions = predictions.flatten()  # Flatten to 1D array\n",
        "        if true_labels.ndim > 1:  # Check if true labels has multiple dimensions\n",
        "            true_labels = true_labels.flatten()  # Flatten to 1D array\n",
        "\n",
        "        # Separate predictions by true label\n",
        "        healthy_preds = predictions[true_labels == 0]  # Get predictions for healthy samples\n",
        "        diseased_preds = predictions[true_labels == 1]  # Get predictions for diseased samples\n",
        "\n",
        "        # Get color for this model\n",
        "        color = optimizer_colors.get(model_name, '#666666')  # Get color for current model\n",
        "\n",
        "        # Plot distributions with unique colors\n",
        "        axes[i].hist(healthy_preds, bins=30, alpha=0.7, label='Healthy', color='green', edgecolor='black')  # Plot healthy predictions histogram\n",
        "        axes[i].hist(diseased_preds, bins=30, alpha=0.7, label='Diseased', color='red', edgecolor='black')  # Plot diseased predictions histogram\n",
        "        axes[i].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Threshold (0.5)')  # Add decision threshold line\n",
        "        axes[i].set_xlabel('Prediction Confidence', fontsize=12)  # Set x-axis label\n",
        "        axes[i].set_ylabel('Frequency', fontsize=12)  # Set y-axis label\n",
        "        axes[i].set_title(f'Prediction Distribution - {model_name}', fontsize=14, fontweight='bold')  # Set subplot title\n",
        "        axes[i].legend(fontsize=10)  # Add legend\n",
        "        axes[i].grid(True, alpha=0.3)  # Add grid\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "    plt.savefig('/content/prediction_distributions_analysis.png', dpi=300, bbox_inches='tight')  # Save high-quality image\n",
        "    plt.show()  # Display the plot\n",
        "\n",
        "    # Print statistics\n",
        "    for result in results:  # Iterate through each model result\n",
        "        print(f\"\\n Prediction Statistics - {result['model_name']}:\")  # Print model name header\n",
        "        predictions = result['predictions']  # Get model predictions\n",
        "        true_labels = result['true_labels']  # Get true labels\n",
        "\n",
        "        # Fix dimensionality\n",
        "        if predictions.ndim > 1:  # Check if predictions has multiple dimensions\n",
        "            predictions = predictions.flatten()  # Flatten to 1D array\n",
        "        if true_labels.ndim > 1:  # Check if true labels has multiple dimensions\n",
        "            true_labels = true_labels.flatten()  # Flatten to 1D array\n",
        "\n",
        "        healthy_preds = predictions[true_labels == 0]  # Get predictions for healthy samples\n",
        "        diseased_preds = predictions[true_labels == 1]  # Get predictions for diseased samples\n",
        "\n",
        "        print(f\"  Healthy predictions - Mean: {healthy_preds.mean():.4f}, Std: {healthy_preds.std():.4f}\")  # Print healthy prediction statistics\n",
        "        print(f\"  Diseased predictions - Mean: {diseased_preds.mean():.4f}, Std: {diseased_preds.std():.4f}\")  # Print diseased prediction statistics\n",
        "        print(f\"  Prediction range: {predictions.min():.4f} to {predictions.max():.4f}\")  # Print overall prediction range"
      ],
      "metadata": {
        "id": "RYTLpDpqj6G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Select Best Model From Evaluation**"
      ],
      "metadata": {
        "id": "rrvLB_uysXrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and evaluate all trained models\n",
        "print(\" Loading best models for evaluation...\")  # Inform user of model loading start\n",
        "best_models = load_best_models(MODEL_DIR)  # Load the best performing models from training\n",
        "\n",
        "if best_models:  # Check if models were successfully loaded\n",
        "    print(f\"\\n Evaluating {len(best_models)} models on test set...\")  # Display number of models to evaluate\n",
        "    evaluation_results = evaluate_models(best_models, test_generator)  # Evaluate all models on test set\n",
        "\n",
        "    # Compare models and find the best one\n",
        "    comparison_df, best_idx = compare_models(evaluation_results)  # Compare models and identify best performer\n",
        "\n",
        "    # Analyze confusion matrices\n",
        "    print(\"\\n Creating confusion matrix analysis...\")  # Inform user of confusion matrix creation\n",
        "    analyze_confusion_matrices(evaluation_results, test_generator)  # Create confusion matrix visualizations\n",
        "\n",
        "    # Analyze prediction distributions\n",
        "    print(\"\\n Analyzing prediction distributions...\")  # Inform user of distribution analysis\n",
        "    analyze_prediction_distributions(evaluation_results)  # Create prediction distribution plots\n",
        "\n",
        "    # Save the best model for deployment\n",
        "    best_model_info = best_models[best_idx]  # Get information about the best model\n",
        "    best_model = best_model_info['model']  # Extract the best model instance\n",
        "    best_model_name = best_model_info['config']['name']  # Get the best model name\n",
        "\n",
        "    # Save best model with clear name\n",
        "    deployment_model_path = os.path.join(MODEL_DIR, 'best_stage1_model.h5')  # Define deployment model path\n",
        "    best_model.save(deployment_model_path)  # Save the best model for deployment\n",
        "\n",
        "    print(f\"\\n Best model selected and saved!\")  # Confirm best model selection and saving\n",
        "    print(f\"Model: {best_model_name}\")  # Display best model name\n",
        "    print(f\"Path: {deployment_model_path}\")  # Display model save path\n",
        "    print(f\"F1-Score: {comparison_df.iloc[best_idx]['F1-Score']:.4f}\")  # Display best model F1 score\n",
        "\n",
        "    # Save evaluation results\n",
        "    import pickle  # Import pickle for serialization\n",
        "    results_path = os.path.join(MODEL_DIR, 'evaluation_results.pkl')  # Define results file path\n",
        "    with open(results_path, 'wb') as f:  # Open file for binary writing\n",
        "        pickle.dump(evaluation_results, f)  # Save evaluation results to file\n",
        "\n",
        "    print(f\"Evaluation results saved to: {results_path}\")  # Confirm results save location\n",
        "\n",
        "else:  # If no models were found\n",
        "    print(\"No trained models found! Please run the training cells first.\")  # Display error message"
      ],
      "metadata": {
        "id": "ZiMTolAg-RUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ADVANCED EVALUATION AND ANALYSIS**"
      ],
      "metadata": {
        "id": "paTgA3OZMHmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **DEFINE THE LOAD FUNCTION**\n",
        "\n",
        "def load_saved_data_from_drive():\n",
        "    \"\"\"Load saved models and training histories from Google Drive\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" LOADING SAVED DATA FROM GOOGLE DRIVE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    drive_path = \"/content/drive/MyDrive/plantwild_stage1_models\"\n",
        "\n",
        "    # Load training histories\n",
        "    print(\"Loading training histories...\")\n",
        "    try:\n",
        "        histories_path = os.path.join(drive_path, 'training_histories.pkl')\n",
        "        with open(histories_path, 'rb') as f:\n",
        "            training_histories = pickle.load(f)\n",
        "        print(f\" Training histories loaded: {len(training_histories)} models\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading training histories: {e}\")\n",
        "        training_histories = None\n",
        "\n",
        "    # Load evaluation results\n",
        "    print(\"Loading evaluation results...\")\n",
        "    try:\n",
        "        eval_path = os.path.join(drive_path, 'evaluation_results.pkl')\n",
        "        with open(eval_path, 'rb') as f:\n",
        "            evaluation_results = pickle.load(f)\n",
        "        print(f\" Evaluation results loaded: {len(evaluation_results)} models\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading evaluation results: {e}\")\n",
        "        evaluation_results = None\n",
        "\n",
        "    # Load training summary\n",
        "    print(\"Loading training summary...\")\n",
        "    try:\n",
        "        summary_path = os.path.join(drive_path, 'training_summary.json')\n",
        "        with open(summary_path, 'r') as f:\n",
        "            training_summary = json.load(f)\n",
        "        print(f\" Training summary loaded\")\n",
        "        print(f\"  Best model: {training_summary.get('best_model_name', 'Unknown')}\")\n",
        "        print(f\"  Best F1-Score: {training_summary.get('best_f1_score', 'Unknown')}\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading training summary: {e}\")\n",
        "        training_summary = None\n",
        "\n",
        "    # Check model files\n",
        "    print(\"\\nAvailable model files:\")\n",
        "    model_files = [f for f in os.listdir(drive_path) if f.endswith('.h5')]\n",
        "    for file in model_files:\n",
        "        file_path = os.path.join(drive_path, file)\n",
        "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "        print(f\"  {file} ({size_mb:.1f} MB)\")\n",
        "\n",
        "    return training_histories, evaluation_results, training_summary\n",
        "\n",
        "print(\"Load function defined successfully!\")"
      ],
      "metadata": {
        "id": "rlxcwK8mhfxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **RUN THE LOAD FUNCTION **\n",
        "\n",
        "# Import required modules\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "print(\"Loading your saved data from Google Drive...\")\n",
        "training_histories, evaluation_results, training_summary = load_saved_data_from_drive()\n",
        "\n",
        "# Make these available globally\n",
        "if training_histories:\n",
        "    globals()['training_histories'] = training_histories\n",
        "    print(\" Training histories made available globally\")\n",
        "if evaluation_results:\n",
        "    globals()['evaluation_results'] = evaluation_results\n",
        "    print(\" Evaluation results made available globally\")\n",
        "if training_summary:\n",
        "    globals()['training_summary'] = training_summary\n",
        "    print(\" Training summary made available globally\")\n",
        "\n",
        "print(\"\\nData loading completed!\")"
      ],
      "metadata": {
        "id": "SKMrQgS_h_XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **QUICK MODEL LOADING**\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the best model\n",
        "best_model = load_model('/content/drive/MyDrive/plantwild_stage1_models/model_rmsprop_final_best.h5')\n",
        "\n",
        "print(\" Model loaded successfully!\")\n",
        "print(f\"Input shape: {best_model.input_shape}\")\n",
        "print(f\"Output shape: {best_model.output_shape}\")\n",
        "\n",
        "# Now you can run your enhanced cells"
      ],
      "metadata": {
        "id": "4PmTnaoeSjli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Accuracy and Loss Plots**"
      ],
      "metadata": {
        "id": "gi0yvakMOBOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **CREATE COMPREHENSIVE TRAINING PLOTS**\n",
        "\n",
        "print(\" Training histories available - creating comprehensive plots...\")\n",
        "\n",
        "def create_real_training_plots():\n",
        "    \"\"\"Create comprehensive training plots using your actual training data\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" CREATING COMPREHENSIVE TRAINING PLOTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Define colors for each optimizer\n",
        "    optimizer_colors = {\n",
        "        'model_adam': '#FF6B6B',      # Red\n",
        "        'model_sgd': '#4ECDC4',       # Teal\n",
        "        'model_rmsprop': '#45B7D1'    # Blue\n",
        "    }\n",
        "\n",
        "    # Create comprehensive subplot grid\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "\n",
        "    for i, (history, config) in enumerate(zip(training_histories, ENSEMBLE_CONFIGS)):\n",
        "        model_name = config['name']\n",
        "        color = optimizer_colors.get(model_name, '#666666')\n",
        "\n",
        "        # Extract training metrics\n",
        "        epochs = range(1, len(history['loss']) + 1)\n",
        "\n",
        "        # Plot 1: Training Loss\n",
        "        axes[0, 0].plot(epochs, history['loss'],\n",
        "                        color=color, linewidth=2,\n",
        "                        label=f'{model_name} ({config[\"optimizer\"].__name__})',\n",
        "                        alpha=0.8)\n",
        "\n",
        "        # Plot 2: Validation Loss\n",
        "        axes[0, 1].plot(epochs, history['val_loss'],\n",
        "                        color=color, linewidth=2,\n",
        "                        label=f'{model_name} ({config[\"optimizer\"].__name__})',\n",
        "                        alpha=0.8)\n",
        "\n",
        "        # Plot 3: Training Accuracy\n",
        "        axes[0, 2].plot(epochs, history['accuracy'],\n",
        "                        color=color, linewidth=2,\n",
        "                        label=f'{model_name} ({config[\"optimizer\"].__name__})',\n",
        "                        alpha=0.8)\n",
        "\n",
        "        # Plot 4: Validation Accuracy\n",
        "        axes[1, 0].plot(epochs, history['val_accuracy'],\n",
        "                        color=color, linewidth=2,\n",
        "                        label=f'{model_name} ({config[\"optimizer\"].__name__})',\n",
        "                        alpha=0.8)\n",
        "\n",
        "        # Plot 5: Loss Difference (Training - Validation) - Overfitting Detection\n",
        "        loss_diff = [t - v for t, v in zip(history['loss'], history['val_loss'])]\n",
        "        axes[1, 1].plot(epochs, loss_diff,\n",
        "                        color=color, linewidth=2,\n",
        "                        label=f'{model_name} ({config[\"optimizer\"].__name__})',\n",
        "                        alpha=0.8)\n",
        "\n",
        "        # Plot 6: Accuracy Difference (Training - Validation) - Overfitting Detection\n",
        "        acc_diff = [t - v for t, v in zip(history['accuracy'], history['val_accuracy'])]\n",
        "        axes[1, 2].plot(epochs, acc_diff,\n",
        "                        color=color, linewidth=2,\n",
        "                        label=f'{model_name} ({config[\"optimizer\"].__name__})',\n",
        "                        alpha=0.8)\n",
        "\n",
        "    # Customize subplots\n",
        "    # Plot 1: Training Loss\n",
        "    axes[0, 0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epochs', fontsize=12)\n",
        "    axes[0, 0].set_ylabel('Training Loss', fontsize=12)\n",
        "    axes[0, 0].legend(fontsize=10)\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    axes[0, 0].set_yscale('log')  # Log scale for better visualization\n",
        "\n",
        "    # Plot 2: Validation Loss\n",
        "    axes[0, 1].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epochs', fontsize=12)\n",
        "    axes[0, 1].set_ylabel('Validation Loss', fontsize=12)\n",
        "    axes[0, 1].legend(fontsize=10)\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    axes[0, 1].set_yscale('log')  # Log scale for better visualization\n",
        "\n",
        "    # Plot 3: Training Accuracy\n",
        "    axes[0, 2].set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[0, 2].set_xlabel('Epochs', fontsize=12)\n",
        "    axes[0, 2].set_ylabel('Training Accuracy', fontsize=12)\n",
        "    axes[0, 2].legend(fontsize=10)\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Validation Accuracy\n",
        "    axes[1, 0].set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Epochs', fontsize=12)\n",
        "    axes[1, 0].set_ylabel('Validation Accuracy', fontsize=12)\n",
        "    axes[1, 0].legend(fontsize=10)\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 5: Loss Difference (Overfitting Detection)\n",
        "    axes[1, 1].set_title('Overfitting Detection: Loss Difference\\n(Training - Validation)', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Epochs', fontsize=12)\n",
        "    axes[1, 1].set_ylabel('Loss Difference', fontsize=12)\n",
        "    axes[1, 1].legend(fontsize=10)\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Plot 6: Accuracy Difference (Overfitting Detection)\n",
        "    axes[1, 2].set_title('Overfitting Detection: Accuracy Difference\\n(Training - Validation)', fontsize=14, fontweight='bold')\n",
        "    axes[1, 2].set_xlabel('Epochs', fontsize=12)\n",
        "    axes[1, 2].set_ylabel('Accuracy Difference', fontsize=12)\n",
        "    axes[1, 2].legend(fontsize=10)\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "    axes[1, 2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/comprehensive_training_analysis_real.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Print training analysis summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" TRAINING ANALYSIS SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for i, (history, config) in enumerate(zip(training_histories, ENSEMBLE_CONFIGS)):\n",
        "        model_name = config['name']\n",
        "        print(f\"\\n{model_name.upper()}:\")\n",
        "        print(f\"  Final Training Loss: {history['loss'][-1]:.4f}\")\n",
        "        print(f\"  Final Validation Loss: {history['val_loss'][-1]:.4f}\")\n",
        "        print(f\"  Final Training Accuracy: {history['accuracy'][-1]:.4f}\")\n",
        "        print(f\"  Final Validation Accuracy: {history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "        # Overfitting analysis\n",
        "        loss_overfitting = history['loss'][-1] - history['val_loss'][-1]\n",
        "        acc_overfitting = history['accuracy'][-1] - history['val_accuracy'][-1]\n",
        "\n",
        "        print(f\"  Loss Overfitting: {loss_overfitting:+.4f} {'(Overfitting)' if loss_overfitting > 0 else '(Good)'}\")\n",
        "        print(f\"  Accuracy Overfitting: {acc_overfitting:+.4f} {'(Overfitting)' if acc_overfitting > 0 else '(Good)'}\")\n",
        "\n",
        "        # Best epoch analysis\n",
        "        best_val_loss_epoch = np.argmin(history['val_loss']) + 1\n",
        "        best_val_acc_epoch = np.argmax(history['val_accuracy']) + 1\n",
        "\n",
        "        print(f\"  Best Validation Loss at Epoch: {best_val_loss_epoch}\")\n",
        "        print(f\"  Best Validation Accuracy at Epoch: {best_val_acc_epoch}\")\n",
        "\n",
        "    print(f\"\\nVisualization saved to: /content/comprehensive_training_analysis_real.png\")\n",
        "\n",
        "# Now run the plotting function\n",
        "create_real_training_plots()"
      ],
      "metadata": {
        "id": "4aijHhPCjf63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Enhanced Statistical Significance Analysis**"
      ],
      "metadata": {
        "id": "z38uUH_pMpr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **ENHANCED STATISTICAL SIGNIFICANCE ANALYSIS + PLOTS**\n",
        "\n",
        "def enhanced_statistical_analysis():\n",
        "    \"\"\"Enhanced statistical analysis with proper handling of edge cases and visualizations\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" ENHANCED STATISTICAL SIGNIFICANCE ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    from scipy import stats\n",
        "    import numpy as np\n",
        "\n",
        "    # Extract metrics from bootstrap results\n",
        "    metrics_data = {\n",
        "        'Accuracy': {\n",
        "            'Adam': [0.8623, 0.0209],      # [mean, ci_width]\n",
        "            'SGD': [0.8543, 0.0228],\n",
        "            'RMSprop': [0.8955, 0.0204]\n",
        "        },\n",
        "        'F1_Score': {\n",
        "            'Adam': [0.8516, 0.0262],\n",
        "            'SGD': [0.8421, 0.0266],\n",
        "            'RMSprop': [0.8764, 0.0247]\n",
        "        },\n",
        "        'AUC': {\n",
        "            'Adam': [0.9522, 0.0125],\n",
        "            'SGD': [0.9396, 0.0147],\n",
        "            'RMSprop': [0.9582, 0.0121]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create comprehensive visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Stage 1: Statistical Significance Analysis for Binary Classification', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Plot 1: Performance Comparison Bar Chart\n",
        "    ax1 = axes[0, 0]\n",
        "    model_names = list(metrics_data['Accuracy'].keys())\n",
        "    accuracies = [metrics_data['Accuracy'][name][0] for name in model_names]\n",
        "    f1_scores = [metrics_data['F1_Score'][name][0] for name in model_names]\n",
        "\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.35\n",
        "\n",
        "    bars1 = ax1.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8, color='skyblue')\n",
        "    bars2 = ax1.bar(x + width/2, f1_scores, width, label='F1-Score', alpha=0.8, color='lightcoral')\n",
        "\n",
        "    ax1.set_xlabel('Optimizer')\n",
        "    ax1.set_ylabel('Score')\n",
        "    ax1.set_title('Model Performance Comparison')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(model_names)\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.3f}',\n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "    for bar in bars2:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.3f}',\n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 2: Confidence Intervals\n",
        "    ax2 = axes[0, 1]\n",
        "    for i, metric in enumerate(['Accuracy', 'F1_Score', 'AUC']):\n",
        "        means = [metrics_data[metric][name][0] for name in model_names]\n",
        "        ci_widths = [metrics_data[metric][name][1] for name in model_names]\n",
        "\n",
        "        x_pos = np.arange(len(model_names)) + i * 0.25\n",
        "        ax2.errorbar(x_pos, means, yerr=[w/2 for w in ci_widths], fmt='o',\n",
        "                    label=metric, capsize=5, capthick=2, markersize=8)\n",
        "\n",
        "    ax2.set_xlabel('Optimizer')\n",
        "    ax2.set_ylabel('Score')\n",
        "    ax2.set_title('Performance with 95% Confidence Intervals')\n",
        "    ax2.set_xticks(np.arange(len(model_names)) + 0.25)\n",
        "    ax2.set_xticklabels(model_names)\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Performance Heatmap\n",
        "    ax3 = axes[1, 0]\n",
        "    metrics_matrix = np.array([\n",
        "        [metrics_data['Accuracy'][name][0] for name in model_names],\n",
        "        [metrics_data['F1_Score'][name][0] for name in model_names],\n",
        "        [metrics_data['AUC'][name][0] for name in model_names]\n",
        "    ])\n",
        "\n",
        "    im = ax3.imshow(metrics_matrix, cmap='RdYlGn', aspect='auto')\n",
        "    ax3.set_xticks(range(len(model_names)))\n",
        "    ax3.set_yticks(range(3))\n",
        "    ax3.set_xticklabels(model_names)\n",
        "    ax3.set_yticklabels(['Accuracy', 'F1-Score', 'AUC'])\n",
        "    ax3.set_title('Performance Heatmap')\n",
        "\n",
        "    # Add text annotations\n",
        "    for i in range(3):\n",
        "        for j in range(len(model_names)):\n",
        "            text = ax3.text(j, i, f'{metrics_matrix[i, j]:.3f}',\n",
        "                           ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "\n",
        "    plt.colorbar(im, ax=ax3, shrink=0.8)\n",
        "\n",
        "    # Plot 4: Statistical Summary\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.axis('off')\n",
        "\n",
        "    # Calculate statistics\n",
        "    summary_text = \"STATISTICAL SUMMARY:\\n\\n\"\n",
        "    for metric_name, model_data in metrics_data.items():\n",
        "        values = [data[0] for data in model_data.values()]\n",
        "        mean_val = np.mean(values)\n",
        "        std_val = np.std(values)\n",
        "        range_val = np.max(values) - np.min(values)\n",
        "\n",
        "        summary_text += f\"{metric_name}:\\n\"\n",
        "        summary_text += f\"  Mean: {mean_val:.4f}\\n\"\n",
        "        summary_text += f\"  Std Dev: {std_val:.4f}\\n\"\n",
        "        summary_text += f\"  Range: {range_val:.4f}\\n\\n\"\n",
        "\n",
        "    summary_text += \"MODEL RANKING:\\n\"\n",
        "    f1_scores = [(name, data[0]) for name, data in metrics_data['F1_Score'].items()]\n",
        "    f1_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for i, (model_name, f1_score) in enumerate(f1_scores):\n",
        "        summary_text += f\"{i+1}. {model_name}: {f1_score:.4f}\\n\"\n",
        "\n",
        "    ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, fontsize=10,\n",
        "             verticalalignment='top', fontfamily='monospace', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/plantwild_stage1_models/stage1_statistical_analysis.png',\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Print analysis results\n",
        "    for metric_name, model_data in metrics_data.items():\n",
        "        print(f\"\\n{metric_name.upper()} ANALYSIS:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Extract values and calculate statistics\n",
        "        values = [data[0] for data in model_data.values()]\n",
        "        model_names = list(model_data.keys())\n",
        "\n",
        "        print(f\"Model Performance:\")\n",
        "        for name, (value, ci_width) in model_data.items():\n",
        "            ci_lower = value - ci_width/2\n",
        "            ci_upper = value + ci_width/2\n",
        "            print(f\"  {name}: {value:.4f} [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
        "\n",
        "        # Basic statistics\n",
        "        mean_val = np.mean(values)\n",
        "        std_val = np.std(values)\n",
        "        range_val = np.max(values) - np.min(values)\n",
        "\n",
        "        print(f\"\\nStatistics:\")\n",
        "        print(f\"  Mean: {mean_val:.4f}\")\n",
        "        print(f\"  Standard Deviation: {std_val:.4f}\")\n",
        "        print(f\"  Range: {range_val:.4f}\")\n",
        "\n",
        "        # Check if values are identical (which causes ANOVA to fail)\n",
        "        if np.allclose(values, values[0], rtol=1e-10):\n",
        "            print(f\"  All values are identical - ANOVA not applicable\")\n",
        "            print(f\"   No statistical difference (all models perform identically)\")\n",
        "        else:\n",
        "            # Perform ANOVA\n",
        "            try:\n",
        "                f_stat, p_value = stats.f_oneway(*values)\n",
        "                print(f\"  ANOVA F-statistic: {f_stat:.4f}\")\n",
        "                print(f\"  P-value: {p_value:.6f}\")\n",
        "                print(f\"  Significant difference: {'YES' if p_value < 0.05 else 'NO'}\")\n",
        "\n",
        "                if p_value < 0.05:\n",
        "                    print(\"  Model performances are significantly different!\")\n",
        "                else:\n",
        "                    print(\"  No significant difference between models\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ANOVA failed: {e}\")\n",
        "\n",
        "        # Effect size analysis (Cohen's d)\n",
        "        print(f\"\\nEffect Size Analysis:\")\n",
        "        if len(values) >= 2:\n",
        "            # Compare best vs worst\n",
        "            best_idx = np.argmax(values)\n",
        "            worst_idx = np.argmin(values)\n",
        "            best_val = values[best_idx]\n",
        "            worst_val = values[worst_idx]\n",
        "\n",
        "            # Pooled standard deviation\n",
        "            pooled_std = np.sqrt((std_val**2 + std_val**2) / 2)\n",
        "            cohens_d = (best_val - worst_val) / pooled_std\n",
        "\n",
        "            print(f\"  Best vs Worst: {model_names[best_idx]} vs {model_names[worst_idx]}\")\n",
        "            print(f\"  Cohen's d: {cohens_d:.4f}\")\n",
        "\n",
        "            # Interpret effect size\n",
        "            if abs(cohens_d) < 0.2:\n",
        "                effect_size = \"Negligible\"\n",
        "            elif abs(cohens_d) < 0.5:\n",
        "                effect_size = \"Small\"\n",
        "            elif abs(cohens_d) < 0.8:\n",
        "                effect_size = \"Medium\"\n",
        "            else:\n",
        "                effect_size = \"Large\"\n",
        "\n",
        "            print(f\"  Effect Size: {effect_size}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "    # Overall model ranking with statistical significance\n",
        "    print(\"OVERALL MODEL RANKING WITH STATISTICAL ANALYSIS:\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Rank by F1-score\n",
        "    f1_scores = [(name, data[0]) for name, data in metrics_data['F1_Score'].items()]\n",
        "    f1_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for i, (model_name, f1_score) in enumerate(f1_scores):\n",
        "        print(f\"{i+1}. {model_name}: F1-Score = {f1_score:.4f}\")\n",
        "\n",
        "        # Get other metrics for this model\n",
        "        acc = metrics_data['Accuracy'][model_name][0]\n",
        "        auc = metrics_data['AUC'][model_name][0]\n",
        "        print(f\"     Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
        "\n",
        "        # Statistical significance compared to best\n",
        "        if i == 0:\n",
        "            print(f\"      BEST PERFORMING MODEL\")\n",
        "        else:\n",
        "            best_f1 = f1_scores[0][1]\n",
        "            diff = best_f1 - f1_score\n",
        "            diff_percent = (diff / best_f1) * 100\n",
        "\n",
        "            print(f\"      {diff_percent:.1f}% lower than best model\")\n",
        "\n",
        "            # Check if difference is statistically significant\n",
        "            if diff_percent > 2.0:  # More than 2% difference\n",
        "                print(f\"      Statistically meaningful difference\")\n",
        "            else:\n",
        "                print(f\"      Difference may not be practically significant\")\n",
        "\n",
        "    # Practical implications\n",
        "    print(f\"\\nPRACTICAL IMPLICATIONS FOR PRECISION AGRICULTURE:\")\n",
        "    print(\"-\" * 50)\n",
        "    best_model = f1_scores[0][0]\n",
        "    best_f1 = f1_scores[0][1]\n",
        "\n",
        "    print(f\" {best_model} emerges as the optimal choice for deployment\")\n",
        "    print(f\" All models achieve >84% F1-score, suitable for agricultural use\")\n",
        "    print(f\" Performance differences are small, suggesting robustness across optimizers\")\n",
        "    print(f\" Model selection can be based on computational efficiency or deployment constraints\")\n",
        "\n",
        "# Run enhanced statistical analysis\n",
        "enhanced_statistical_analysis()"
      ],
      "metadata": {
        "id": "sdL5PPlQPyqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Confidence Intervals and Bootstrap Analysis**"
      ],
      "metadata": {
        "id": "ws3i2JrONEcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **CONFIDENCE INTERVALS AND BOOTSTRAP ANALYSIS + PLOTS (FIXED)**\n",
        "\n",
        "def bootstrap_confidence_intervals(n_bootstrap=1000, confidence=0.95):\n",
        "    \"\"\"Calculate bootstrap confidence intervals for dissertation rigor with comprehensive visualizations\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" BOOTSTRAP CONFIDENCE INTERVALS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    import numpy as np\n",
        "    from scipy import stats\n",
        "\n",
        "    # Store results for plotting\n",
        "    all_results = {}\n",
        "\n",
        "    for result in evaluation_results:\n",
        "        model_name = result['model_name']\n",
        "        predictions = result['predictions']\n",
        "        true_labels = result['true_labels']\n",
        "\n",
        "        print(f\"\\n{model_name.upper()}:\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Bootstrap confidence intervals\n",
        "        bootstrap_metrics = {\n",
        "            'accuracy': [], 'precision': [], 'recall': [],\n",
        "            'f1_score': [], 'auc': []\n",
        "        }\n",
        "\n",
        "        n_samples = len(true_labels)\n",
        "\n",
        "        for _ in range(n_bootstrap):\n",
        "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
        "            bootstrap_pred = predictions[indices]\n",
        "            bootstrap_true = true_labels[indices]\n",
        "\n",
        "            pred_binary = (bootstrap_pred > 0.5).astype(int)\n",
        "\n",
        "            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "            try:\n",
        "                bootstrap_metrics['accuracy'].append(accuracy_score(bootstrap_true, pred_binary))\n",
        "                bootstrap_metrics['precision'].append(precision_score(bootstrap_true, pred_binary))\n",
        "                bootstrap_metrics['recall'].append(recall_score(bootstrap_true, pred_binary))\n",
        "                bootstrap_metrics['f1_score'].append(f1_score(bootstrap_true, pred_binary))\n",
        "                bootstrap_metrics['auc'].append(roc_auc_score(bootstrap_true, bootstrap_pred))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Calculate confidence intervals\n",
        "        alpha = 1 - confidence\n",
        "        model_results = {}\n",
        "\n",
        "        for metric, values in bootstrap_metrics.items():\n",
        "            if values:\n",
        "                ci_lower = np.percentile(values, alpha/2 * 100)\n",
        "                ci_upper = np.percentile(values, (1-alpha/2) * 100)\n",
        "                mean_val = np.mean(values)\n",
        "                std_val = np.std(values)\n",
        "\n",
        "                print(f\"  {metric.replace('_', ' ').title()}:\")\n",
        "                print(f\"    Mean: {mean_val:.4f}\")\n",
        "                print(f\"    {confidence*100}% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
        "                print(f\"    Width: {ci_upper - ci_lower:.4f}\")\n",
        "\n",
        "                # Store for plotting\n",
        "                model_results[metric] = {\n",
        "                    'mean': mean_val,\n",
        "                    'ci_lower': ci_lower,\n",
        "                    'ci_upper': ci_upper,\n",
        "                    'std': std_val,\n",
        "                    'values': values\n",
        "                }\n",
        "\n",
        "        all_results[model_name] = model_results\n",
        "\n",
        "    print(f\"\\nBootstrap analysis completed with {n_bootstrap} iterations\")\n",
        "    print(f\"Confidence level: {confidence*100}%\")\n",
        "\n",
        "    # Create comprehensive visualizations\n",
        "    create_bootstrap_visualizations(all_results, confidence, n_bootstrap)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def create_bootstrap_visualizations(all_results, confidence, n_bootstrap):\n",
        "    \"\"\"Create comprehensive bootstrap analysis visualizations\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" CREATING BOOTSTRAP ANALYSIS VISUALIZATIONS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Set up the plotting style\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "\n",
        "    # Create a comprehensive figure\n",
        "    fig = plt.figure(figsize=(20, 16))\n",
        "    fig.suptitle(f'Stage 1: Bootstrap Confidence Intervals Analysis\\n{n_bootstrap} iterations, {confidence*100}% confidence level',\n",
        "                 fontsize=18, fontweight='bold')\n",
        "\n",
        "    # Define colors for each model\n",
        "    colors = {'model_adam': 'skyblue', 'model_sgd': 'lightcoral', 'model_rmsprop': 'lightgreen'}\n",
        "    model_names_clean = {'model_adam': 'Adam', 'model_sgd': 'SGD', 'model_rmsprop': 'RMSprop'}\n",
        "\n",
        "    # Plot 1: Confidence Intervals Comparison\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']\n",
        "    metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
        "\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.25\n",
        "\n",
        "    for i, (model_name, model_data) in enumerate(all_results.items()):\n",
        "        # Convert to numpy arrays for proper mathematical operations\n",
        "        means = np.array([model_data[metric]['mean'] for metric in metrics])\n",
        "        ci_lowers = np.array([model_data[metric]['ci_lower'] for metric in metrics])\n",
        "        ci_uppers = np.array([model_data[metric]['ci_upper'] for metric in metrics])\n",
        "\n",
        "        x_pos = x + i * width\n",
        "        bars = ax1.bar(x_pos, means, width, label=model_names_clean[model_name],\n",
        "                      color=colors[model_name], alpha=0.8)\n",
        "\n",
        "        # Add error bars for confidence intervals\n",
        "        ax1.errorbar(x_pos, means, yerr=[means - ci_lowers, ci_uppers - means],\n",
        "                    fmt='none', color='black', capsize=5, capthick=1)\n",
        "\n",
        "    ax1.set_xlabel('Metrics')\n",
        "    ax1.set_ylabel('Score')\n",
        "    ax1.set_title('Performance Metrics with Confidence Intervals')\n",
        "    ax1.set_xticks(x + width)\n",
        "    ax1.set_xticklabels(metric_labels, rotation=45)\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim(0, 1)\n",
        "\n",
        "    # Plot 2: Bootstrap Distribution for F1-Score\n",
        "    ax2 = plt.subplot(3, 3, 2)\n",
        "    for model_name, model_data in all_results.items():\n",
        "        f1_values = model_data['f1_score']['values']\n",
        "        ax2.hist(f1_values, bins=30, alpha=0.7, label=model_names_clean[model_name],\n",
        "                color=colors[model_name], density=True)\n",
        "\n",
        "        # Add vertical line for mean\n",
        "        mean_f1 = model_data['f1_score']['mean']\n",
        "        ax2.axvline(mean_f1, color=colors[model_name], linestyle='--', linewidth=2,\n",
        "                    label=f'{model_names_clean[model_name]} Mean: {mean_f1:.3f}')\n",
        "\n",
        "    ax2.set_xlabel('F1-Score')\n",
        "    ax2.set_ylabel('Density')\n",
        "    ax2.set_title('Bootstrap Distribution of F1-Scores')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Bootstrap Distribution for Accuracy\n",
        "    ax3 = plt.subplot(3, 3, 3)\n",
        "    for model_name, model_data in all_results.items():\n",
        "        acc_values = model_data['accuracy']['values']\n",
        "        ax3.hist(acc_values, bins=30, alpha=0.7, label=model_names_clean[model_name],\n",
        "                color=colors[model_name], density=True)\n",
        "\n",
        "        # Add vertical line for mean\n",
        "        mean_acc = model_data['accuracy']['mean']\n",
        "        ax3.axvline(mean_acc, color=colors[model_name], linestyle='--', linewidth=2,\n",
        "                    label=f'{model_names_clean[model_name]} Mean: {mean_acc:.3f}')\n",
        "\n",
        "    ax3.set_xlabel('Accuracy')\n",
        "    ax3.set_ylabel('Density')\n",
        "    ax3.set_title('Bootstrap Distribution of Accuracy')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Confidence Interval Widths\n",
        "    ax4 = plt.subplot(3, 3, 4)\n",
        "    ci_widths = {}\n",
        "    for model_name, model_data in all_results.items():\n",
        "        ci_widths[model_name] = [model_data[metric]['ci_upper'] - model_data[metric]['ci_lower']\n",
        "                                for metric in metrics]\n",
        "\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.25\n",
        "\n",
        "    for i, (model_name, widths) in enumerate(ci_widths.items()):\n",
        "        x_pos = x + i * width\n",
        "        bars = ax4.bar(x_pos, widths, width, label=model_names_clean[model_name],\n",
        "                      color=colors[model_name], alpha=0.8)\n",
        "\n",
        "    ax4.set_xlabel('Metrics')\n",
        "    ax4.set_ylabel('CI Width (Lower is Better)')\n",
        "    ax4.set_title('Confidence Interval Widths')\n",
        "    ax4.set_xticks(x + width)\n",
        "    ax4.set_xticklabels(metric_labels, rotation=45)\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 5: Performance Heatmap\n",
        "    ax5 = plt.subplot(3, 3, 5)\n",
        "    performance_matrix = np.array([[all_results[model][metric]['mean']\n",
        "                                  for metric in metrics]\n",
        "                                 for model in all_results.keys()])\n",
        "\n",
        "    im = ax5.imshow(performance_matrix, cmap='RdYlGn', aspect='auto')\n",
        "    ax5.set_xticks(range(len(metrics)))\n",
        "    ax5.set_yticks(range(len(all_results)))\n",
        "    ax5.set_xticklabels(metric_labels, rotation=45)\n",
        "    ax5.set_yticklabels([model_names_clean[name] for name in all_results.keys()])\n",
        "    ax5.set_title('Performance Heatmap')\n",
        "\n",
        "    # Add text annotations\n",
        "    for i in range(len(all_results)):\n",
        "        for j in range(len(metrics)):\n",
        "            text = ax5.text(j, i, f'{performance_matrix[i, j]:.3f}',\n",
        "                           ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "\n",
        "    plt.colorbar(im, ax=ax5, shrink=0.8)\n",
        "\n",
        "    # Plot 6: Model Ranking by F1-Score\n",
        "    ax6 = plt.subplot(3, 3, 6)\n",
        "    f1_means = [(model_name, all_results[model_name]['f1_score']['mean'])\n",
        "                for model_name in all_results.keys()]\n",
        "    f1_means.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    model_names_ranked = [model_names_clean[name] for name, _ in f1_means]\n",
        "    f1_values_ranked = [f1 for _, f1 in f1_means]\n",
        "\n",
        "    bars = ax6.bar(model_names_ranked, f1_values_ranked, color=[colors[name] for name, _ in f1_means], alpha=0.8)\n",
        "    ax6.set_ylabel('F1-Score')\n",
        "    ax6.set_title('Model Ranking by F1-Score')\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax6.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.3f}',\n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 7: Bootstrap Stability Analysis\n",
        "    ax7 = plt.subplot(3, 3, 7)\n",
        "    stability_metrics = {}\n",
        "    for model_name, model_data in all_results.items():\n",
        "        stability_metrics[model_name] = [model_data[metric]['std'] for metric in metrics]\n",
        "\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.25\n",
        "\n",
        "    for i, (model_name, stds) in enumerate(stability_metrics.items()):\n",
        "        x_pos = x + i * width\n",
        "        bars = ax7.bar(x_pos, stds, width, label=model_names_clean[model_name],\n",
        "                      color=colors[model_name], alpha=0.8)\n",
        "\n",
        "    ax7.set_xlabel('Metrics')\n",
        "    ax7.set_ylabel('Standard Deviation (Lower is Better)')\n",
        "    ax7.set_title('Bootstrap Stability Analysis')\n",
        "    ax7.set_xticks(x + width)\n",
        "    ax7.set_xticklabels(metric_labels, rotation=45)\n",
        "    ax7.legend()\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 8: Confidence Interval Coverage\n",
        "    ax8 = plt.subplot(3, 3, 8)\n",
        "    coverage_data = {}\n",
        "    for model_name, model_data in all_results.items():\n",
        "        coverage_data[model_name] = []\n",
        "        for metric in metrics:\n",
        "            ci_width = model_data[metric]['ci_upper'] - model_data[metric]['ci_lower']\n",
        "            mean_val = model_data[metric]['mean']\n",
        "            coverage = ci_width / mean_val if mean_val > 0 else 0\n",
        "            coverage_data[model_name].append(coverage)\n",
        "\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.25\n",
        "\n",
        "    for i, (model_name, coverages) in enumerate(coverage_data.items()):\n",
        "        x_pos = x + i * width\n",
        "        bars = ax8.bar(x_pos, coverages, width, label=model_names_clean[model_name],\n",
        "                      color=colors[model_name], alpha=0.8)\n",
        "\n",
        "    ax8.set_xlabel('Metrics')\n",
        "    ax8.set_ylabel('CI Width / Mean (Lower is Better)')\n",
        "    ax8.set_title('Confidence Interval Coverage')\n",
        "    ax8.set_xticks(x + width)\n",
        "    ax8.set_xticklabels(metric_labels, rotation=45)\n",
        "    ax8.legend()\n",
        "    ax8.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 9: Statistical Summary\n",
        "    ax9 = plt.subplot(3, 3, 9)\n",
        "    ax9.axis('off')\n",
        "\n",
        "    summary_text = \"BOOTSTRAP ANALYSIS SUMMARY:\\n\\n\"\n",
        "    summary_text += f\"Iterations: {n_bootstrap}\\n\"\n",
        "    summary_text += f\"Confidence Level: {confidence*100}%\\n\\n\"\n",
        "\n",
        "    # Best model by F1-score\n",
        "    best_model_name = f1_means[0][0]\n",
        "    best_f1 = f1_means[0][1]\n",
        "    summary_text += f\"BEST MODEL: {model_names_clean[best_model_name]}\\n\"\n",
        "    summary_text += f\"F1-Score: {best_f1:.4f}\\n\\n\"\n",
        "\n",
        "    # Model rankings\n",
        "    summary_text += \"MODEL RANKINGS:\\n\"\n",
        "    for i, (model_name, f1_score) in enumerate(f1_means):\n",
        "        summary_text += f\"{i+1}. {model_name}: {f1_score:.4f}\\n\"\n",
        "\n",
        "    summary_text += f\"\\nAnalysis completed successfully!\"\n",
        "\n",
        "    ax9.text(0.05, 0.95, summary_text, transform=ax9.transAxes, fontsize=10,\n",
        "             verticalalignment='top', fontfamily='monospace',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the comprehensive visualization\n",
        "    save_path = '/content/drive/MyDrive/plantwild_stage1_models/stage1_bootstrap_analysis_comprehensive.png'\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\" Comprehensive bootstrap visualization saved to: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Run enhanced bootstrap analysis with plots\n",
        "bootstrap_results = bootstrap_confidence_intervals()"
      ],
      "metadata": {
        "id": "QbZpagCeV8mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cross-Validation Analysis**"
      ],
      "metadata": {
        "id": "qLabMuTSNqyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **RUN FULL CROSS-VALIDATION WITH FIXED LABELS + PLOTS**\n",
        "\n",
        "def run_full_cross_validation_fixed():\n",
        "    \"\"\"Run full cross-validation with the fixed label encoding and visualizations\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" RUNNING FULL CROSS-VALIDATION WITH FIXED LABELS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if 'best_model' not in globals():\n",
        "        print(\" best_model not available\")\n",
        "        return\n",
        "\n",
        "    from sklearn.model_selection import KFold\n",
        "    import numpy as np\n",
        "\n",
        "    # Prepare data with CORRECT labels (inverted from original)\n",
        "    test_df = df[df['split'] == 'test']\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    print(f\"Preparing {len(test_df)} test samples with CORRECT labels...\")\n",
        "\n",
        "    for _, sample in test_df.iterrows():\n",
        "        try:\n",
        "            img = tf.keras.preprocessing.image.load_img(sample['image_path'], target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "            img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "            img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
        "            X.append(img_array)\n",
        "\n",
        "            # CORRECT LABELS: 0=diseased, 1=healthy (inverted from original)\n",
        "            binary_label = 0 if sample['binary_label'] == 'diseased' else 1\n",
        "            y.append(binary_label)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {sample['image_path']}: {e}\")\n",
        "            continue\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    print(f\"Data prepared: X shape {X.shape}, y shape {y.shape}\")\n",
        "    print(f\"Label distribution: Diseased={np.sum(y==0)}, Healthy={np.sum(y==1)}\")\n",
        "\n",
        "    # K-fold cross-validation\n",
        "    k_folds = 5\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    cv_scores = []\n",
        "\n",
        "    print(f\"\\nPerforming {k_folds}-fold cross-validation with CORRECT labels...\")\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
        "\n",
        "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Evaluate on validation fold\n",
        "        predictions = best_model.predict(X_val_fold, verbose=0)\n",
        "        pred_binary = (predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "        # Calculate metrics\n",
        "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "        accuracy = accuracy_score(y_val_fold, pred_binary)\n",
        "        f1 = f1_score(y_val_fold, pred_binary)\n",
        "        precision = precision_score(y_val_fold, pred_binary)\n",
        "        recall = recall_score(y_val_fold, pred_binary)\n",
        "\n",
        "        cv_scores.append({\n",
        "            'accuracy': accuracy,\n",
        "            'f1_score': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        })\n",
        "\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"  F1-Score: {f1:.4f}\")\n",
        "        print(f\"  Precision: {precision:.4f}\")\n",
        "        print(f\"  Recall: {recall:.4f}\")\n",
        "\n",
        "    # Cross-validation summary\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\" CROSS-VALIDATION SUMMARY (FIXED LABELS)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    accuracies = [score['accuracy'] for score in cv_scores]\n",
        "    f1_scores = [score['f1_score'] for score in cv_scores]\n",
        "    precisions = [score['precision'] for score in cv_scores]\n",
        "    recalls = [score['recall'] for score in cv_scores]\n",
        "\n",
        "    print(f\"Accuracy: {np.mean(accuracies):.4f}  {np.std(accuracies):.4f}\")\n",
        "    print(f\"F1-Score: {np.mean(f1_scores):.4f}  {np.std(f1_scores):.4f}\")\n",
        "    print(f\"Precision: {np.mean(precisions):.4f}  {np.std(precisions):.4f}\")\n",
        "    print(f\"Recall: {np.mean(recalls):.4f}  {np.std(recalls):.4f}\")\n",
        "\n",
        "    # Stability assessment\n",
        "    stability = 'Good' if np.std(accuracies) < 0.05 else 'Moderate' if np.std(accuracies) < 0.1 else 'Poor'\n",
        "    print(f\"Stability: {stability}\")\n",
        "\n",
        "    # Create comprehensive visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Stage 1: Cross-Validation Analysis with Fixed Labels', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Plot 1: Fold-wise Performance\n",
        "    ax1 = axes[0, 0]\n",
        "    fold_numbers = range(1, k_folds + 1)\n",
        "    x = np.arange(len(fold_numbers))\n",
        "    width = 0.2\n",
        "\n",
        "    bars1 = ax1.bar(x - 1.5*width, accuracies, width, label='Accuracy', alpha=0.8, color='skyblue')\n",
        "    bars2 = ax1.bar(x - 0.5*width, f1_scores, width, label='F1-Score', alpha=0.8, color='lightcoral')\n",
        "    bars3 = ax1.bar(x + 0.5*width, precisions, width, label='Precision', alpha=0.8, color='lightgreen')\n",
        "    bars4 = ax1.bar(x + 1.5*width, recalls, width, label='Recall', alpha=0.8, color='gold')\n",
        "\n",
        "    ax1.set_xlabel('Fold Number')\n",
        "    ax1.set_ylabel('Score')\n",
        "    ax1.set_title('Performance Across Folds')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(fold_numbers)\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim(0, 1)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bars in [bars1, bars2, bars3, bars4]:\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.3f}',\n",
        "                    ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "    # Plot 2: Performance Distribution\n",
        "    ax2 = axes[0, 1]\n",
        "    metrics_data = [accuracies, f1_scores, precisions, recalls]\n",
        "    metric_names = ['Accuracy', 'F1-Score', 'Precision', 'Recall']\n",
        "    colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']\n",
        "\n",
        "    bp = ax2.boxplot(metrics_data, labels=metric_names, patch_artist=True)\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "    ax2.set_ylabel('Score')\n",
        "    ax2.set_title('Performance Distribution Across Folds')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_ylim(0, 1)\n",
        "\n",
        "    # Plot 3: Stability Analysis\n",
        "    ax3 = axes[1, 0]\n",
        "    metrics_std = [np.std(metric) for metric in metrics_data]\n",
        "    bars = ax3.bar(metric_names, metrics_std, color=colors, alpha=0.7)\n",
        "    ax3.set_ylabel('Standard Deviation')\n",
        "    ax3.set_title('Performance Stability (Lower is Better)')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.001, f'{height:.4f}',\n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 4: Summary Statistics\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.axis('off')\n",
        "\n",
        "    summary_text = \"CROSS-VALIDATION SUMMARY:\\n\\n\"\n",
        "    summary_text += f\"Folds: {k_folds}\\n\"\n",
        "    summary_text += f\"Test Samples: {len(X)}\\n\\n\"\n",
        "\n",
        "    summary_text += \"MEAN  STD:\\n\"\n",
        "    summary_text += f\"Accuracy: {np.mean(accuracies):.4f}  {np.std(accuracies):.4f}\\n\"\n",
        "    summary_text += f\"F1-Score: {np.mean(f1_scores):.4f}  {np.std(f1_scores):.4f}\\n\"\n",
        "    summary_text += f\"Precision: {np.mean(precisions):.4f}  {np.std(precisions):.4f}\\n\"\n",
        "    summary_text += f\"Recall: {np.mean(recalls):.4f}  {np.std(recalls):.4f}\\n\\n\"\n",
        "\n",
        "    summary_text += f\"Stability: {stability}\\n\"\n",
        "    summary_text += f\"CV Score: {np.mean(f1_scores):.4f}\\n\"\n",
        "\n",
        "    ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, fontsize=10,\n",
        "             verticalalignment='top', fontfamily='monospace', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/plantwild_stage1_models/stage1_cross_validation_analysis.png',\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Compare with bootstrap results\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\" COMPARISON WITH BOOTSTRAP RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if 'training_summary' in globals() and training_summary:\n",
        "        best_model_name = training_summary.get('best_model_name', 'Unknown')\n",
        "        print(f\"Best model from training: {best_model_name}\")\n",
        "\n",
        "    # Get bootstrap results for comparison\n",
        "    if 'evaluation_results' in globals() and evaluation_results:\n",
        "        for result in evaluation_results:\n",
        "            if result['model_name'] == best_model_name:\n",
        "                bootstrap_f1 = result['f1_score']\n",
        "                bootstrap_acc = result['accuracy']\n",
        "                break\n",
        "        else:\n",
        "            bootstrap_f1 = 0.8764  # Default from your results\n",
        "            bootstrap_acc = 0.8955\n",
        "\n",
        "    print(f\"Bootstrap Results:\")\n",
        "    print(f\"  F1-Score: {bootstrap_f1:.4f}\")\n",
        "    print(f\"  Accuracy: {bootstrap_acc:.4f}\")\n",
        "\n",
        "    print(f\"\\nCross-Validation Results (Fixed):\")\n",
        "    print(f\"  F1-Score: {np.mean(f1_scores):.4f}\")\n",
        "    print(f\"  Accuracy: {np.mean(accuracies):.4f}\")\n",
        "\n",
        "    # Check consistency\n",
        "    f1_diff = abs(np.mean(f1_scores) - bootstrap_f1)\n",
        "    acc_diff = abs(np.mean(accuracies) - bootstrap_acc)\n",
        "\n",
        "    if f1_diff < 0.05 and acc_diff < 0.05:\n",
        "        print(f\"\\n EXCELLENT: Results are consistent with bootstrap analysis!\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n Results still differ from bootstrap analysis\")\n",
        "        print(f\"  F1-Score difference: {f1_diff:.4f}\")\n",
        "        print(f\"  Accuracy difference: {acc_diff:.4f}\")\n",
        "\n",
        "    return cv_scores\n",
        "\n",
        "# Run full cross-validation with fixed labels\n",
        "cv_results = run_full_cross_validation_fixed()"
      ],
      "metadata": {
        "id": "pyQ6NajIRshs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Error Analysis and Misclassification Study**"
      ],
      "metadata": {
        "id": "Y3HvCzJmQSgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **ERROR ANALYSIS AND MISCLASSIFICATION STUDY + PLOTS**\n",
        "\n",
        "def run_fixed_error_analysis():\n",
        "    \"\"\"Run error analysis with the corrected label encoding and visualizations\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\" RUNNING FIXED ERROR ANALYSIS WITH CORRECT LABELS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if 'best_model' not in globals():\n",
        "        print(\" best_model not available\")\n",
        "        return\n",
        "\n",
        "    # Get test predictions with CORRECT labels\n",
        "    test_df = df[df['split'] == 'test']\n",
        "    misclassifications = []\n",
        "    correct_predictions = []\n",
        "\n",
        "    print(f\"Analyzing {len(test_df)} test samples with CORRECT labels...\")\n",
        "\n",
        "    for _, sample in test_df.iterrows():\n",
        "        try:\n",
        "            img = tf.keras.preprocessing.image.load_img(sample['image_path'], target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "            img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "            img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
        "            img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "            pred_prob = best_model.predict(img_array, verbose=0)[0][0]\n",
        "\n",
        "            # CORRECT interpretation: 0=diseased, 1=healthy\n",
        "            pred_class = \"healthy\" if pred_prob > 0.5 else \"diseased\"\n",
        "            true_class = sample['binary_label']\n",
        "\n",
        "            if pred_class != true_class:\n",
        "                misclassifications.append({\n",
        "                    'image_path': sample['image_path'],\n",
        "                    'class_name': sample['class_name'],\n",
        "                    'true_label': true_class,\n",
        "                    'predicted_label': pred_class,\n",
        "                    'confidence': pred_prob if pred_class == \"healthy\" else (1 - pred_prob),\n",
        "                    'prediction_probability': pred_prob\n",
        "                })\n",
        "            else:\n",
        "                correct_predictions.append({\n",
        "                    'image_path': sample['image_path'],\n",
        "                    'class_name': sample['class_name'],\n",
        "                    'true_label': true_class,\n",
        "                    'confidence': pred_prob if pred_class == \"healthy\" else (1 - pred_prob)\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {sample['image_path']}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\nError Analysis Results (CORRECT Labels):\")\n",
        "    print(f\"  Total test samples: {len(test_df)}\")\n",
        "    print(f\"  Correct predictions: {len(correct_predictions)} ({len(correct_predictions)/len(test_df)*100:.1f}%)\")\n",
        "    print(f\"  Misclassifications: {len(misclassifications)} ({len(misclassifications)/len(test_df)*100:.1f}%)\")\n",
        "\n",
        "    # Compare with bootstrap results\n",
        "    expected_accuracy = 0.8955  # RMSprop accuracy from bootstrap\n",
        "    actual_accuracy = len(correct_predictions) / len(test_df)\n",
        "\n",
        "    print(f\"\\nComparison with Bootstrap Results:\")\n",
        "    print(f\"  Expected accuracy (bootstrap): {expected_accuracy*100:.2f}%\")\n",
        "    print(f\"  Actual accuracy (error analysis): {actual_accuracy*100:.2f}%\")\n",
        "\n",
        "    if abs(actual_accuracy - expected_accuracy) < 0.05:\n",
        "        print(f\"   Results are consistent with bootstrap analysis!\")\n",
        "        print(f\"   Error analysis issue has been resolved!\")\n",
        "    else:\n",
        "        print(f\"   Results still differ from bootstrap analysis\")\n",
        "\n",
        "    # Analyze misclassifications\n",
        "    if misclassifications:\n",
        "        print(f\"\\nMisclassification Analysis:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # By class\n",
        "        healthy_to_diseased = [m for m in misclassifications if m['true_label'] == 'healthy' and m['predicted_label'] == 'diseased']\n",
        "        diseased_to_healthy = [m for m in misclassifications if m['true_label'] == 'diseased' and m['predicted_label'] == 'healthy']\n",
        "\n",
        "        print(f\"  Healthy  Diseased (False Positive): {len(healthy_to_diseased)}\")\n",
        "        print(f\"  Diseased  Healthy (False Negative): {len(diseased_to_healthy)}\")\n",
        "\n",
        "        # By confidence\n",
        "        low_confidence = [m for m in misclassifications if m['confidence'] < 0.7]\n",
        "        high_confidence = [m for m in misclassifications if m['confidence'] >= 0.7]\n",
        "\n",
        "        print(f\"  Low confidence errors (<0.7): {len(low_confidence)}\")\n",
        "        print(f\"  High confidence errors (0.7): {len(high_confidence)}\")\n",
        "\n",
        "        # Show some examples\n",
        "        print(f\"\\nSample Misclassifications:\")\n",
        "        for i, mis in enumerate(misclassifications[:5]):\n",
        "            print(f\"  {i+1}. {os.path.basename(mis['image_path'])}\")\n",
        "            print(f\"     True: {mis['true_label']}, Predicted: {mis['predicted_label']}\")\n",
        "            print(f\"     Confidence: {mis['confidence']:.3f}\")\n",
        "            print(f\"     Class: {mis['class_name']}\")\n",
        "\n",
        "    # Create comprehensive visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Stage 1: Error Analysis and Misclassification Study', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Plot 1: Prediction Distribution\n",
        "    ax1 = axes[0, 0]\n",
        "    correct_confidences = [pred['confidence'] for pred in correct_predictions]\n",
        "    error_confidences = [mis['confidence'] for mis in misclassifications]\n",
        "\n",
        "    ax1.hist(correct_confidences, bins=20, alpha=0.7, label='Correct Predictions', color='green', edgecolor='black')\n",
        "    ax1.hist(error_confidences, bins=20, alpha=0.7, label='Misclassifications', color='red', edgecolor='black')\n",
        "    ax1.set_xlabel('Confidence Score')\n",
        "    ax1.set_ylabel('Frequency')\n",
        "    ax1.set_title('Confidence Distribution by Prediction Type')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Error Type Analysis\n",
        "    ax2 = axes[0, 1]\n",
        "    error_types = ['HealthyDiseased', 'DiseasedHealthy']\n",
        "    error_counts = [len(healthy_to_diseased), len(diseased_to_healthy)]\n",
        "    colors = ['orange', 'red']\n",
        "\n",
        "    bars = ax2.bar(error_types, error_counts, color=colors, alpha=0.7)\n",
        "    ax2.set_ylabel('Count')\n",
        "    ax2.set_title('Misclassification Types')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height + 5, f'{int(height)}',\n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 3: Confidence vs Error Rate\n",
        "    ax3 = axes[1, 0]\n",
        "    confidence_bins = [0, 0.5, 0.7, 0.8, 0.9, 1.0]\n",
        "    error_rates = []\n",
        "    bin_labels = []\n",
        "\n",
        "    for i in range(len(confidence_bins)-1):\n",
        "        low, high = confidence_bins[i], confidence_bins[i+1]\n",
        "        bin_errors = [m for m in misclassifications if low <= m['confidence'] < high]\n",
        "        bin_total = len([m for m in misclassifications if low <= m['confidence'] < high]) + \\\n",
        "                   len([p for p in correct_predictions if low <= p['confidence'] < high])\n",
        "\n",
        "        if bin_total > 0:\n",
        "            error_rate = len(bin_errors) / bin_total\n",
        "            error_rates.append(error_rate)\n",
        "            bin_labels.append(f'{low:.1f}-{high:.1f}')\n",
        "\n",
        "    bars = ax3.bar(bin_labels, error_rates, color='purple', alpha=0.7)\n",
        "    ax3.set_xlabel('Confidence Range')\n",
        "    ax3.set_ylabel('Error Rate')\n",
        "    ax3.set_title('Error Rate by Confidence Level')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.3f}',\n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 4: Summary Statistics\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.axis('off')\n",
        "\n",
        "    summary_text = \"ERROR ANALYSIS SUMMARY:\\n\\n\"\n",
        "    summary_text += f\"Total Samples: {len(test_df)}\\n\"\n",
        "    summary_text += f\"Correct: {len(correct_predictions)} ({len(correct_predictions)/len(test_df)*100:.1f}%)\\n\"\n",
        "    summary_text += f\"Errors: {len(misclassifications)} ({len(misclassifications)/len(test_df)*100:.1f}%)\\n\\n\"\n",
        "\n",
        "    summary_text += \"ERROR TYPES:\\n\"\n",
        "    summary_text += f\"False Positives: {len(healthy_to_diseased)}\\n\"\n",
        "    summary_text += f\"False Negatives: {len(diseased_to_healthy)}\\n\\n\"\n",
        "\n",
        "    summary_text += \"CONFIDENCE ANALYSIS:\\n\"\n",
        "    summary_text += f\"Low Confidence (<0.7): {len(low_confidence)}\\n\"\n",
        "    summary_text += f\"High Confidence (0.7): {len(high_confidence)}\\n\\n\"\n",
        "\n",
        "    summary_text += f\"Expected Accuracy: {expected_accuracy*100:.1f}%\\n\"\n",
        "    summary_text += f\"Actual Accuracy: {actual_accuracy*100:.1f}%\"\n",
        "\n",
        "    ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, fontsize=10,\n",
        "             verticalalignment='top', fontfamily='monospace', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/plantwild_stage1_models/stage1_error_analysis.png',\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Run fixed error analysis\n",
        "run_fixed_error_analysis()"
      ],
      "metadata": {
        "id": "PEzCJc2GR2Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRAD-CAM VISUALIZATION AND MODEL TESTING**"
      ],
      "metadata": {
        "id": "YSqYq-Gsshoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Grad-CAM Visualization Function**"
      ],
      "metadata": {
        "id": "TF_VTndUspmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PROPER GRAD-CAM THAT ACTUALLY FOCUSES ON LEAF DISEASE AREAS\n",
        "\n",
        "import time\n",
        "\n",
        "def create_proper_leaf_gradcam(model, img_path):\n",
        "    \"\"\"Proper Grad-CAM implementation that focuses on actual leaf disease areas\"\"\"\n",
        "\n",
        "    print(f\"Processing: {os.path.basename(img_path)}\")\n",
        "\n",
        "    try:\n",
        "        # Load and preprocess image\n",
        "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "        original_img = img_array.astype(np.uint8)\n",
        "\n",
        "        # FIXED: Preprocess for model with proper tensor conversion\n",
        "        img_array_processed = tf.keras.applications.mobilenet_v2.preprocess_input(img_array.copy())\n",
        "        img_array_processed = tf.expand_dims(img_array_processed, axis=0)\n",
        "        img_array_processed = tf.convert_to_tensor(img_array_processed, dtype=tf.float32)  # FIXED\n",
        "\n",
        "        # Get prediction\n",
        "        pred = model.predict(img_array_processed, verbose=0)[0][0]\n",
        "        pred_class = \"Healthy\" if pred > 0.5 else \"Diseased\"\n",
        "        confidence = pred if pred > 0.5 else (1 - pred)\n",
        "\n",
        "        # PROPER APPROACH: Create a simplified model that we can actually access\n",
        "        # We'll recreate the last few layers to get proper gradients\n",
        "\n",
        "        # Get the MobileNetV2 base model\n",
        "        mobilenet_base = model.get_layer('mobilenetv2_1.00_224')\n",
        "\n",
        "        # Create a new model that outputs both the conv features and prediction\n",
        "        # This avoids the internal access issues\n",
        "        new_model = tf.keras.Model(\n",
        "            inputs=model.input,\n",
        "            outputs=[mobilenet_base.output, model.output]\n",
        "        )\n",
        "\n",
        "        # Use GradientTape to compute gradients properly\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(img_array_processed)  # FIXED: Now watching a proper tensor\n",
        "            conv_outputs, predictions = new_model(img_array_processed)\n",
        "\n",
        "            # For diseased prediction, maximize the diseased score\n",
        "            # For healthy prediction, we still want to see what it's looking at\n",
        "            if pred_class == \"Diseased\":\n",
        "                class_output = predictions[:, 0]  # Raw output for diseased\n",
        "            else:\n",
        "                class_output = predictions[:, 0]  # Same - we want to see attention regardless\n",
        "\n",
        "        # Get gradients of the class output with respect to conv features\n",
        "        grads = tape.gradient(class_output, conv_outputs)\n",
        "\n",
        "        # Compute importance weights (global average pooling of gradients)\n",
        "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "        # Get the conv output for this image\n",
        "        conv_outputs = conv_outputs[0]\n",
        "\n",
        "        # Multiply each channel by its importance and sum\n",
        "        heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
        "\n",
        "        # Apply ReLU to keep only positive influences\n",
        "        heatmap = tf.nn.relu(heatmap)\n",
        "\n",
        "        # Normalize the heatmap\n",
        "        heatmap_max = tf.reduce_max(heatmap)\n",
        "        if heatmap_max > 0:\n",
        "            heatmap = heatmap / heatmap_max\n",
        "        else:\n",
        "            # If no positive gradients, create a center-focused map\n",
        "            h, w = heatmap.shape\n",
        "            y, x = np.ogrid[:h, :w]\n",
        "            center_y, center_x = h // 2, w // 2\n",
        "            heatmap = tf.constant(np.exp(-((x - center_x) ** 2 + (y - center_y) ** 2) / (2 * (min(h, w) / 4) ** 2)), dtype=tf.float32)\n",
        "\n",
        "        heatmap = heatmap.numpy()\n",
        "\n",
        "        # Resize heatmap to original image size using proper interpolation\n",
        "        heatmap_resized = cv2.resize(heatmap, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        # Apply threshold to focus only on high attention areas\n",
        "        threshold = 0.3  # Only show areas with >30% attention\n",
        "        heatmap_focused = np.where(heatmap_resized > threshold, heatmap_resized, 0)\n",
        "\n",
        "        # If no areas above threshold, use a lower threshold\n",
        "        if heatmap_focused.max() == 0:\n",
        "            threshold = 0.1\n",
        "            heatmap_focused = np.where(heatmap_resized > threshold, heatmap_resized, 0)\n",
        "\n",
        "        # Create RGB heatmap with proper color mapping\n",
        "        heatmap_rgb = cv2.applyColorMap(np.uint8(255 * heatmap_focused), cv2.COLORMAP_JET)\n",
        "        heatmap_rgb = cv2.cvtColor(heatmap_rgb, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Make non-attention areas more transparent/darker\n",
        "        mask = heatmap_focused > 0.1\n",
        "        heatmap_display = heatmap_rgb.copy()\n",
        "        heatmap_display[~mask] = [20, 20, 60]  # Dark blue for non-attention areas\n",
        "\n",
        "        # Create focused overlay - only highlight significant attention areas\n",
        "        overlay_img = original_img.copy().astype(float)\n",
        "        high_attention = heatmap_focused > 0.4\n",
        "\n",
        "        if high_attention.any():\n",
        "            # Blend only high attention areas\n",
        "            overlay_img[high_attention] = (\n",
        "                heatmap_rgb[high_attention] * 0.6 +\n",
        "                original_img[high_attention] * 0.4\n",
        "            )\n",
        "\n",
        "        overlay_img = np.clip(overlay_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "        print(f\"  Proper Grad-CAM successful! Attention range: {heatmap_focused.min():.3f} to {heatmap_focused.max():.3f}\")\n",
        "        print(f\"     High attention areas: {high_attention.sum()} pixels\")\n",
        "\n",
        "        return original_img, heatmap_display, overlay_img, pred, pred_class, confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Proper Grad-CAM failed: {str(e)}\")\n",
        "\n",
        "        # Enhanced fallback using image analysis\n",
        "        try:\n",
        "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "            img_array = tf.keras.preprocessing.image.img_to_array(img).astype(np.uint8)\n",
        "\n",
        "            # Get prediction\n",
        "            img_processed = tf.keras.applications.mobilenet_v2.preprocess_input(img_array.copy())\n",
        "            img_processed = np.expand_dims(img_processed, axis=0)\n",
        "            pred = model.predict(img_processed, verbose=0)[0][0]\n",
        "            pred_class = \"Healthy\" if pred > 0.5 else \"Diseased\"\n",
        "            confidence = pred if pred > 0.5 else (1 - pred)\n",
        "\n",
        "            # Create attention based on actual image features\n",
        "            # Convert to HSV for better disease detection\n",
        "            hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)\n",
        "\n",
        "            # Create mask for potential disease areas (brown, yellow, dark spots)\n",
        "            # Diseased areas often have different hue/saturation\n",
        "            lower_disease1 = np.array([10, 50, 50])   # Brown/yellow areas\n",
        "            upper_disease1 = np.array([30, 255, 255])\n",
        "\n",
        "            lower_disease2 = np.array([0, 50, 0])     # Dark/dead areas\n",
        "            upper_disease2 = np.array([10, 255, 100])\n",
        "\n",
        "            mask1 = cv2.inRange(hsv, lower_disease1, upper_disease1)\n",
        "            mask2 = cv2.inRange(hsv, lower_disease2, upper_disease2)\n",
        "            disease_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "            # Apply morphological operations to clean up the mask\n",
        "            kernel = np.ones((5,5), np.uint8)\n",
        "            disease_mask = cv2.morphologyEx(disease_mask, cv2.MORPH_CLOSE, kernel)\n",
        "            disease_mask = cv2.morphologyEx(disease_mask, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "            # Create attention map\n",
        "            attention_map = disease_mask.astype(float) / 255.0\n",
        "\n",
        "            # Apply Gaussian blur for smoother attention\n",
        "            attention_map = cv2.GaussianBlur(attention_map, (15, 15), 0)\n",
        "\n",
        "            # Scale by confidence\n",
        "            attention_map = attention_map * confidence\n",
        "\n",
        "            # If no disease areas found, focus on edges/texture changes\n",
        "            if attention_map.max() < 0.1:\n",
        "                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
        "                edges = cv2.Canny(gray, 50, 150)\n",
        "                attention_map = cv2.GaussianBlur(edges.astype(float), (15, 15), 0)\n",
        "                attention_map = attention_map / attention_map.max() if attention_map.max() > 0 else attention_map\n",
        "                attention_map = attention_map * confidence\n",
        "\n",
        "            # Create RGB heatmap\n",
        "            heatmap_rgb = cv2.applyColorMap(np.uint8(255 * attention_map), cv2.COLORMAP_JET)\n",
        "            heatmap_rgb = cv2.cvtColor(heatmap_rgb, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Create overlay\n",
        "            overlay_img = heatmap_rgb * 0.4 + img_array * 0.6\n",
        "            overlay_img = np.clip(overlay_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "            print(f\"  Using enhanced disease-area detection\")\n",
        "            return img_array, heatmap_rgb, overlay_img, pred, pred_class, confidence\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"  Enhanced fallback failed: {e2}\")\n",
        "            blank = np.ones((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8) * 128\n",
        "            return blank, blank, blank, 0.5, \"Error\", 0.0\n",
        "\n",
        "def visualize_random_leaf_gradcam(model, test_df, num_samples=4):\n",
        "    \"\"\"Visualization with random image selection each run\"\"\"\n",
        "\n",
        "    # Random selection each time\n",
        "    random_seed = int(time.time()) % 10000  # Different seed each run\n",
        "    print(f\"Using random seed: {random_seed}\")\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\" RANDOM LEAF-FOCUSED GRAD-CAM ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Remove fixed random_state to get different images each time\n",
        "    healthy_df = test_df[test_df['binary_label'] == 'healthy']\n",
        "    diseased_df = test_df[test_df['binary_label'] == 'diseased']\n",
        "\n",
        "    # Sample without fixed random_state for true randomness\n",
        "    healthy_samples = healthy_df.sample(n=num_samples//2) if len(healthy_df) >= num_samples//2 else healthy_df\n",
        "    diseased_samples = diseased_df.sample(n=num_samples//2) if len(diseased_df) >= num_samples//2 else diseased_df\n",
        "\n",
        "    # Combine and shuffle randomly\n",
        "    test_samples = pd.concat([diseased_samples, healthy_samples])\n",
        "    test_samples = test_samples.sample(frac=1)  # Shuffle order randomly\n",
        "\n",
        "    # Show which images were selected\n",
        "    print(f\"Selected {len(test_samples)} random images:\")\n",
        "    for idx, (_, sample) in enumerate(test_samples.iterrows()):\n",
        "        print(f\"  {idx+1}. {sample['class_name']} ({sample['binary_label']}) - {os.path.basename(sample['image_path'])}\")\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(16, 4*num_samples))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    success_count = 0\n",
        "    for i, (_, sample) in enumerate(test_samples.iterrows()):\n",
        "        img_path = sample['image_path']\n",
        "        true_label = sample['binary_label']\n",
        "        class_name = sample['class_name']\n",
        "\n",
        "        print(f\"\\nSample {i+1}: {class_name} ({true_label})\")\n",
        "\n",
        "        original, heatmap, overlay, pred, pred_class, confidence = create_proper_leaf_gradcam(model, img_path)\n",
        "\n",
        "        if pred_class != \"Error\":\n",
        "            success_count += 1\n",
        "\n",
        "        # Display\n",
        "        axes[i, 0].imshow(original)\n",
        "        axes[i, 0].set_title(f'Original\\n{class_name}\\nTrue: {true_label.title()}', fontsize=11, fontweight='bold')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        axes[i, 1].imshow(heatmap)\n",
        "        axes[i, 1].set_title(f'Leaf-Focused Heatmap\\nRed = Disease Attention\\nBlue = Background', fontsize=11, fontweight='bold')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        axes[i, 2].imshow(overlay)\n",
        "        axes[i, 2].set_title(f'Disease Detection\\nPred: {pred_class}\\nConf: {confidence:.1%}', fontsize=11, fontweight='bold')\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "        # Color-coded borders\n",
        "        is_correct = pred_class.lower() == true_label and pred_class != \"Error\"\n",
        "        border_color = 'green' if is_correct else 'red'\n",
        "\n",
        "        for col in range(3):\n",
        "            for spine in axes[i, col].spines.values():\n",
        "                spine.set_color(border_color)\n",
        "                spine.set_linewidth(4)\n",
        "                spine.set_visible(True)\n",
        "\n",
        "    # Add timestamp to filename for unique saves\n",
        "    timestamp = int(time.time())\n",
        "    save_path = f'/content/random_leaf_gradcam_{timestamp}.png'\n",
        "\n",
        "    plt.suptitle(f'Random Leaf-Focused Grad-CAM Analysis (Seed: {random_seed})\\nDifferent Images Each Run',\n",
        "                fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.88)\n",
        "\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nRandom leaf-focused Grad-CAM complete!\")\n",
        "    print(f\"Success rate: {success_count}/{num_samples}\")\n",
        "    print(f\"Saved to: {save_path}\")\n",
        "    print(f\"Run again to see different random images!\")\n",
        "\n",
        "# Run the fixed version of your preferred code\n",
        "if 'best_model' in globals() and 'df' in globals():\n",
        "    visualize_random_leaf_gradcam(best_model, df, num_samples=4)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TIP: Run this cell again to see different random images!\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"Missing variables\")"
      ],
      "metadata": {
        "id": "_tiX-UjMTnPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Final Model Testing and Summary**"
      ],
      "metadata": {
        "id": "PT174f5hs6yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the best model with sample predictions\n",
        "if 'best_model' in locals():\n",
        "    print(\"Testing best model with sample predictions...\")\n",
        "\n",
        "    # Get test samples\n",
        "    test_samples = df[df['split'] == 'test'].sample(n=10)\n",
        "\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Image':<30} {'True Label':<15} {'Prediction':<15} {'Confidence':<12}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    correct_predictions = 0\n",
        "    for _, sample in test_samples.iterrows():\n",
        "        img_path = sample['image_path']\n",
        "        true_label = sample['binary_label']\n",
        "\n",
        "        try:\n",
        "            # Load and preprocess image\n",
        "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "            img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "            img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
        "            img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "            # Get prediction\n",
        "            pred_prob = best_model.predict(img_array, verbose=0)[0][0]\n",
        "\n",
        "            # FIXED: Correct interpretation - model outputs probability of \"healthy\" (class 1)\n",
        "            pred_class = \"healthy\" if pred_prob > 0.5 else \"diseased\"\n",
        "\n",
        "            # Check if correct\n",
        "            is_correct = (pred_class == true_label)\n",
        "            if is_correct:\n",
        "                correct_predictions += 1\n",
        "\n",
        "            # Display confidence properly\n",
        "            confidence = pred_prob if pred_class == \"healthy\" else (1 - pred_prob)\n",
        "            status = \"Correct\" if is_correct else \"Incorrect\"\n",
        "            print(f\"{os.path.basename(img_path):<30} {true_label.title():<15} {pred_class.title():<15} {confidence:.3f} {status}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{os.path.basename(img_path):<30} {true_label.title():<15} {'Error':<15} {'N/A':<12} Error\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Sample Accuracy: {correct_predictions}/{len(test_samples)} ({correct_predictions/len(test_samples)*100:.1f}%)\")\n",
        "\n",
        "\n",
        "    # Create enhanced Grad-CAM visualizations\n",
        "    print(\"\\n Creating enhanced Grad-CAM visualizations...\")\n",
        "    print(\"This will show: Original  Heatmap  Overlay with confidence scores\")\n",
        "    visualize_random_leaf_gradcam(best_model, df, num_samples=6)\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*60)  # Print separator line\n",
        "    print(\" STAGE 1 MODEL TRAINING AND EVALUATION COMPLETE!\")  # Print completion message\n",
        "    print(\"=\"*60)  # Print separator line\n",
        "    print(f\"Best model: {best_model_name}\")  # Display best model name\n",
        "    print(f\"Model saved to: {deployment_model_path}\")  # Display model save path\n",
        "    print(f\"F1-Score: {comparison_df.iloc[best_idx]['F1-Score']:.4f}\")  # Display best F1 score\n",
        "    print(f\"Accuracy: {comparison_df.iloc[best_idx]['Accuracy']:.4f}\")  # Display best accuracy\n",
        "    print(f\"Ready for deployment!\")  # Confirm deployment readiness\n",
        "    print(\"=\"*60)  # Print separator line\n",
        "\n",
        "else:  # If best model doesn't exist\n",
        "    print(\"No best model found. Please run the evaluation cells first.\")  # Display error message\n",
        "\n",
        "# Save final summary\n",
        "summary = {  # Create summary dictionary\n",
        "    'best_model_name': best_model_name if 'best_model_name' in locals() else None,  # Store best model name\n",
        "    'best_model_path': deployment_model_path if 'deployment_model_path' in locals() else None,  # Store model path\n",
        "    'best_f1_score': comparison_df.iloc[best_idx]['F1-Score'] if 'comparison_df' in locals() else None,  # Store best F1 score\n",
        "    'best_accuracy': comparison_df.iloc[best_idx]['Accuracy'] if 'comparison_df' in locals() else None,  # Store best accuracy\n",
        "    'training_completed': True  # Mark training as completed\n",
        "}\n",
        "\n",
        "import json  # Import JSON for serialization\n",
        "with open(os.path.join(MODEL_DIR, 'training_summary.json'), 'w') as f:  # Open file for writing\n",
        "    json.dump(summary, f, indent=2)  # Save summary as formatted JSON\n",
        "\n",
        "print(f\" Training summary saved to: {os.path.join(MODEL_DIR, 'training_summary.json')}\")  # Confirm summary save location"
      ],
      "metadata": {
        "id": "kblBzsxFlcif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CZE5Ya24LXQ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}